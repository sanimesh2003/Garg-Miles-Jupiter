{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab575a6a-0d95-4dc3-b026-bd59db3297f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------\n",
    "# Cell 1 : Imports & Global Configuration\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "# %matplotlib inline is a magic command specific to Jupyter notebooks.\n",
    "# It tells Jupyter to display matplotlib plots inline (within the notebook),\n",
    "# instead of in a separate external window.\n",
    "%matplotlib inline\n",
    "\n",
    "# ==============================================================\n",
    "# Core Python libraries for data science, mathematics, etc.\n",
    "# ==============================================================\n",
    "import numpy as np               # Numerical computing library, fundamental for arrays and math operations\n",
    "import pandas as pd              # Data manipulation and analysis library\n",
    "import matplotlib.pyplot as plt  # Library for generating plots and figures\n",
    "import os                        # Provides a way of using operating system dependent functionality (e.g., file paths)\n",
    "import glob                      # Used to find file paths matching a specified pattern\n",
    "import time                      # Provides time-related functions (e.g., for measuring durations)\n",
    "from datetime import datetime    # Allows handling of dates and times in a convenient way\n",
    "import requests                  # Allows sending HTTP requests (used here for Horizons API calls)\n",
    "import math                      # Math module for constants and mathematical functions\n",
    "\n",
    "# ==============================================================\n",
    "# FITS and astronomy-specific tools\n",
    "# ==============================================================\n",
    "from astropy.io import fits                         # For reading/writing FITS files (astronomical data)\n",
    "from astropy.visualization import (ZScaleInterval,ImageNormalize)   # Helps with image normalization/scaling in astronomy\n",
    "from astropy.wcs import WCS                         # World Coordinate System (used for astronomical image coordinates)\n",
    "from photutils.detection import DAOStarFinder        # Used to detect stars or point sources in an image\n",
    "from photutils.aperture import (aperture_photometry,CircularAperture)    # Used for photometric measurements in apertures\n",
    "from astroquery.vizier import Vizier                # Allows querying of catalogs from the Vizier service\n",
    "from reproject import reproject_interp               # Image reprojection, e.g., aligning images to a common WCS\n",
    "from astroalign import register                      # Automatically align two astronomical images\n",
    "from lightkurve import search_lightcurvefile         # For working with Kepler/TESS light curve data\n",
    "import aplpy                                       # For advanced plotting of FITS images (astronomy)\n",
    "\n",
    "# ==============================================================\n",
    "# Image processing libraries\n",
    "# ==============================================================\n",
    "from skimage import filters, measure, morphology  # Various image processing algorithms (e.g., thresholding, morphological ops)\n",
    "from skimage.measure import label, regionprops    # Tools to label and measure regions in binary images\n",
    "from skimage.morphology import binary_closing, disk\n",
    "from matplotlib.patches import Ellipse           # Allows drawing ellipse shapes on matplotlib figures\n",
    "import matplotlib.pyplot as plt                  # (Imported again, but typically we'd have it once; duplicates are harmless)\n",
    "import cv2                                       # OpenCV for advanced image processing operations\n",
    "\n",
    "# ==============================================================\n",
    "# Plotly for interactive visualizations\n",
    "# ==============================================================\n",
    "import plotly.graph_objects as go  # For creating interactive plots with Plotly\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# Constants used in this project\n",
    "# --------------------------------------------------------------\n",
    "horizons_url = 'https://ssd.jpl.nasa.gov/api/horizons_file.api'\n",
    "# horizons_url: The base URL endpoint for NASA JPL Horizons API requests,\n",
    "# which retrieves ephemeris data for solar system bodies, spacecraft, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac99c7c-038a-4295-9a6c-fec66352e9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------\n",
    "# Cell 2 : Timer Utilities\n",
    "# --------------------------------------------------------------------------------\n",
    "# This section defines a set of utility functions to help measure and report the\n",
    "# execution time of different sections of a script. The utilities are especially useful\n",
    "# for performance profiling and debugging, as they allow you to track how long each part\n",
    "# of your code takes to execute.\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Initialize a global list to store timing records for each section of code.\n",
    "# Each record in the list will be a dictionary containing:\n",
    "#   - 'section': A string label for the code section.\n",
    "#   - 'start'  : A timestamp (float) when the section started.\n",
    "#   - 'end'    : A timestamp (float) when the section ended (initially None).\n",
    "#   - 'duration': The computed duration of the section (in seconds, initially None).\n",
    "time_records = []  # This list serves as the central repository for all timing data.\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Define a function to mark the beginning of a new section of code that you wish to time.\n",
    "def start_section(section_name):\n",
    "    \"\"\"\n",
    "    Record the start time of a code section.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    section_name : str\n",
    "        A descriptive name or label for the code section whose start time is being recorded.\n",
    "\n",
    "    Process:\n",
    "    --------\n",
    "    1. Capture the current time using time.time(), which returns the time in seconds since the epoch.\n",
    "    2. Create a new dictionary (i.e., a timing record) with the following keys:\n",
    "         - 'section': Stores the provided section name.\n",
    "         - 'start'  : Stores the current timestamp captured in step 1.\n",
    "         - 'end'    : Initialized to None, as the section has not yet completed.\n",
    "         - 'duration': Initialized to None, since the elapsed time has not been calculated yet.\n",
    "    3. Append this dictionary to the global list 'time_records' for later use.\n",
    "    \n",
    "    Example:\n",
    "    --------\n",
    "    To time a data loading process, you might call:\n",
    "        start_section(\"Data Loading\")\n",
    "    \"\"\"\n",
    "    # Get the current time (in seconds since the epoch) and store it in the variable 'now'.\n",
    "    now = time.time()  # 'time.time()' returns a floating-point number representing the current time.\n",
    "    \n",
    "    # Append a new dictionary to the global 'time_records' list.\n",
    "    # This dictionary encapsulates all necessary timing details for the current section.\n",
    "    time_records.append({\n",
    "        'section': section_name,  # Assign the provided section name to the 'section' key.\n",
    "        'start': now,             # Record the current timestamp under the 'start' key.\n",
    "        'end': None,              # Initialize the 'end' key to None; it will be updated when the section ends.\n",
    "        'duration': None          # Initialize the 'duration' key to None; will be computed later.\n",
    "    })\n",
    "    # End of the start_section function.\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Define a function to mark the end of the most recent section and compute its duration.\n",
    "def end_section():\n",
    "    \"\"\"\n",
    "    Record the end time for the most recent section and compute its duration.\n",
    "\n",
    "    Process:\n",
    "    --------\n",
    "    1. Capture the current time as the ending timestamp using time.time().\n",
    "    2. Check if there is at least one timing record in the 'time_records' list.\n",
    "       If the list is empty (i.e., no section was started), the function returns immediately.\n",
    "    3. Retrieve the most recent timing record from the list (i.e., the last dictionary).\n",
    "    4. Update the 'end' key in this record with the current timestamp.\n",
    "    5. Calculate the duration by subtracting the 'start' time from the current time, and update\n",
    "       the 'duration' key with this value.\n",
    "       \n",
    "    Example:\n",
    "    --------\n",
    "    After completing a timed code section, you would call:\n",
    "        end_section()\n",
    "    This marks the end of the most recently started section.\n",
    "    \"\"\"\n",
    "    # Capture the current time, which will be used as the end time for the section.\n",
    "    now = time.time()  # 'now' is a float representing the current timestamp.\n",
    "    \n",
    "    # Check if there are any timing records in the 'time_records' list.\n",
    "    # If the list is empty, then no section has been started, and we have nothing to end.\n",
    "    if not time_records:\n",
    "        return  # Exit the function early if 'time_records' is empty.\n",
    "    \n",
    "    # Retrieve the most recent timing record (i.e., the last entry in the list).\n",
    "    record = time_records[-1]  # This is the timing record corresponding to the most recent section.\n",
    "    \n",
    "    # Update the 'end' key of the retrieved record with the current timestamp.\n",
    "    record['end'] = now  # Mark the time at which the section ended.\n",
    "    \n",
    "    # Compute the duration for the section by subtracting the recorded 'start' time from the current time.\n",
    "    # The resulting duration (in seconds) is stored in the 'duration' key.\n",
    "    record['duration'] = now - record['start']\n",
    "    # End of the end_section function.\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Define a function to print a detailed summary of the timing information for all recorded sections.\n",
    "def print_time_summary():\n",
    "    \"\"\"\n",
    "    Print a structured table summarizing timing details for each recorded section,\n",
    "    as well as the total runtime for the entire process.\n",
    "\n",
    "    Output Details:\n",
    "    ---------------\n",
    "    - Each row in the table corresponds to a recorded section and displays:\n",
    "         * The section name.\n",
    "         * The start time formatted as HH:MM:SS.ms (milliseconds precision).\n",
    "         * The end time formatted in the same manner (or \"N/A\" if not available).\n",
    "         * The duration in seconds, formatted to three decimal places (or \"N/A\" if not computed).\n",
    "    - At the end of the table, the total runtime is printed. This is calculated as the difference\n",
    "      between the earliest start time (from the first section) and the latest end time (from the last section).\n",
    "    \n",
    "    Process:\n",
    "    --------\n",
    "    1. Check if there are any timing records; if none exist, the function returns without printing.\n",
    "    2. Print a header to denote the beginning of the timing summary.\n",
    "    3. Calculate the width for the 'Section' column by determining the length of the longest section name.\n",
    "    4. Print a header row with column titles for Section, Start, End, and Duration.\n",
    "    5. Draw a separator line for visual clarity.\n",
    "    6. Compute the total runtime as the difference between the end time of the last section\n",
    "       and the start time of the first section.\n",
    "    7. Iterate over each timing record, formatting the start and end times from timestamps to human-readable\n",
    "       strings (using the HH:MM:SS.ms format). If a section hasn't ended, \"N/A\" is displayed.\n",
    "    8. Print each section's details in a formatted row.\n",
    "    9. Print a final separator and the total runtime at the bottom of the summary.\n",
    "    \n",
    "    Example:\n",
    "    --------\n",
    "    After timing multiple sections of your script, simply call:\n",
    "        print_time_summary()\n",
    "    to display the timing summary table.\n",
    "    \"\"\"\n",
    "    # First, check if there are any timing records available to print.\n",
    "    if not time_records:\n",
    "        return  # If 'time_records' is empty, exit the function without printing anything.\n",
    "    \n",
    "    # Print an introductory header for the timing summary.\n",
    "    print(\"\\n=== PROCESSING TIME SUMMARY ===\")\n",
    "\n",
    "    # --------------------------------------------------------------------------------\n",
    "    # Determine the appropriate width for the 'Section' column in the output table.\n",
    "    # This is calculated based on the length of the longest section name recorded.\n",
    "    max_len = max(len(r['section']) for r in time_records)  # Find the maximum length of the section names.\n",
    "    # Ensure a minimum width of 20 characters for the column, adding a little extra padding (+2) if needed.\n",
    "    name_col_width = max(20, max_len + 2)\n",
    "\n",
    "    # --------------------------------------------------------------------------------\n",
    "    # Print the header row for the timing summary table.\n",
    "    # The formatting instructions:\n",
    "    #   - The section name is left-aligned within a width specified by 'name_col_width'.\n",
    "    #   - The start and end times are right-aligned in fields 15 characters wide.\n",
    "    #   - The duration (in seconds) is right-aligned in a field 14 characters wide.\n",
    "    print(f\"{'Section':<{name_col_width}}  {'Start':>15}   {'End':>15}   {'Duration (s)':>14}\")\n",
    "    \n",
    "    # Print a separator line that spans the combined width of the table for clarity.\n",
    "    print(\"-\" * (name_col_width + 50))\n",
    "\n",
    "    # --------------------------------------------------------------------------------\n",
    "    # Calculate the total runtime of the entire process.\n",
    "    # This is computed as the difference between the end time of the last section and the start time of the first section.\n",
    "    total_time = time_records[-1]['end'] - time_records[0]['start']\n",
    "\n",
    "    # --------------------------------------------------------------------------------\n",
    "    # Loop through each timing record to print the details for each section.\n",
    "    for r in time_records:\n",
    "        # Extract the section name from the current timing record.\n",
    "        section = r['section']\n",
    "        \n",
    "        # Convert the recorded 'start' timestamp (a float) into a formatted string:\n",
    "        #   - datetime.fromtimestamp() converts the timestamp to a datetime object.\n",
    "        #   - strftime(\"%H:%M:%S.%f\") formats the datetime object into a string with hours, minutes, seconds, and microseconds.\n",
    "        #   - The slicing [:-3] trims the last three digits of microseconds to display milliseconds instead.\n",
    "        start_dt = datetime.fromtimestamp(r['start']).strftime(\"%H:%M:%S.%f\")[:-3]\n",
    "        \n",
    "        # Similarly, convert the 'end' timestamp to a formatted string if it exists.\n",
    "        # If the 'end' key is None (i.e., the section did not end properly), display \"N/A\" instead.\n",
    "        end_dt = datetime.fromtimestamp(r['end']).strftime(\"%H:%M:%S.%f\")[:-3] if r['end'] else \"N/A\"\n",
    "        \n",
    "        # Format the 'duration' value to three decimal places.\n",
    "        # If 'duration' is None (i.e., not computed), display \"N/A\" as the output.\n",
    "        duration = f\"{r['duration']:.3f}\" if r['duration'] else \"N/A\"\n",
    "\n",
    "        # Print the details for this section in a formatted table row.\n",
    "        # The formatting ensures that all columns (Section, Start, End, Duration) are aligned properly.\n",
    "        print(f\"{section:<{name_col_width}}  {start_dt:>15}   {end_dt:>15}   {duration:>14}\")\n",
    "\n",
    "    # --------------------------------------------------------------------------------\n",
    "    # After printing all section rows, print another separator line for a neat ending.\n",
    "    print(\"-\" * (name_col_width + 50))\n",
    "    \n",
    "    # Finally, print the total runtime for the entire process.\n",
    "    # The label 'Total Runtime' is left-aligned in the section column,\n",
    "    # and the total runtime value is formatted to three decimal places.\n",
    "    print(f\"{'Total Runtime':<{name_col_width}}                       {total_time:.3f} seconds\\n\")\n",
    "    # End of the print_time_summary function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4ebd24-48f5-4652-bf34-ec934e09e0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------\n",
    "# Cell 3 : Horizons API Functions\n",
    "# --------------------------------------------------------------\n",
    "# This cell contains functions to interact with the JPL Horizons API.\n",
    "# The Horizons API provides astronomical ephemeris data (e.g., positions, brightness)\n",
    "# for solar system bodies. The functions below help in:\n",
    "#   1. Constructing the POST data payload to send to Horizons.\n",
    "#   2. Parsing the returned text data to extract specific values.\n",
    "#   3. Managing the overall process of sending the request and retrieving results.\n",
    "\n",
    "def create_input_content(dateobs, timeobs):\n",
    "    \"\"\"\n",
    "    Build the text content needed to POST a request to JPL Horizons.\n",
    "    \n",
    "    This function constructs a multi-line string (which serves as a configuration file)\n",
    "    containing various commands and parameters required by the Horizons API to generate\n",
    "    ephemeris data. Specifically, it sets:\n",
    "    \n",
    "      - COMMAND='599': This selects the target object, here '599' corresponds to Jupiter.\n",
    "      - OBJ_DATA='YES': Indicates that object-specific data should be returned.\n",
    "      - MAKE_EPHEM='YES': Instructs Horizons to generate ephemeris data.\n",
    "      - TABLE_TYPE='OBSERVER': Specifies that the output should be in an observer-centric format.\n",
    "      - CENTER='500@399': Defines the observer’s location. The string '500@399' typically \n",
    "        denotes a geocentric viewpoint (or whichever center is chosen).\n",
    "      - TLIST='{dateobs} {timeobs}': Sets the date and time for which the ephemeris is needed.\n",
    "      - QUANTITIES='9,20,23,24': Specifies which quantities to include in the output:\n",
    "          * 9  => Visual magnitude and surface brightness.\n",
    "          * 20 => Observer range and range-rate.\n",
    "          * 23 => Sun-Observer-Target (SOT) elongation.\n",
    "          * 24 => Phase angle.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    dateobs : str\n",
    "        The observation date, typically in the format 'YYYY-MM-DD' (e.g., '2021-05-03').\n",
    "    timeobs : str\n",
    "        The observation time, typically in the format 'HH:MM:SS' (e.g., '12:34:56').\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        A multi-line string containing the necessary commands and parameters, formatted\n",
    "        in a way that the Horizons API can interpret when sent as the 'input.txt' file.\n",
    "    \"\"\"\n",
    "    # The returned string is formatted as an f-string to dynamically include the provided date and time.\n",
    "    # The special markers !$$SOF and !$$EOF denote the Start and End Of File, required by Horizons.\n",
    "    return f\"\"\"\n",
    "    !$$SOF\n",
    "    COMMAND='599'\n",
    "    OBJ_DATA='YES'\n",
    "    MAKE_EPHEM='YES'\n",
    "    TABLE_TYPE='OBSERVER'\n",
    "    CENTER='500@399'\n",
    "    TLIST='{dateobs} {timeobs}'\n",
    "    QUANTITIES='9,20,23,24'\n",
    "    !$$EOF\n",
    "    \"\"\"\n",
    "    # This string, once generated, is ready to be attached as a file in a POST request to the API.\n",
    "\n",
    "\n",
    "def parse_horizons_text_for_delta_and_sbrt(horizons_text):\n",
    "    \"\"\"\n",
    "    Given the raw text response from Horizons, extract the observer distance (delta)\n",
    "    and the surface brightness (S-brt) from the ephemeris data block.\n",
    "    \n",
    "    The Horizons response text contains a section delineated by $$SOE (Start Of Ephemeris)\n",
    "    and $$EOE (End Of Ephemeris). This block contains the actual data rows with multiple columns.\n",
    "    \n",
    "    Expected token breakdown in a data row:\n",
    "      - tokens[0] : Date/Time part 1\n",
    "      - tokens[1] : Date/Time part 2\n",
    "      - tokens[2] : Apparent magnitude (APmag)\n",
    "      - tokens[3] : Surface brightness (S-brt)\n",
    "      - tokens[4] : Observer distance (delta, in astronomical units, AU)\n",
    "      - tokens[5] : Range rate (deldot)\n",
    "      - tokens[6] : Sun-Observer-Target (SOT) elongation\n",
    "      - tokens[7] : (Often a formatting token, e.g., \"/r\")\n",
    "      - tokens[8] : Another token (e.g., S-T-O)\n",
    "    \n",
    "    The function will:\n",
    "      1. Split the entire response text into individual lines.\n",
    "      2. Locate the indices of the lines containing '$$SOE' and '$$EOE'.\n",
    "      3. Extract the lines between these markers, which contain the ephemeris data.\n",
    "      4. Iterate over each non-empty line in the data block:\n",
    "           - Split the line into tokens.\n",
    "           - If the token list has fewer than 9 elements, skip it.\n",
    "           - Otherwise, attempt to convert token[3] and token[4] into floats:\n",
    "             * token[3] -> s_brt (surface brightness)\n",
    "             * token[4] -> delta (observer distance in AU)\n",
    "      5. Return the parsed (delta, s_brt) values.\n",
    "    \n",
    "    If the expected markers are not found or if parsing fails, the function returns (None, None).\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    horizons_text : str\n",
    "        The complete raw text output from the Horizons API.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple:\n",
    "        A tuple (delta, s_brt) where:\n",
    "         - delta : float or None, representing the observer distance in AU.\n",
    "         - s_brt : float or None, representing the surface brightness.\n",
    "    \"\"\"\n",
    "    # Split the entire Horizons text into a list of individual lines.\n",
    "    lines = horizons_text.splitlines()\n",
    "    \n",
    "    try:\n",
    "        # Identify the index of the line that contains '$$SOE', indicating the start of ephemeris data.\n",
    "        start_index = next(i for i, line in enumerate(lines) if '$$SOE' in line)\n",
    "        # Similarly, identify the index of the line that contains '$$EOE', indicating the end of ephemeris data.\n",
    "        end_index   = next(i for i, line in enumerate(lines) if '$$EOE' in line)\n",
    "    except StopIteration:\n",
    "        # If either '$$SOE' or '$$EOE' is not found, the generator expression will raise StopIteration.\n",
    "        # In that case, print an error message and return (None, None) to indicate failure.\n",
    "        print(\"Could not find $$SOE/$$EOE in Horizons output.\")\n",
    "        return None, None\n",
    "\n",
    "    # Extract the ephemeris data block by taking the lines that fall between the '$$SOE' and '$$EOE' markers.\n",
    "    # Note: start_index+1 skips the '$$SOE' marker line; end_index is not included (i.e., it stops before '$$EOE').\n",
    "    ephem_lines = lines[start_index+1 : end_index]\n",
    "\n",
    "    # Loop through each line in the extracted data block.\n",
    "    for ln in ephem_lines:\n",
    "        # Remove any leading or trailing whitespace to ensure clean tokenization.\n",
    "        ln = ln.strip()\n",
    "        # If the line is empty (i.e., it contains no characters after stripping), skip it.\n",
    "        if not ln:\n",
    "            continue  # Continue to the next line if this one is blank.\n",
    "        # Split the line into tokens based on whitespace. Each token should correspond to a data field.\n",
    "        tokens = ln.split()\n",
    "\n",
    "        # Validate that we have the minimum expected number of tokens (9 in this case).\n",
    "        if len(tokens) < 9:\n",
    "            continue  # If there are fewer than 9 tokens, skip this line as it doesn't contain all required data.\n",
    "        try:\n",
    "            # Parse the surface brightness from token[3]:\n",
    "            s_brt  = float(tokens[3])\n",
    "            # Parse the observer distance (delta) from token[4]:\n",
    "            dist_au = float(tokens[4])\n",
    "            # If both conversions are successful, return the parsed values.\n",
    "            return dist_au, s_brt\n",
    "        except ValueError:\n",
    "            # If there is a ValueError during conversion (e.g., non-numeric value encountered),\n",
    "            # skip this line and continue to the next one.\n",
    "            continue\n",
    "\n",
    "    # If no valid data line is found after processing all lines, return (None, None) to indicate failure.\n",
    "    return None, None\n",
    "\n",
    "\n",
    "def get_horizons_data(dateobs, timeobs):\n",
    "    \"\"\"\n",
    "    Uses create_input_content() to build the Horizons POST data, sends the POST request,\n",
    "    and returns the parsed observer distance (delta) and surface brightness (s_brt).\n",
    "    \n",
    "    Process Overview:\n",
    "      1. Generate the POST payload using the provided date and time.\n",
    "      2. Send a POST request to the Horizons API using the global 'horizons_url'.\n",
    "         - The 'data' parameter specifies the response format as text.\n",
    "         - The 'files' parameter attaches the generated content as 'input.txt'.\n",
    "      3. Check the HTTP response status:\n",
    "         - If the status code is not 200 (OK), print an error message and return (None, None).\n",
    "      4. If successful, pass the response text to parse_horizons_text_for_delta_and_sbrt()\n",
    "         to extract the desired parameters.\n",
    "      5. Return the extracted (delta, s_brt) tuple.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    dateobs : str\n",
    "        The observation date in 'YYYY-MM-DD' format (e.g., '2021-05-03').\n",
    "    timeobs : str\n",
    "        The observation time in 'HH:MM:SS' format (e.g., '12:34:56').\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple:\n",
    "        (delta, s_brt) where:\n",
    "         - delta : float or None, representing the observer's distance in AU.\n",
    "         - s_brt : float or None, representing the surface brightness.\n",
    "    \"\"\"\n",
    "    # Build the POST data payload by calling create_input_content with the specified date and time.\n",
    "    # The resulting 'content' variable is a multi-line string formatted for the Horizons API.\n",
    "    content = create_input_content(dateobs, timeobs)\n",
    "    \n",
    "    # Send the POST request to the Horizons API using the global URL (horizons_url).\n",
    "    # The request is configured with:\n",
    "    #   - A 'data' field specifying that the desired output format is plain text.\n",
    "    #   - A 'files' field attaching the generated content as if it were a file named 'input.txt'.\n",
    "    resp = requests.post(\n",
    "        horizons_url,  # Global variable representing the API endpoint.\n",
    "        data={'format': 'text'},  # Request parameter indicating output should be in text format.\n",
    "        files={'input': ('input.txt', content)}  # Attach the generated content as a file.\n",
    "    )\n",
    "\n",
    "    # Check if the HTTP response status code is not 200 (which indicates an error in the request).\n",
    "    if resp.status_code != 200:\n",
    "        # Print an error message including the problematic status code.\n",
    "        print(f\"Failed Horizons request: {resp.status_code}\")\n",
    "        # Return (None, None) to signal that the data could not be retrieved.\n",
    "        return None, None\n",
    "    \n",
    "    # If the request was successful (HTTP 200 OK), parse the response text to extract 'delta' and 's_brt'.\n",
    "    # The function parse_horizons_text_for_delta_and_sbrt handles the extraction.\n",
    "    return parse_horizons_text_for_delta_and_sbrt(resp.text)\n",
    "    # End of get_horizons_data function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1e0568-255b-447d-a104-741465b81b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------\n",
    "# Cell 4 : Image Processing Utilities\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "# Import OpenCV (cv2) to enable advanced image processing operations.\n",
    "# OpenCV is widely used for tasks such as image transformation, filtering, and more.\n",
    "import cv2  # Ensure OpenCV is available\n",
    "\n",
    "def elliptical_mask(shape, center, major_axis, minor_axis, angle_deg):\n",
    "    \"\"\"\n",
    "    Create a boolean mask (2D numpy array of True/False) indicating which pixels\n",
    "    fall inside a given ellipse.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    shape : tuple of (height, width)\n",
    "        The dimensions of the image or array on which the mask will be applied.\n",
    "    center : tuple of (cx, cy)\n",
    "        The (x, y) coordinates of the center of the ellipse.\n",
    "    major_axis : float\n",
    "        The full length (diameter) along the ellipse's major axis.\n",
    "    minor_axis : float\n",
    "        The full length (diameter) along the ellipse's minor axis.\n",
    "    angle_deg : float\n",
    "        The rotation angle of the ellipse in degrees.\n",
    "        The angle is measured from the x-axis in the OpenCV coordinate convention.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    numpy.ndarray\n",
    "        A 2D boolean numpy array of the same shape as specified, where:\n",
    "          - True  indicates that the pixel lies inside the ellipse.\n",
    "          - False indicates that the pixel lies outside the ellipse.\n",
    "    \"\"\"\n",
    "    # Unpack the 'shape' tuple into individual dimensions:\n",
    "    # 'h' represents the height (number of rows) and 'w' represents the width (number of columns).\n",
    "    (h, w) = shape\n",
    "\n",
    "    # Create coordinate grids using numpy's ogrid.\n",
    "    # np.ogrid is used for generating open multi-dimensional \"meshgrids\", which is memory efficient.\n",
    "    # y_grid: a column vector with values ranging from 0 to h-1 (each row index).\n",
    "    # x_grid: a row vector with values ranging from 0 to w-1 (each column index).\n",
    "    y_grid, x_grid = np.ogrid[0:h, 0:w]\n",
    "\n",
    "    # Extract the center coordinates (cx, cy) from the provided 'center' tuple.\n",
    "    # 'cx' is the x-coordinate and 'cy' is the y-coordinate of the ellipse's center.\n",
    "    cx, cy = center\n",
    "\n",
    "    # Convert the provided rotation angle from degrees to radians.\n",
    "    # This is necessary because the trigonometric functions in numpy (np.cos, np.sin)\n",
    "    # expect the angle in radians.\n",
    "    theta = np.deg2rad(angle_deg)\n",
    "\n",
    "    # Compute the semi-axis lengths:\n",
    "    # The ellipse's major_axis and minor_axis are given as full diameters.\n",
    "    # We need the semi-major axis (half of the major axis) and semi-minor axis (half of the minor axis).\n",
    "    a = major_axis / 2.0  # Semi-major axis: half the length of the major axis.\n",
    "    b = minor_axis / 2.0  # Semi-minor axis: half the length of the minor axis.\n",
    "\n",
    "    # Shift the coordinate grids so that the origin (0,0) is moved to the ellipse's center (cx, cy).\n",
    "    # This simplifies the subsequent computations, as the ellipse equation assumes the ellipse is centered at the origin.\n",
    "    x_shifted = x_grid - cx  # Subtract the x-coordinate of the center from each x value.\n",
    "    y_shifted = y_grid - cy  # Subtract the y-coordinate of the center from each y value.\n",
    "\n",
    "    # Pre-calculate the cosine and sine of the rotation angle.\n",
    "    # These values are used in the rotation transformation of the coordinate system.\n",
    "    cos_t = np.cos(theta)\n",
    "    sin_t = np.sin(theta)\n",
    "\n",
    "    # Rotate the shifted coordinate system by the specified angle.\n",
    "    # This transformation aligns the ellipse with the coordinate axes, allowing us to apply the standard ellipse formula.\n",
    "    # The rotation is performed using the following standard formulas:\n",
    "    #   x_prime = x_shifted * cos(theta) + y_shifted * sin(theta)\n",
    "    #   y_prime = -x_shifted * sin(theta) + y_shifted * cos(theta)\n",
    "    x_prime =  x_shifted * cos_t + y_shifted * sin_t\n",
    "    y_prime = -x_shifted * sin_t + y_shifted * cos_t\n",
    "\n",
    "    # Apply the standard ellipse equation:\n",
    "    #   (x_prime^2) / (a^2) + (y_prime^2) / (b^2) <= 1\n",
    "    # This equation defines the set of points (x_prime, y_prime) that lie inside or on the boundary of the ellipse.\n",
    "    ellipse_equation = (x_prime**2) / (a * a) + (y_prime**2) / (b * b)\n",
    "\n",
    "    # Return a boolean array where each element is True if the corresponding pixel\n",
    "    # satisfies the ellipse equation (i.e., lies inside the ellipse), and False otherwise.\n",
    "    return ellipse_equation <= 1.0\n",
    "\n",
    "\n",
    "def rotate_image_full(data, cx, cy, angle_deg):\n",
    "    \"\"\"\n",
    "    Rotate the entire 2D array `data` (e.g., an image) around the specified center (cx, cy)\n",
    "    by a given angle in degrees, using OpenCV's warpAffine function. The rotated image\n",
    "    retains the same dimensions as the input.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : numpy.ndarray\n",
    "        A 2D numpy array representing the image or data to rotate.\n",
    "    cx, cy : float\n",
    "        The x and y coordinates of the center of rotation.\n",
    "    angle_deg : float\n",
    "        The rotation angle in degrees. In OpenCV, positive angles indicate counter-clockwise rotation.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    numpy.ndarray\n",
    "        A new 2D numpy array containing the rotated image/data, with the same dimensions as the original.\n",
    "    \"\"\"\n",
    "    # Retrieve the dimensions of the input data array.\n",
    "    # 'rows' is the number of rows (height) and 'cols' is the number of columns (width).\n",
    "    rows, cols = data.shape\n",
    "\n",
    "    # Create a 2x3 rotation matrix using OpenCV's cv2.getRotationMatrix2D function.\n",
    "    # This matrix encapsulates both the rotation and translation required to perform the rotation.\n",
    "    # Parameters for getRotationMatrix2D:\n",
    "    #   - The center of rotation, provided as a tuple (cx, cy).\n",
    "    #   - The rotation angle in degrees.\n",
    "    #   - A scale factor (set to 1.0 for no scaling).\n",
    "    M = cv2.getRotationMatrix2D((cx, cy), angle_deg, 1.0)\n",
    "\n",
    "    # Apply the affine transformation to the input data using cv2.warpAffine.\n",
    "    # This function transforms the image using the rotation matrix 'M'.\n",
    "    # The third parameter (cols, rows) defines the size of the output image, which is kept the same.\n",
    "    # The 'flags' parameter, set to cv2.INTER_LINEAR, uses linear interpolation to compute pixel values.\n",
    "    rotated = cv2.warpAffine(data, M, (cols, rows), flags=cv2.INTER_LINEAR)\n",
    "\n",
    "    # Return the resulting rotated image.\n",
    "    return rotated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9eb1a10-aa12-4964-a02a-02c5f804fbdf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------\n",
    "# Cell 5 : Main Analysis / Pipeline\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "# Start the overall script timer to measure total execution time.\n",
    "# \"Script Start\" is a label that helps identify this overall timing record.\n",
    "start_section(\"Script Start\")\n",
    "# --------------------------------------------------------------------------------\n",
    "# The above call to start_section() records the current time into the global\n",
    "# time_records list with the label \"Script Start\" to later compute the total runtime.\n",
    "\n",
    "# -----------------------------------------\n",
    "# A) Get all FITS files\n",
    "# -----------------------------------------\n",
    "# Start a timer for the section that searches for FITS files.\n",
    "start_section(\"Glob FITS files\")\n",
    "\n",
    "# Use the glob module to search recursively for all FITS files in the specified directory.\n",
    "# The pattern indicates:\n",
    "#   - \"jupiter_data_WFC3_F631DRC/MAST_2023_08_29T0311/HST/\" is the base directory.\n",
    "#   - The first \"*\" matches any subdirectory inside HST.\n",
    "#   - The second \"*\" matches any subdirectory inside that subdirectory.\n",
    "#   - \"*.fits\" matches any file ending with the .fits extension.\n",
    "fits_files = glob.glob(\"jupiter_data_WFC3_F631DRC/MAST_2023_08_29T0311/HST/*/*.fits\")\n",
    "# --------------------------------------------------------------------------------\n",
    "# At this point, fits_files is a list containing the file paths to all .fits files\n",
    "# matching the above pattern.\n",
    "\n",
    "# End the timer for \"Glob FITS files\" once the search is complete.\n",
    "end_section()\n",
    "\n",
    "# Check if any FITS files were found.\n",
    "if len(fits_files) == 0:\n",
    "    # If no files are found, notify the user.\n",
    "    print(\"No FITS files found in the specified directory.\")\n",
    "    # End the overall timer (or this section) and print the timing summary.\n",
    "    end_section()\n",
    "    print_time_summary()\n",
    "    # Optionally, you might want to exit the script if no files are found.\n",
    "    # raise SystemExit(\"No FITS files found.\")  # Optionally exit\n",
    "else:\n",
    "    # If FITS files are found, print the total count.\n",
    "    print(f\"Found {len(fits_files)} FITS files.\\n\")\n",
    "# --------------------------------------------------------------------------------\n",
    "# This branch ensures that the rest of the processing only occurs if there are files.\n",
    "\n",
    "# -----------------------------------------\n",
    "# B) Read FITS metadata\n",
    "# -----------------------------------------\n",
    "# Create an empty list to store metadata and image data from each FITS file.\n",
    "file_details = []\n",
    "# Start the timer for reading FITS metadata.\n",
    "start_section(\"Reading FITS metadata\")\n",
    "\n",
    "# Loop over each FITS file found in the previous step.\n",
    "for fits_file in fits_files:\n",
    "    try:\n",
    "        # Open the FITS file using astropy.io.fits in a context manager.\n",
    "        # The 'with' statement ensures the file is properly closed after processing.\n",
    "        with fits.open(fits_file) as hdul:\n",
    "            # 'hdul' is an HDUList (list of Header/Data Units).\n",
    "            # Typically, hdul[0] is the primary HDU (contains header and sometimes data),\n",
    "            # and hdul[1] is expected to contain the actual image data.\n",
    "            header = hdul[0].header   # Extract the primary header.\n",
    "            raw_data = hdul[1].data   # Extract the image data from the first extension.\n",
    "\n",
    "            # Check if the raw_data is valid. If not, create a minimal placeholder array.\n",
    "            if raw_data is None:\n",
    "                # Create a 1x1 array of type float32 filled with 0.0.\n",
    "                data = np.zeros((1, 1), dtype=np.float32)\n",
    "            else:\n",
    "                # Replace any NaN or infinite values with 0.0 to avoid issues in computations.\n",
    "                data = np.nan_to_num(raw_data, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32)\n",
    "\n",
    "            # Compute the total flux by summing all pixel values.\n",
    "            total_flux = float(np.sum(data))\n",
    "\n",
    "            # Extract key metadata from the FITS header.\n",
    "            # If a key is missing, use 'N/A' as a default.\n",
    "            file_name = os.path.basename(fits_file)  # Get just the filename.\n",
    "            date_obs = header.get('DATE-OBS', 'N/A')   # Observation date.\n",
    "            time_obs = header.get('TIME-OBS', 'N/A')     # Observation time.\n",
    "            telescope = header.get('TELESCOP', 'N/A')    # Telescope used.\n",
    "            instrument = header.get('INSTRUME', 'N/A')   # Instrument used.\n",
    "            exposure_time = header.get('EXPTIME', 'N/A')   # Exposure duration.\n",
    "            t_filter = header.get('FILTER', 'N/A')         # Filter information.\n",
    "\n",
    "            # Append a tuple with all the relevant information to file_details.\n",
    "            file_details.append((\n",
    "                file_name,      # Name of the file.\n",
    "                data,           # Processed image data.\n",
    "                total_flux,     # Computed total flux.\n",
    "                date_obs,       # Date of observation.\n",
    "                time_obs,       # Time of observation.\n",
    "                telescope,      # Telescope name.\n",
    "                instrument,     # Instrument name.\n",
    "                exposure_time,  # Exposure time.\n",
    "                t_filter        # Filter used.\n",
    "            ))\n",
    "    except Exception as e:\n",
    "        # If an error occurs while processing a file, print the error and continue with the next file.\n",
    "        print(f\"Error processing file {fits_file}: {e}\")\n",
    "\n",
    "# End the timer for \"Reading FITS metadata\" after processing all files.\n",
    "end_section()\n",
    "\n",
    "# -----------------------------------------\n",
    "# C) Sort the file_details by DATE-OBS & TIME-OBS\n",
    "# -----------------------------------------\n",
    "# Start the timer for sorting the file details.\n",
    "start_section(\"Sort file_details\")\n",
    "\n",
    "# Sort the file_details list based on observation date and time.\n",
    "# The lambda function extracts the date (element index 3) and time (element index 4)\n",
    "# to be used as the sorting keys.\n",
    "file_details = sorted(file_details, key=lambda x: (x[3], x[4]))\n",
    "\n",
    "# End the timer for the sorting operation.\n",
    "end_section()\n",
    "\n",
    "# -----------------------------------------\n",
    "# D) Process images: rotate, find ellipse flux\n",
    "# -----------------------------------------\n",
    "# Start a timer for the entire image processing section.\n",
    "start_section(\"Processing images\")\n",
    "\n",
    "# Initialize an empty list to store computed metrics (summaries) for each image.\n",
    "# Each summary will be a tuple containing various values such as file number,\n",
    "# surface brightness, measured flux values, and observation details.\n",
    "ellipse_summaries = []\n",
    "# The format of each tuple is:\n",
    "# (file_num, f_name, s_brt, e_flux, delta, ef_err, f_avg, fa_err, d_obs, t_obs)\n",
    "\n",
    "# Iterate over each file detail along with its index (starting at 1).\n",
    "for i, (file_name, data, total_flux, date_obs, time_obs,\n",
    "        telescope, instrument, exposure_time, t_filter) in enumerate(file_details, start=1):\n",
    "\n",
    "    # Start a timer for processing the individual file.\n",
    "    start_section(f\"Process {file_name}\")\n",
    "\n",
    "    #========================#\n",
    "    # 1) Get Horizons Data\n",
    "    #========================#\n",
    "    # Initialize variables to hold Horizons data:\n",
    "    # dist_au: Distance from observer in astronomical units.\n",
    "    # s_brt: Surface brightness value from the Horizons API.\n",
    "    dist_au = None\n",
    "    s_brt = None\n",
    "    # Only query Horizons if both observation date and time are valid.\n",
    "    if date_obs != 'N/A' and time_obs != 'N/A':\n",
    "        # Call the get_horizons_data() function to fetch data from JPL Horizons.\n",
    "        # It returns a tuple: (distance in AU, surface brightness value).\n",
    "        dist_au, s_brt_val = get_horizons_data(date_obs, time_obs)\n",
    "        # If a valid surface brightness value is returned, assign it.\n",
    "        if s_brt_val:\n",
    "            s_brt = s_brt_val\n",
    "\n",
    "    #========================#\n",
    "    # 2) Build Original 8-bit Image\n",
    "    #========================#\n",
    "    # Convert the original image data to an 8-bit format suitable for display.\n",
    "    if data.size > 1:\n",
    "        # Compute the 99th percentile of the pixel values to determine a high cutoff.\n",
    "        high_cut_orig = np.percentile(data, 99)\n",
    "        # Clip the data to the range [0, high_cut_orig] to suppress extreme outliers.\n",
    "        clipped_orig = np.clip(data, 0, high_cut_orig)\n",
    "        # Normalize the clipped data to a 0-255 range and convert to 8-bit unsigned integers.\n",
    "        norm_8u_orig = cv2.normalize(clipped_orig, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
    "    else:\n",
    "        # If the data array is invalid or has only one pixel, create a default black image.\n",
    "        norm_8u_orig = np.zeros((1, 1), dtype=np.uint8)\n",
    "\n",
    "    #========================#\n",
    "    # 3) Rotate the image\n",
    "    #========================#\n",
    "    # Set a default rotation angle.\n",
    "    rotate_angle = 0.0\n",
    "    # Compute the center of the image (cx, cy) to use as the pivot for rotation.\n",
    "    cx, cy = (data.shape[1] / 2.0, data.shape[0] / 2.0)\n",
    "\n",
    "    # Apply a low-level threshold to the original 8-bit image to isolate significant features.\n",
    "    tmp_thresh_val = 5  # A low threshold value to separate background from the object.\n",
    "    # cv2.threshold returns a tuple, where the second element is the binary image.\n",
    "    _, tmp_thresh = cv2.threshold(norm_8u_orig, tmp_thresh_val, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # Find external contours in the binary image.\n",
    "    tmp_contours, _ = cv2.findContours(tmp_thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    # If any contours are found, use the largest one to estimate the rotation.\n",
    "    if len(tmp_contours) > 0:\n",
    "        # Select the contour with the maximum area.\n",
    "        largest_tmp_contour = max(tmp_contours, key=cv2.contourArea)\n",
    "        # Fit an ellipse to the largest contour if it has at least 5 points (a requirement for ellipse fitting).\n",
    "        if len(largest_tmp_contour) >= 5:\n",
    "            # cv2.fitEllipse returns the center, size (width, height), and rotation angle of the ellipse.\n",
    "            (temp_cx, temp_cy), (w, h), angle = cv2.fitEllipse(largest_tmp_contour)\n",
    "            # If the ellipse appears vertical (height greater than width), adjust the angle by adding 90°.\n",
    "            if h > w:\n",
    "                angle += 90\n",
    "                # Swap width and height to reflect the adjustment.\n",
    "                w, h = h, w\n",
    "            # Set the rotation angle for the image to the fitted ellipse angle.\n",
    "            rotate_angle = angle\n",
    "\n",
    "    # Rotate the original image data using our rotate_image_full() function.\n",
    "    # This rotates the image around the center (cx, cy) by the computed rotate_angle.\n",
    "    rotated_data = rotate_image_full(data, cx, cy, rotate_angle)\n",
    "    # Compute the total flux in the rotated image by summing its pixel values.\n",
    "    rotated_flux = float(np.sum(rotated_data))\n",
    "\n",
    "    #=====================================================#\n",
    "    # 4) Detect ellipse on rotated data, measure flux\n",
    "    #=====================================================#\n",
    "    # Process the rotated image to detect the elliptical region of interest.\n",
    "    # Compute the 99th percentile to define a high cutoff for the rotated image.\n",
    "    high_cut_rot = np.percentile(rotated_data, 99)\n",
    "    # Clip the rotated data to suppress extreme values.\n",
    "    clipped_rot = np.clip(rotated_data, 0, high_cut_rot)\n",
    "    # Normalize the clipped data to an 8-bit scale.\n",
    "    norm_8u_rot = cv2.normalize(clipped_rot, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
    "\n",
    "    # Apply a binary threshold to the normalized rotated image.\n",
    "    threshold_value = 5\n",
    "    _, thr_rot = cv2.threshold(norm_8u_rot, threshold_value, 255, cv2.THRESH_BINARY)\n",
    "    # Find external contours in the thresholded rotated image.\n",
    "    rot_contours, _ = cv2.findContours(thr_rot, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Initialize variables for flux measurements and error estimates.\n",
    "    ellipse_flux = 0.0  # Flux within the fitted ellipse.\n",
    "    cropped_flux = 0.0  # Flux within the bounding rectangle around the contour.\n",
    "    flux_avg = 0.0      # A computed flux average (flux per unit exposure and distance).\n",
    "    ef_err = 0.0        # Error estimate for ellipse flux (using Poisson statistics).\n",
    "    fa_err = 0.0        # Error estimate for flux average.\n",
    "\n",
    "    # Create an overlay image for visualization by converting the grayscale rotated image\n",
    "    # into a color (BGR) image. This allows colored drawing of contours and ellipses.\n",
    "    overlay_img = cv2.cvtColor(norm_8u_rot, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "    # If contours were detected in the rotated image, proceed to analyze them.\n",
    "    if len(rot_contours) > 0:\n",
    "        # Identify the largest contour by area.\n",
    "        rot_largest_contour = max(rot_contours, key=cv2.contourArea)\n",
    "        # If the largest contour has enough points, fit an ellipse to it.\n",
    "        if len(rot_largest_contour) >= 5:\n",
    "            # Fit an ellipse and retrieve its center, size, and rotation angle.\n",
    "            (rcx, rcy), (rw, rh), rangle = cv2.fitEllipse(rot_largest_contour)\n",
    "            # Adjust the ellipse parameters if it is oriented vertically.\n",
    "            if rh > rw:\n",
    "                rangle += 90\n",
    "                rw, rh = rh, rw\n",
    "            # Generate a boolean mask for pixels inside the fitted ellipse.\n",
    "            mask_ell = elliptical_mask(rotated_data.shape, (rcx, rcy), rw, rh, rangle)\n",
    "            # Sum the flux (pixel values) inside the elliptical region.\n",
    "            ellipse_flux = float(np.sum(rotated_data[mask_ell]))\n",
    "            # Draw the fitted ellipse on the overlay image in green (color code: (0, 255, 0)).\n",
    "            cv2.ellipse(overlay_img, ((rcx, rcy), (rw, rh), rangle), (0, 255, 0), 2)\n",
    "\n",
    "        # Also compute a bounding rectangle around the largest contour.\n",
    "        rx, ry, rW, rH = cv2.boundingRect(rot_largest_contour)\n",
    "        # Crop the rotated image to the area defined by the bounding rectangle.\n",
    "        cropped_data = rotated_data[ry:ry + rH, rx:rx + rW]\n",
    "        # Compute the total flux in the cropped region.\n",
    "        cropped_flux = float(np.sum(cropped_data))\n",
    "        # Draw the bounding rectangle on the overlay image in blue (color code: (255, 0, 0)).\n",
    "        cv2.rectangle(overlay_img, (rx, ry), (rx + rW, ry + rH), (255, 0, 0), 2)\n",
    "\n",
    "        # Estimate the error in the ellipse flux measurement using Poisson statistics.\n",
    "        # The error is approximated as the square root of the ellipse flux.\n",
    "        ef_err = np.sqrt(ellipse_flux) if ellipse_flux > 0 else 0.0\n",
    "\n",
    "        # Compute the flux average if valid exposure time and distance are available.\n",
    "        # The flux average is defined as ellipse_flux divided by (exposure_time * dist_au).\n",
    "        if exposure_time not in [None, 'N/A', 0] and dist_au:\n",
    "            flux_avg = ellipse_flux / (float(exposure_time) * dist_au)\n",
    "            # Propagate the error estimate similarly.\n",
    "            fa_err = ef_err / (float(exposure_time) * dist_au)\n",
    "\n",
    "    #========================#\n",
    "    # 5) Save row in summaries\n",
    "    #========================#\n",
    "    # Append a tuple with all computed parameters for the current image to the ellipse_summaries list.\n",
    "    ellipse_summaries.append((\n",
    "        i,                  # file_num: Sequential number of the file.\n",
    "        file_name,          # f_name: Name of the FITS file.\n",
    "        s_brt,              # s_brt: Surface brightness from Horizons data.\n",
    "        ellipse_flux,       # e_flux: Flux measured within the ellipse.\n",
    "        dist_au,            # delta: Distance from observer (in AU) from Horizons data.\n",
    "        ef_err,             # ef_err: Error in ellipse flux.\n",
    "        flux_avg,           # f_avg: Computed flux average.\n",
    "        fa_err,             # fa_err: Error in flux average.\n",
    "        date_obs,           # d_obs: Observation date.\n",
    "        time_obs            # t_obs: Observation time.\n",
    "    ))\n",
    "\n",
    "    #--------------------------------#\n",
    "    # 6) Print parameter-value table\n",
    "    #--------------------------------#\n",
    "    # Create a list of tuples where each tuple contains a parameter label and its value.\n",
    "    param_values = [\n",
    "        (\"File #\",           str(i)),\n",
    "        (\"File Name\",        file_name),\n",
    "        (\"DATE-OBS\",         date_obs),\n",
    "        (\"TIME-OBS\",         time_obs),\n",
    "        (\"Telescope\",        telescope),\n",
    "        (\"Instrument\",       instrument),\n",
    "        (\"Filter\",           t_filter),\n",
    "        (\"surface brightness (Jupiter)\",\n",
    "         f\"{s_brt:.3f}\" if s_brt else \"N/A\"),\n",
    "        (\"Total Flux\",       f\"{total_flux:.2f}\"),\n",
    "        (\"Flux (Rotated)\",   f\"{rotated_flux:.2f}\"),\n",
    "        (\"Ellipse Flux\",     f\"{ellipse_flux:.2f}\"),\n",
    "        (\"Cropped Flux\",     f\"{cropped_flux:.2f}\"),\n",
    "        (\"Exposure Time\",    f\"{exposure_time}\"),\n",
    "        (\"Delta (AU)\",       f\"{dist_au:.5f}\" if dist_au else \"N/A\"),\n",
    "        (\"Flux Average\",     f\"{flux_avg:.2f}\")\n",
    "    ]\n",
    "\n",
    "    # Print a header and separator for clarity.\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(f\"  Parameter-Value Table for {file_name}\")\n",
    "    print(\"=\" * 60)\n",
    "    # Define a column width for proper alignment of parameter labels.\n",
    "    col_width = 35\n",
    "    # Loop over each parameter and its corresponding value and print them.\n",
    "    for (p, v) in param_values:\n",
    "        print(f\"{p:<{col_width}} {v}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    #--------------------------------#\n",
    "    # 7) Visualization: 4-panel plot\n",
    "    #--------------------------------#\n",
    "    # Create a figure with 4 subplots arranged in a 1x4 grid.\n",
    "    # figsize specifies the overall dimensions of the figure.\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(22, 5))\n",
    "\n",
    "    # Panel (A): Display the original normalized 8-bit image.\n",
    "    axes[0].imshow(norm_8u_orig, cmap='gray', origin='lower')\n",
    "    axes[0].set_title(f\"Normalized 8-bit\\n(Total Flux={total_flux:.2f})\", fontsize=10)\n",
    "    axes[0].axis('off')  # Remove axis ticks and labels.\n",
    "\n",
    "    # Panel (B): Display the rotated image.\n",
    "    axes[1].imshow(norm_8u_rot, cmap='gray', origin='lower')\n",
    "    axes[1].set_title(f\"Rotated Angle={rotate_angle:.1f}°\\n(Flux={rotated_flux:.2f})\", fontsize=10)\n",
    "    axes[1].axis('off')\n",
    "\n",
    "    # Panel (C): Display the overlay image showing the detected contour and fitted ellipse.\n",
    "    axes[2].imshow(overlay_img, origin='lower')\n",
    "    axes[2].set_title(f\"Contour+Ellipse\\n(EllipseFlux={ellipse_flux:.2f})\", fontsize=10)\n",
    "    axes[2].axis('off')\n",
    "\n",
    "    # Panel (D): Display the cropped region.\n",
    "    if cropped_data.size > 1:\n",
    "        # For the cropped data, compute the 99th percentile cutoff.\n",
    "        high_cut_crop = np.percentile(cropped_data, 99)\n",
    "        # Clip and normalize the cropped image to 8-bit.\n",
    "        clipped_crop = np.clip(cropped_data, 0, high_cut_crop)\n",
    "        norm_8u_crop = cv2.normalize(clipped_crop, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
    "        axes[3].imshow(norm_8u_crop, cmap='gray', origin='lower')\n",
    "        axes[3].set_title(f\"Cropped\\n(Flux={cropped_flux:.2f})\", fontsize=10)\n",
    "        axes[3].axis('off')\n",
    "    else:\n",
    "        # If no cropped data is found, fallback to displaying the rotated image.\n",
    "        axes[3].imshow(norm_8u_rot, cmap='gray', origin='lower')\n",
    "        axes[3].set_title(\"Cropped\\n(No contour found)\", fontsize=10)\n",
    "        axes[3].axis('off')\n",
    "\n",
    "    # Set a super title for the figure that displays the current file name.\n",
    "    plt.suptitle(f\"File: {file_name}\", fontsize=12)\n",
    "    # Adjust the layout to prevent overlap between subplots.\n",
    "    plt.tight_layout()\n",
    "    # Display the figure.\n",
    "    plt.show()\n",
    "\n",
    "    # End the timer for processing this specific file.\n",
    "    end_section()  # end \"Process {file_name}\"\n",
    "\n",
    "# End the timer for the \"Processing images\" section.\n",
    "end_section()  # end \"Processing images\"\n",
    "\n",
    "# End the overall script timer.\n",
    "end_section()  # end \"Script Start\"\n",
    "\n",
    "# After all processing is complete, print a structured summary of timing for all sections.\n",
    "print_time_summary()\n",
    "# --------------------------------------------------------------------------------\n",
    "# This final call outputs a table summarizing the start time, end time, and duration\n",
    "# for each timed section, as well as the total runtime of the entire script.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "face4924-36f8-4654-a7f2-c1806bf316db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#-------------------------------\n",
    "# TEST CASE Cell 6 : Debug / Test Horizons\n",
    "#-------------------------------\n",
    "def test_horizons_api(file_index=0):\n",
    "    \"\"\"\n",
    "    Debug function to demonstrate the Horizons API response for one selected FITS file from `file_details`.\n",
    "\n",
    "    Steps:\n",
    "      1) Show relevant FITS header info (DATE-OBS, TIME-OBS).\n",
    "      2) Display the exact input content being sent to Horizons.\n",
    "      3) Retrieve the full text response from Horizons and print it.\n",
    "      4) Demonstrate how the parser extracts delta and S-brt from that text.\n",
    "      5) Compare with the standard get_horizons_data() function output.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    file_index : int\n",
    "        The index into our file_details list. Default=0 means the first file.\n",
    "    \"\"\"\n",
    "    # ------------------------------------------------------------------------------\n",
    "    # Step 0: Basic Safety Check\n",
    "    # Before proceeding, ensure that the global list 'file_details' exists and is populated.\n",
    "    # If it's empty, that means the main analysis hasn't been run, so we abort.\n",
    "    if not file_details:\n",
    "        print(\"No file_details found. Make sure you've run the main analysis cell(s).\")\n",
    "        return\n",
    "\n",
    "    # Check if the provided file_index is within the valid range.\n",
    "    if file_index < 0 or file_index >= len(file_details):\n",
    "        print(f\"Invalid file_index {file_index}; valid range is [0..{len(file_details)-1}]\")\n",
    "        return\n",
    "\n",
    "    # ------------------------------------------------------------------------------\n",
    "    # Step 1: Extract Relevant Information from the Selected FITS File\n",
    "    # Unpack the tuple from file_details corresponding to the specified file_index.\n",
    "    # The tuple contains: (file_name, data, total_flux, date_obs, time_obs, telescope, instrument, exposure_time, t_filter)\n",
    "    (\n",
    "        file_name,      # The name of the FITS file (extracted from the file path).\n",
    "        data,           # The image data array from the FITS file.\n",
    "        total_flux,     # The total flux computed from the image data.\n",
    "        date_obs,       # The observation date (from the FITS header).\n",
    "        time_obs,       # The observation time (from the FITS header).\n",
    "        telescope,      # Telescope information (from the FITS header).\n",
    "        instrument,     # Instrument information (from the FITS header).\n",
    "        exposure_time,  # Exposure time (from the FITS header).\n",
    "        t_filter        # Filter used (from the FITS header).\n",
    "    ) = file_details[file_index]\n",
    "\n",
    "    # Print out the extracted FITS file information in a formatted manner.\n",
    "    print(\"=== Selected FITS File Information ===\")\n",
    "    print(f\"Index:         {file_index}\")\n",
    "    print(f\"File Name:     {file_name}\")\n",
    "    print(f\"DATE-OBS:      {date_obs}\")\n",
    "    print(f\"TIME-OBS:      {time_obs}\")\n",
    "    print(f\"Telescope:     {telescope}\")\n",
    "    print(f\"Instrument:    {instrument}\")\n",
    "    print(f\"Filter:        {t_filter}\")\n",
    "    print(f\"Exposure Time: {exposure_time}\")\n",
    "    print(\"======================================\\n\")\n",
    "\n",
    "    # ------------------------------------------------------------------------------\n",
    "    # Step 2: Display the Exact Content Being Sent to Horizons\n",
    "    # Call the create_input_content() function with the observation date and time.\n",
    "    # This function returns a multi-line string that contains all the commands and parameters\n",
    "    # formatted for the Horizons API request.\n",
    "    horizons_input = create_input_content(date_obs, time_obs)\n",
    "    \n",
    "    # Print out the generated input content so that the user can inspect exactly what is sent.\n",
    "    print(\"=== Content Sent to Horizons API ===\")\n",
    "    print(horizons_input)\n",
    "    print(\"=====================================\\n\")\n",
    "\n",
    "    # ------------------------------------------------------------------------------\n",
    "    # Step 3: Retrieve and Print the Full Text Response from Horizons\n",
    "    # Import the requests module here (if not already imported globally) for making HTTP requests.\n",
    "    import requests\n",
    "\n",
    "    # Define the URL endpoint for the Horizons API.\n",
    "    horizons_url_debug = 'https://ssd.jpl.nasa.gov/api/horizons_file.api'\n",
    "\n",
    "    # Send a POST request to the Horizons API.\n",
    "    # Parameters:\n",
    "    #   - data={'format': 'text'} requests the response in plain text format.\n",
    "    #   - files={'input': ('input.txt', horizons_input)} attaches the input content as a file.\n",
    "    debug_response = requests.post(\n",
    "        horizons_url_debug,\n",
    "        data={'format': 'text'},\n",
    "        files={'input': ('input.txt', horizons_input)}\n",
    "    )\n",
    "\n",
    "    # Check if the request was successful by verifying the HTTP status code.\n",
    "    if debug_response.status_code != 200:\n",
    "        # If the request failed (status code is not 200), print an error message.\n",
    "        print(f\"Failed Horizons request: {debug_response.status_code}\")\n",
    "        return\n",
    "\n",
    "    # If successful, retrieve the full text response from the API.\n",
    "    horizons_full_text = debug_response.text\n",
    "\n",
    "    # Print the full text response for debugging purposes.\n",
    "    print(\"=== Full Horizons API Response ===\")\n",
    "    print(horizons_full_text)\n",
    "    print(\"==================================\\n\")\n",
    "\n",
    "    # ------------------------------------------------------------------------------\n",
    "    # Step 4: Demonstrate How to Parse Out 'delta' and 'S-brt' from the Response\n",
    "    # Split the full text response into individual lines.\n",
    "    lines = horizons_full_text.splitlines()\n",
    "    \n",
    "    try:\n",
    "        # Locate the line indices where the ephemeris data begins and ends.\n",
    "        # '$$SOE' marks the Start Of Ephemeris and '$$EOE' marks the End Of Ephemeris.\n",
    "        start_index = next(i for i, line in enumerate(lines) if '$$SOE' in line)\n",
    "        end_index   = next(i for i, line in enumerate(lines) if '$$EOE' in line)\n",
    "    except StopIteration:\n",
    "        # If either marker is not found, parsing cannot proceed.\n",
    "        print(\"Could not find $$SOE/$$EOE in Horizons output—parsing aborted.\")\n",
    "        return\n",
    "\n",
    "    # Extract only the lines that contain the ephemeris data.\n",
    "    ephem_lines = lines[start_index+1 : end_index]\n",
    "\n",
    "    # Initialize variables to hold the extracted observer distance and surface brightness.\n",
    "    dist_au_extracted = None  # Will store the extracted delta (distance in AU).\n",
    "    s_brt_extracted   = None  # Will store the extracted surface brightness (S-brt).\n",
    "    relevant_line     = None  # Will store the line used for parsing (for display purposes).\n",
    "\n",
    "    # Iterate through each line in the ephemeris block.\n",
    "    for ln in ephem_lines:\n",
    "        ln = ln.strip()  # Remove leading/trailing whitespace.\n",
    "        if not ln:\n",
    "            continue  # Skip empty lines.\n",
    "        # Split the line into tokens (individual data columns).\n",
    "        tokens = ln.split()\n",
    "        # Save the current line as the one we are using for parsing.\n",
    "        relevant_line = ln\n",
    "        # The expected format is that tokens[3] holds S-brt and tokens[4] holds delta.\n",
    "        try:\n",
    "            s_brt_extracted = float(tokens[3])  # Convert S-brt to a float.\n",
    "            dist_au_extracted = float(tokens[4])  # Convert delta to a float.\n",
    "        except:\n",
    "            # If conversion fails, simply pass; in a robust implementation, you might log this.\n",
    "            pass\n",
    "        # Break after processing the first non-empty line for demonstration purposes.\n",
    "        break\n",
    "\n",
    "    # Print out the parsing demonstration results.\n",
    "    print(\"=== Parsing Ephemeris Lines ===\")\n",
    "    if relevant_line:\n",
    "        print(f\"Line used for parsing:\\n{relevant_line}\\n\")\n",
    "        print(f\"Extracted distance (delta) [OLD WAY]: {dist_au_extracted}\")\n",
    "        print(f\"Extracted surface brightness (S-brt) [OLD WAY]: {s_brt_extracted}\")\n",
    "    else:\n",
    "        print(\"No valid line found with ephemeris data.\")\n",
    "    print(\"================================\\n\")\n",
    "\n",
    "    # ------------------------------------------------------------------------------\n",
    "    # Step 5: Compare with the Official get_horizons_data() Function Output\n",
    "    # Call the standard get_horizons_data() function to retrieve parsed values.\n",
    "    debug_delta, debug_sbrt = get_horizons_data(date_obs, time_obs)\n",
    "    # Print the results from get_horizons_data() for comparison.\n",
    "    print(\"=== Comparison with get_horizons_data() ===\")\n",
    "    print(f\"[CORRECT] get_horizons_data returned delta = {debug_delta}, s_brt = {debug_sbrt}\")\n",
    "    print(\"===========================================\\n\")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Example usage: Call the test function for the first file (file_index=0)\n",
    "test_horizons_api(file_index=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607c59c3-3520-41c1-9161-8c850d73f58a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------\n",
    "# Cell 7 : Final Data Summaries & Plots\n",
    "# --------------------------------------------------------------\n",
    "# This cell is responsible for building a summary table from the processed\n",
    "# image data stored in the global variable `ellipse_summaries` and printing it\n",
    "# with nicely formatted, auto-adjusted, centered columns.\n",
    "# It also includes a helper function `print_dynamic_table` to handle the printing.\n",
    "\n",
    "def print_dynamic_table(rows):\n",
    "    \"\"\"\n",
    "    Print a table with auto-adjusted column widths and centered text.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    rows : list of lists\n",
    "        A list where each sub-list represents one row of the table.\n",
    "        Typically, the first row (rows[0]) is the header row containing column titles.\n",
    "\n",
    "    Process:\n",
    "    --------\n",
    "    1. Convert every item in every row to a string (ensuring uniform text output).\n",
    "    2. Determine the maximum string length for each column so that the printed\n",
    "       table has columns with appropriate widths.\n",
    "    3. Print the header row with text centered in each column.\n",
    "    4. Print a separator line below the header.\n",
    "    5. Print the remaining rows (data rows) with each cell centered.\n",
    "    \"\"\"\n",
    "    # --------------------------------------------------------------------------\n",
    "    # Step 1: Convert all items in the table to strings.\n",
    "    # This ensures that regardless of the original data type (int, float, etc.),\n",
    "    # each item will be printed as text.\n",
    "    str_rows = [[str(item) for item in row] for row in rows]\n",
    "    # The above is a nested list comprehension that iterates over each row in 'rows',\n",
    "    # and then over each item in the row, converting it to a string.\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    # Step 2: Determine the maximum width needed for each column.\n",
    "    # We assume all rows have the same number of columns, so we take the length of the first row.\n",
    "    num_cols = len(str_rows[0])\n",
    "    # Create a list 'col_widths' where each element corresponds to the maximum string length\n",
    "    # found in that column across all rows.\n",
    "    col_widths = [max(len(row[i]) for row in str_rows) for i in range(num_cols)]\n",
    "    # For each column index i (ranging from 0 to num_cols-1), we:\n",
    "    #   - Iterate over each row in 'str_rows'\n",
    "    #   - Calculate the length of the string at position i in that row\n",
    "    #   - Take the maximum length found; this is the width for column i.\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    # Step 3: Print the header row (which is assumed to be the first row in the table).\n",
    "    header_row = str_rows[0]\n",
    "    # Build a separator line (a list of strings) for each column by repeating '-' for the column's width.\n",
    "    sep_line = [\"-\" * col_widths[i] for i in range(num_cols)]\n",
    "    # Print the header row with each cell centered within its column.\n",
    "    # The join() function is used to combine the centered cells with two spaces in between.\n",
    "    print(\"  \".join(cell.center(col_widths[i]) for i, cell in enumerate(header_row)))\n",
    "    # Print the separator line below the header row.\n",
    "    print(\"  \".join(sep_line))\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    # Step 4: Print each data row (all rows except the first one) with centered text.\n",
    "    for row in str_rows[1:]:\n",
    "        # For each row, center each cell within the predetermined column width,\n",
    "        # then join them together with two spaces between cells.\n",
    "        print(\"  \".join(cell.center(col_widths[i]) for i, cell in enumerate(row)))\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# 1. BUILD THE TABLE\n",
    "# ------------------------------\n",
    "# We now build the data for our table, starting with the header row.\n",
    "# This header row defines the titles for each column.\n",
    "table_rows = [\n",
    "    [\n",
    "        \"File #\",         # Column 1: File number (sequential index).\n",
    "        \"File Name\",      # Column 2: Name of the FITS file.\n",
    "        \"SB (Jupiter)\",   # Column 3: Surface brightness for Jupiter (from Horizons API).\n",
    "        \"Ellipse Flux\",   # Column 4: Flux measured within the detected ellipse.\n",
    "        \"delta\",          # Column 5: Observer distance (delta) from Horizons API.\n",
    "        \"Error Bars EF\",  # Column 6: Error estimate for the ellipse flux.\n",
    "        \"Flux Average\",   # Column 7: Computed flux average.\n",
    "        \"Error Bars FA\",  # Column 8: Error estimate for the flux average.\n",
    "        \"DATE-OBS\",       # Column 9: Observation date.\n",
    "        \"TIME-OBS\"        # Column 10: Observation time.\n",
    "    ]\n",
    "]\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Now we loop over the global 'ellipse_summaries' list to fill in the data rows.\n",
    "# Each entry in ellipse_summaries is a tuple with the following structure:\n",
    "# (file_num, f_name, s_brt, e_flux, delta, ef_err, f_avg, fa_err, d_obs, t_obs)\n",
    "for (file_num, f_name, s_brt, e_flux, delta, ef_err, f_avg, fa_err, d_obs, t_obs) in ellipse_summaries:\n",
    "    \n",
    "    # ------------------------------------------------------------------------------\n",
    "    # Convert each numeric value to a formatted string.\n",
    "    # If the value is None or evaluates to False, substitute with \"N/A\".\n",
    "    # This ensures that missing or invalid data is clearly indicated.\n",
    "    s_brt_str   = f\"{s_brt:.3f}\"   if s_brt   else \"N/A\"  # Format surface brightness to 3 decimal places.\n",
    "    e_flux_str  = f\"{e_flux:.2f}\"   if e_flux  else \"N/A\"  # Format ellipse flux to 2 decimal places.\n",
    "    delta_str   = f\"{delta:.5f}\"    if delta   else \"N/A\"  # Format delta (observer distance) to 5 decimal places.\n",
    "    ef_err_str  = f\"{ef_err:.2f}\"   if ef_err  else \"N/A\"  # Format error for ellipse flux to 2 decimal places.\n",
    "    f_avg_str   = f\"{f_avg:.3f}\"    if f_avg   else \"N/A\"  # Format flux average to 3 decimal places.\n",
    "    fa_err_str  = f\"{fa_err:.3f}\"   if fa_err  else \"N/A\"  # Format error for flux average to 3 decimal places.\n",
    "\n",
    "    # ------------------------------------------------------------------------------\n",
    "    # Build a new row (a list of strings) corresponding to the current entry.\n",
    "    # The new row has the same order of fields as defined in the header.\n",
    "    new_row = [\n",
    "        str(file_num),   # Convert file number to string.\n",
    "        f_name,          # File name (already a string).\n",
    "        s_brt_str,       # Formatted surface brightness.\n",
    "        e_flux_str,      # Formatted ellipse flux.\n",
    "        delta_str,       # Formatted observer distance.\n",
    "        ef_err_str,      # Formatted error in ellipse flux.\n",
    "        f_avg_str,       # Formatted flux average.\n",
    "        fa_err_str,      # Formatted error in flux average.\n",
    "        d_obs,           # Observation date.\n",
    "        t_obs            # Observation time.\n",
    "    ]\n",
    "    # Append the newly created row to the table_rows list.\n",
    "    table_rows.append(new_row)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Finally, print the entire table using the helper function.\n",
    "# This function will auto-adjust the column widths and center the text.\n",
    "print_dynamic_table(table_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d6eaef-85de-47c9-b9a6-fc4a076187d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Cell 8 : The sun graph (Modified for dot markers for apparent magnitude)\n",
    "# ------------------------------\n",
    "\n",
    "# Import the requests module to handle HTTP requests.\n",
    "import requests\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Define the base URL for the Horizons API.\n",
    "# This URL is used to request ephemeris data in a file-based text format.\n",
    "url = \"https://ssd.jpl.nasa.gov/api/horizons_file.api\"\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Build the input content that will be sent to the Horizons API.\n",
    "# This multi-line string contains specific commands and parameters that instruct\n",
    "# Horizons on what data to return.\n",
    "#\n",
    "# In this example:\n",
    "#   - COMMAND='10' selects the Sun (object number 10 in Horizons).\n",
    "#   - CENTER='500@-48' specifies the observer's location (here, a particular center).\n",
    "#   - MAKE_EPHEM='YES' tells Horizons to generate an ephemeris.\n",
    "#   - EPHEM_TYPE='OBSERVER' requests observer-centric ephemeris data.\n",
    "#   - START_TIME and STOP_TIME define the date range for the ephemeris.\n",
    "#   - STEP_SIZE='60m' sets the time step between ephemeris entries (every 60 minutes).\n",
    "#   - QUANTITIES='9,20' selects specific output quantities:\n",
    "#         * Quantity 9: Typically the apparent magnitude.\n",
    "#         * Quantity 20: Typically the observer distance (delta) and range rate.\n",
    "#   - OUT_UNITS='KM-S' sets the output units (here kilometers and seconds).\n",
    "#   - CSV_FORMAT='NO' specifies that the output should not be in CSV format.\n",
    "#   - The special markers !$$SOF and !$$EOF denote the Start and End Of File.\n",
    "input_content = \"\"\"\n",
    "!$$SOF\n",
    "COMMAND='10'\n",
    "CENTER='500@-48'\n",
    "MAKE_EPHEM='YES'\n",
    "EPHEM_TYPE='OBSERVER'\n",
    "START_TIME='2015-01-19 00:00:00'\n",
    "STOP_TIME='2019-07-21 23:59:00'\n",
    "STEP_SIZE='60m'\n",
    "QUANTITIES='9,20'\n",
    "OUT_UNITS='KM-S'\n",
    "CSV_FORMAT='NO'\n",
    "!$$EOF\n",
    "\"\"\"\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Define additional parameters for the POST request.\n",
    "# Here we indicate that we want the response in plain text format.\n",
    "params = {\"format\": \"text\"}\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Send the POST request to the Horizons API.\n",
    "# The request is composed of:\n",
    "#   - 'url': The API endpoint.\n",
    "#   - 'data': A dictionary with additional parameters (format=text).\n",
    "#   - 'files': The input content is sent as if it were a file named 'input.txt'.\n",
    "response = requests.post(url, data=params, files={'input': ('input.txt', input_content)})\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Split the response text into individual lines.\n",
    "# This makes it easier to search for the ephemeris data block.\n",
    "lines = response.text.splitlines()\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Locate the ephemeris data block within the response text.\n",
    "# The block of data is delimited by lines containing \"$$SOE\" (Start Of Ephemeris)\n",
    "# and \"$$EOE\" (End Of Ephemeris).\n",
    "start_idx = None  # Will hold the index of the line containing \"$$SOE\"\n",
    "end_idx = None    # Will hold the index of the line containing \"$$EOE\"\n",
    "for i, line in enumerate(lines):\n",
    "    if \"$$SOE\" in line:\n",
    "        start_idx = i  # Found the start marker\n",
    "    if \"$$EOE\" in line:\n",
    "        end_idx = i    # Found the end marker\n",
    "        break        # Once the end marker is found, exit the loop\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Prepare empty lists to store the parsed data:\n",
    "#   - timestamps: The date and time of each ephemeris entry.\n",
    "#   - apmag_vals: The apparent magnitude values of the Sun.\n",
    "#   - delta_vals: The observer distance (delta) values.\n",
    "timestamps = []\n",
    "apmag_vals = []\n",
    "delta_vals = []\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Check that both start and end markers were found before proceeding.\n",
    "if start_idx is not None and end_idx is not None:\n",
    "    # Iterate over the lines within the ephemeris data block.\n",
    "    # We skip the marker line itself by starting at start_idx+1, and we stop at end_idx.\n",
    "    for line in lines[start_idx+1:end_idx]:\n",
    "        line = line.strip()  # Remove any leading/trailing whitespace.\n",
    "        if not line:\n",
    "            continue  # Skip empty lines to avoid processing blank data.\n",
    "        tokens = line.split()  # Split the line into individual data tokens based on whitespace.\n",
    "        # Expected token positions based on QUANTITIES='9,20' are:\n",
    "        #   tokens[0]: Date (string)\n",
    "        #   tokens[1]: Time (string)\n",
    "        #   tokens[2]: Apparent magnitude (APmag)\n",
    "        #   tokens[4]: Observer distance (delta)\n",
    "        if len(tokens) >= 5:\n",
    "            # Concatenate the date and time tokens into one datetime string.\n",
    "            dt_str = tokens[0] + \" \" + tokens[1]\n",
    "            try:\n",
    "                # Convert the datetime string into a pandas datetime object.\n",
    "                dt = pd.to_datetime(dt_str)\n",
    "                # Convert the apparent magnitude and delta tokens into floats.\n",
    "                apmag = float(tokens[2])  # Apparent magnitude of the Sun.\n",
    "                delta = float(tokens[4])  # Observer distance.\n",
    "                # Append the parsed values to their corresponding lists.\n",
    "                timestamps.append(dt)\n",
    "                apmag_vals.append(apmag)\n",
    "                delta_vals.append(delta)\n",
    "            except Exception as e:\n",
    "                # If parsing fails for any reason (e.g., conversion error), print an error message.\n",
    "                print(f\"Skipping line due to parse error: {line}\")\n",
    "                continue  # Move on to the next line.\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Once parsing is complete, create a pandas DataFrame to conveniently work with the data.\n",
    "# The DataFrame will contain columns for datetime, apparent magnitude (apmag), and delta.\n",
    "df = pd.DataFrame({\n",
    "    \"datetime\": timestamps,\n",
    "    \"apmag\": apmag_vals,\n",
    "    \"delta\": delta_vals\n",
    "})\n",
    "\n",
    "# Sort the DataFrame by the datetime column to ensure the data is in chronological order.\n",
    "df.sort_values(\"datetime\", inplace=True)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Calculate the number of days since the first ephemeris entry.\n",
    "# This is useful for plotting the data on an x-axis that represents elapsed time.\n",
    "start_time = df[\"datetime\"].min()  # The earliest datetime in the DataFrame.\n",
    "# Compute a new column \"days_since_start\" as the difference in seconds divided by 86400 (seconds per day).\n",
    "df[\"days_since_start\"] = (df[\"datetime\"] - start_time).dt.total_seconds() / 86400.0\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Create a dual-axis plot to display both delta and apparent magnitude versus time.\n",
    "# The left y-axis will show delta (observer distance), and the right y-axis will show\n",
    "# apparent magnitude (apmag) with dot markers.\n",
    "fig, ax_left = plt.subplots(figsize=(12, 6))\n",
    "# Create a secondary y-axis sharing the same x-axis.\n",
    "ax_right = ax_left.twinx()\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Plot delta as a blue line on the left y-axis.\n",
    "ax_left.plot(df[\"days_since_start\"], df[\"delta\"], color='blue', label='Delta')\n",
    "ax_left.set_xlabel(\"Days Since Start\")             # Label for the x-axis.\n",
    "ax_left.set_ylabel(\"Delta\", color='blue')            # Label for the left y-axis.\n",
    "ax_left.tick_params(axis='y', labelcolor='blue')     # Set the tick labels color for clarity.\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Plot apparent magnitude as red scatter points on the right y-axis.\n",
    "# The marker size 's' is set to 0.5 to create tiny dots.\n",
    "ax_right.scatter(df[\"days_since_start\"], df[\"apmag\"], color='red', s=0.5, label='Apparent Mag')\n",
    "ax_right.set_ylabel(\"Apparent Magnitude\", color='red')  # Label for the right y-axis.\n",
    "ax_right.tick_params(axis='y', labelcolor='red')         # Set tick labels to red.\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Add a title to the overall plot that explains the displayed data.\n",
    "plt.title(\"Apparent Magnitude (Right Axis) and Delta (Left Axis) vs. Time\")\n",
    "\n",
    "# Display the plot.\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941c75c7-acab-4a4f-8ce8-1062cb4d82af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Cell 9 : 2. TIME vs. DELTA, Average Flux, and SB vs. Time Plots (Side by Side)\n",
    "# ------------------------------\n",
    "\n",
    "# Import the pandas library for data manipulation and matplotlib.pyplot for plotting.\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# This cell builds time-series data for three different metrics extracted from the\n",
    "# global variable `ellipse_summaries`:\n",
    "#   1) Distance (delta) vs. time\n",
    "#   2) Average Flux vs. time\n",
    "#   3) Surface Brightness vs. time\n",
    "#\n",
    "# Each of these is first stored in a list of records, then converted into a pandas\n",
    "# DataFrame for sorting, manipulation, and plotting.\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "# ================================\n",
    "# 1) Build Distance (delta) vs. Time Data\n",
    "# ================================\n",
    "\n",
    "# Initialize an empty list to store records for delta (observer distance) data.\n",
    "# Each record will be a tuple: (datetime, delta)\n",
    "delta_records = []\n",
    "\n",
    "# Loop over each entry in the global ellipse_summaries list.\n",
    "# Each entry is a tuple with the following structure:\n",
    "# (file_num, f_name, s_brt, e_flux, dist_au, ef_err, f_avg, fa_err, d_obs, t_obs)\n",
    "for (file_num, f_name, s_brt, e_flux, dist_au, ef_err, f_avg, fa_err, d_obs, t_obs) in ellipse_summaries:\n",
    "    # Only process records that have valid observation date and time.\n",
    "    if d_obs != \"N/A\" and t_obs != \"N/A\":\n",
    "        # Combine the date and time strings into a single datetime string.\n",
    "        dt_str = d_obs + \" \" + t_obs\n",
    "        # Parse the datetime string into a pandas datetime object.\n",
    "        dt_parsed = pd.to_datetime(dt_str)\n",
    "        # Append a tuple (datetime, delta) to the list.\n",
    "        delta_records.append((dt_parsed, dist_au))\n",
    "\n",
    "# Create a DataFrame from the list of delta records.\n",
    "# The DataFrame will have two columns: \"datetime\" and \"delta_au\".\n",
    "df_delta = pd.DataFrame(delta_records, columns=[\"datetime\", \"delta_au\"])\n",
    "# Sort the DataFrame by the datetime column to ensure the records are in chronological order.\n",
    "df_delta.sort_values(\"datetime\", inplace=True)\n",
    "\n",
    "# Convert the datetime column to \"days since first observation\":\n",
    "#   - Find the earliest datetime value.\n",
    "#   - Subtract that value from each datetime to compute elapsed time in seconds.\n",
    "#   - Convert seconds to days by dividing by 86400 (number of seconds in a day).\n",
    "start_time = df_delta[\"datetime\"].min()\n",
    "df_delta[\"days_since_start\"] = (df_delta[\"datetime\"] - start_time).dt.total_seconds() / 86400.0\n",
    "\n",
    "# ================================\n",
    "# 2) Build Average Flux vs. Time Data\n",
    "# ================================\n",
    "\n",
    "# Initialize an empty list to store records for average flux data.\n",
    "# Each record will be a tuple: (datetime, f_avg)\n",
    "flux_records = []\n",
    "\n",
    "# Loop over each entry in ellipse_summaries, similar to the previous block.\n",
    "for (file_num, f_name, s_brt, e_flux, dist_au, ef_err, f_avg, fa_err, d_obs, t_obs) in ellipse_summaries:\n",
    "    # Only process records with valid observation date and time.\n",
    "    if d_obs != \"N/A\" and t_obs != \"N/A\":\n",
    "        # Combine the date and time into one string.\n",
    "        dt_str = d_obs + \" \" + t_obs\n",
    "        # Parse the combined string into a datetime object.\n",
    "        dt_parsed = pd.to_datetime(dt_str)\n",
    "        # Append a tuple (datetime, average flux) to the list.\n",
    "        flux_records.append((dt_parsed, f_avg))\n",
    "\n",
    "# Create a DataFrame from the flux records with columns \"datetime\" and \"f_avg\".\n",
    "df_flux = pd.DataFrame(flux_records, columns=[\"datetime\", \"f_avg\"])\n",
    "# Sort the DataFrame chronologically.\n",
    "df_flux.sort_values(\"datetime\", inplace=True)\n",
    "# Compute days since the first observation for the flux data.\n",
    "start_time = df_flux[\"datetime\"].min()  # Note: This start_time may be different from df_delta if records differ.\n",
    "df_flux[\"days_since_start\"] = (df_flux[\"datetime\"] - start_time).dt.total_seconds() / 86400.0\n",
    "\n",
    "# ================================\n",
    "# 3) Build Surface Brightness vs. Time Data\n",
    "# ================================\n",
    "\n",
    "# Initialize an empty list to store records for surface brightness data.\n",
    "# Each record will be a tuple: (datetime, s_brt)\n",
    "sb_records = []\n",
    "\n",
    "# Loop over each entry in ellipse_summaries.\n",
    "for (file_num, f_name, s_brt, e_flux, dist_au, ef_err, f_avg, fa_err, d_obs, t_obs) in ellipse_summaries:\n",
    "    # Process only if observation date and time are valid.\n",
    "    if d_obs != \"N/A\" and t_obs != \"N/A\":\n",
    "        # Create a datetime string by concatenating the date and time.\n",
    "        dt_str = d_obs + \" \" + t_obs\n",
    "        # Convert the datetime string into a pandas datetime object.\n",
    "        dt_parsed = pd.to_datetime(dt_str)\n",
    "        # Append a tuple (datetime, surface brightness) to the list.\n",
    "        sb_records.append((dt_parsed, s_brt))\n",
    "\n",
    "# Create a DataFrame from the surface brightness records with columns \"datetime\" and \"s_brt\".\n",
    "df_sb = pd.DataFrame(sb_records, columns=[\"datetime\", \"s_brt\"])\n",
    "# Sort the DataFrame by datetime.\n",
    "df_sb.sort_values(\"datetime\", inplace=True)\n",
    "# Compute the days since the first observation for the surface brightness data.\n",
    "start_time = df_sb[\"datetime\"].min()\n",
    "df_sb[\"days_since_start\"] = (df_sb[\"datetime\"] - start_time).dt.total_seconds() / 86400.0\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Create a 3-panel side-by-side figure to display the three metrics as separate subplots.\n",
    "# The figure will have 3 columns (one per metric) and a specified figure size.\n",
    "fig, axs = plt.subplots(ncols=3, figsize=(18, 6))\n",
    "\n",
    "# ------------------------------------------\n",
    "# Plot 1: Delta (Distance) vs. Time\n",
    "# ------------------------------------------\n",
    "# Create a scatter plot using the \"days_since_start\" on the x-axis and \"delta_au\" on the y-axis.\n",
    "axs[0].scatter(df_delta[\"days_since_start\"], df_delta[\"delta_au\"], color='black', alpha=0.75)\n",
    "axs[0].set_xlabel(\"Days Since Start\")       # Set the x-axis label.\n",
    "axs[0].set_ylabel(\"Delta (AU)\")               # Set the y-axis label.\n",
    "axs[0].set_title(\"Distance (Delta) vs. Time (Days)\")  # Set the subplot title.\n",
    "\n",
    "# ------------------------------------------\n",
    "# Plot 2: Average Flux vs. Time\n",
    "# ------------------------------------------\n",
    "# Create a scatter plot for average flux.\n",
    "axs[1].scatter(df_flux[\"days_since_start\"], df_flux[\"f_avg\"], color='blue', alpha=0.75)\n",
    "axs[1].set_xlabel(\"Days Since Start\")         # Label for the x-axis.\n",
    "axs[1].set_ylabel(\"Average Flux\")             # Label for the y-axis.\n",
    "axs[1].set_title(\"Average Flux vs. Time (Days)\")  # Set the subplot title.\n",
    "\n",
    "# ------------------------------------------\n",
    "# Plot 3: Surface Brightness vs. Time\n",
    "# ------------------------------------------\n",
    "# Create a scatter plot for surface brightness.\n",
    "axs[2].scatter(df_sb[\"days_since_start\"], df_sb[\"s_brt\"], color='green', alpha=0.75)\n",
    "axs[2].set_xlabel(\"Days Since Start\")         # Label for the x-axis.\n",
    "axs[2].set_ylabel(\"Surface Brightness\")       # Label for the y-axis.\n",
    "axs[2].set_title(\"Surface Brightness vs. Time (Days)\")  # Set the subplot title.\n",
    "\n",
    "# Adjust layout spacing to prevent overlapping of subplots.\n",
    "plt.tight_layout()\n",
    "# Display the 3-panel figure.\n",
    "plt.show()\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Additional Overlay Plots with Dual y-Axes:\n",
    "# These plots overlay two metrics on the same x-axis (Days Since Start)\n",
    "# but use two different y-axes (one on the left and one on the right).\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Overlay i): Delta & Average Flux vs. Time\n",
    "# -----------------------------------------------------------\n",
    "# Create a new figure for the overlay plot.\n",
    "fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "# Plot delta as a scatter plot on the primary y-axis.\n",
    "ax1.scatter(df_delta[\"days_since_start\"], df_delta[\"delta_au\"], color='black', alpha=0.75, label='Delta')\n",
    "ax1.set_xlabel(\"Days Since Start\")         # Set x-axis label.\n",
    "ax1.set_ylabel(\"Delta (AU)\", color='black')  # Set primary y-axis label.\n",
    "ax1.tick_params(axis='y', labelcolor='black')  # Color the y-axis ticks for clarity.\n",
    "\n",
    "# Create a secondary y-axis that shares the same x-axis.\n",
    "ax1_right = ax1.twinx()\n",
    "# Plot average flux on the secondary y-axis.\n",
    "ax1_right.scatter(df_flux[\"days_since_start\"], df_flux[\"f_avg\"], color='blue', alpha=0.75, label='Average Flux')\n",
    "ax1_right.set_ylabel(\"Average Flux\", color='blue')  # Set secondary y-axis label.\n",
    "ax1_right.tick_params(axis='y', labelcolor='blue')   # Color the ticks accordingly.\n",
    "\n",
    "# Set a title for the overlay plot.\n",
    "plt.title(\"Overlay: Delta and Average Flux vs. Time\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Overlay ii): Delta & Surface Brightness vs. Time\n",
    "# -----------------------------------------------------------\n",
    "# Create a new figure for the next overlay.\n",
    "fig, ax2 = plt.subplots(figsize=(12, 6))\n",
    "# Plot delta (distance) on the primary y-axis.\n",
    "ax2.scatter(df_delta[\"days_since_start\"], df_delta[\"delta_au\"], color='black', alpha=0.75, label='Delta')\n",
    "ax2.set_xlabel(\"Days Since Start\")\n",
    "ax2.set_ylabel(\"Delta (AU)\", color='black')\n",
    "ax2.tick_params(axis='y', labelcolor='black')\n",
    "\n",
    "# Create a secondary y-axis for surface brightness.\n",
    "ax2_right = ax2.twinx()\n",
    "# Plot surface brightness on the secondary y-axis.\n",
    "ax2_right.scatter(df_sb[\"days_since_start\"], df_sb[\"s_brt\"], color='green', alpha=0.75, label='Surface Brightness')\n",
    "ax2_right.set_ylabel(\"Surface Brightness\", color='green')\n",
    "ax2_right.tick_params(axis='y', labelcolor='green')\n",
    "\n",
    "# Set the title and adjust layout.\n",
    "plt.title(\"Overlay: Delta and Surface Brightness vs. Time\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Overlay iii): Average Flux & Surface Brightness vs. Time\n",
    "# -----------------------------------------------------------\n",
    "# Create a new figure for the third overlay plot.\n",
    "fig, ax3 = plt.subplots(figsize=(12, 6))\n",
    "# Plot average flux on the primary y-axis.\n",
    "ax3.scatter(df_flux[\"days_since_start\"], df_flux[\"f_avg\"], color='blue', alpha=0.75, label='Average Flux')\n",
    "ax3.set_xlabel(\"Days Since Start\")\n",
    "ax3.set_ylabel(\"Average Flux\", color='blue')\n",
    "ax3.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "# Create a secondary y-axis for surface brightness.\n",
    "ax3_right = ax3.twinx()\n",
    "# Plot surface brightness on the secondary y-axis.\n",
    "ax3_right.scatter(df_sb[\"days_since_start\"], df_sb[\"s_brt\"], color='green', alpha=0.75, label='Surface Brightness')\n",
    "ax3_right.set_ylabel(\"Surface Brightness\", color='green')\n",
    "ax3_right.tick_params(axis='y', labelcolor='green')\n",
    "\n",
    "# Set title and adjust the layout.\n",
    "plt.title(\"Overlay: Average Flux and Surface Brightness vs. Time\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f01a01-90f0-472f-80bd-9c599797424d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Cell 10 : quadruple overlay\n",
    "# ------------------------------\n",
    "\n",
    "# Import required modules for advanced multi-axis plotting:\n",
    "#   - host_subplot: Allows creating a host axes with attached parasite axes.\n",
    "#   - mpl_toolkits.axisartist (imported as AA): Provides support for customizing axis artists.\n",
    "from mpl_toolkits.axes_grid1 import host_subplot\n",
    "import mpl_toolkits.axisartist as AA\n",
    "\n",
    "# Import matplotlib.pyplot for plotting functions and numpy for numerical operations.\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# The goal of this cell is to create a combined multi-axis plot that overlays\n",
    "# several time-series data sets on a single time axis (x-axis) while assigning\n",
    "# multiple y-axes (each with its own scale and label). The overlays include:\n",
    "#\n",
    "#   - Sun Delta (line plot, taken from DataFrame \"df\")\n",
    "#   - Sun Apparent Magnitude (scatter plot, from DataFrame \"df\")\n",
    "#   - Jupiter Delta (scatter plot, from DataFrame \"df_delta\")\n",
    "#   - Jupiter Average Flux (scatter plot, from DataFrame \"df_flux\")\n",
    "#   - Jupiter Surface Brightness (scatter plot, from DataFrame \"df_sb\")\n",
    "#\n",
    "# Note: The DataFrames (df, df_delta, df_flux, df_sb) are assumed to be defined earlier.\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "# Create a new figure with a specified size.\n",
    "fig = plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Create the \"host\" axes using host_subplot.\n",
    "# This axes will act as the base (or host) for the multiple overlaid y-axes.\n",
    "# The parameter 111 indicates a single subplot.\n",
    "# The axes_class=AA.Axes argument allows us to use the AxisArtist toolkit for custom axis management.\n",
    "host = host_subplot(111, axes_class=AA.Axes)\n",
    "\n",
    "# Adjust the right boundary of the host subplot to leave room for additional y-axes.\n",
    "plt.subplots_adjust(right=0.75)\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Create \"parasite\" y-axes.\n",
    "# Each additional axis is created as a twin of the host. \n",
    "# They will later be offset from the default position to avoid overlapping.\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "# par1 will be used for the Sun Apparent Magnitude.\n",
    "par1 = host.twinx()\n",
    "\n",
    "# par2 will be used for Jupiter Delta.\n",
    "par2 = host.twinx()\n",
    "\n",
    "# par3 will be used for Jupiter Average Flux.\n",
    "par3 = host.twinx()\n",
    "\n",
    "# par4 will be used for Jupiter Surface Brightness.\n",
    "par4 = host.twinx()\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Offset the extra parasite axes to the right.\n",
    "# By default, all twinx() axes appear on the right. We use the new_fixed_axis method\n",
    "# to create additional axes at specified offsets.\n",
    "# The offset is specified as a tuple (offset, 0) in points.\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "# Start with an offset of 60 points.\n",
    "offset = 60\n",
    "\n",
    "# For par2:\n",
    "#   - Get the grid helper from par2 and create a new fixed axis on the right.\n",
    "#   - Set its offset to (offset, 0) to move it to the right by 60 points.\n",
    "new_fixed_axis = par2.get_grid_helper().new_fixed_axis\n",
    "par2.axis[\"right\"] = new_fixed_axis(loc=\"right\", axes=par2, offset=(offset, 0))\n",
    "\n",
    "# Increment the offset by 60 for the next parasite axis.\n",
    "offset += 60\n",
    "\n",
    "# For par3:\n",
    "#   - Create a new fixed axis and set its offset accordingly.\n",
    "new_fixed_axis = par3.get_grid_helper().new_fixed_axis\n",
    "par3.axis[\"right\"] = new_fixed_axis(loc=\"right\", axes=par3, offset=(offset, 0))\n",
    "\n",
    "# Increment the offset again.\n",
    "offset += 60\n",
    "\n",
    "# For par4:\n",
    "#   - Create a new fixed axis with the updated offset.\n",
    "new_fixed_axis = par4.get_grid_helper().new_fixed_axis\n",
    "par4.axis[\"right\"] = new_fixed_axis(loc=\"right\", axes=par4, offset=(offset, 0))\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Now plot each of the data series on their respective axes.\n",
    "# It is assumed that the following DataFrames exist:\n",
    "#   - df: Contains Sun data with columns \"days_since_start\", \"delta\", and \"apmag\"\n",
    "#   - df_delta: Contains Jupiter Delta data with columns \"days_since_start\", \"delta_au\"\n",
    "#   - df_flux: Contains Jupiter Average Flux data with columns \"days_since_start\", \"f_avg\"\n",
    "#   - df_sb: Contains Jupiter Surface Brightness data with columns \"days_since_start\", \"s_brt\"\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "# Plot Sun Delta on the host axes as a blue line.\n",
    "# p1 will hold the line object for potential legend creation.\n",
    "p1, = host.plot(df[\"days_since_start\"], df[\"delta\"], color='blue', label='Sun Delta')\n",
    "\n",
    "# Plot Sun Apparent Magnitude on the first parasite axis (par1) as red scatter points.\n",
    "# The marker size is set to 1 for small dots.\n",
    "p2 = par1.scatter(df[\"days_since_start\"], df[\"apmag\"], color='red', s=1, label='Sun Apparent Mag')\n",
    "\n",
    "# Plot Jupiter Delta on the second parasite axis (par2) as black scatter points.\n",
    "p3 = par2.scatter(df_delta[\"days_since_start\"], df_delta[\"delta_au\"], color='black', alpha=0.75, label='Jupiter Delta')\n",
    "\n",
    "# Plot Jupiter Average Flux on the third parasite axis (par3) as blue scatter points.\n",
    "p4 = par3.scatter(df_flux[\"days_since_start\"], df_flux[\"f_avg\"], color='blue', alpha=0.75, label='Jupiter Avg Flux')\n",
    "\n",
    "# Plot Jupiter Surface Brightness on the fourth parasite axis (par4) as green scatter points.\n",
    "p5 = par4.scatter(df_sb[\"days_since_start\"], df_sb[\"s_brt\"], color='green', alpha=0.75, label='Jupiter SB')\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Set labels for the x-axis and each y-axis:\n",
    "#   - The host x-axis is labeled \"Days Since Start\".\n",
    "#   - The host y-axis (left) represents Sun Delta.\n",
    "#   - Each parasite axis gets its own y-axis label.\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "# Set x-axis label on the host.\n",
    "host.set_xlabel(\"Days Since Start\")\n",
    "# Set host y-axis label (for Sun Delta).\n",
    "host.set_ylabel(\"Sun Delta\")\n",
    "# Set label for par1 (Sun Apparent Magnitude).\n",
    "par1.set_ylabel(\"Sun Apparent Mag\")\n",
    "# Set label for par2 (Jupiter Delta).\n",
    "par2.set_ylabel(\"Jupiter Delta\")\n",
    "# Set label for par3 (Jupiter Avg Flux).\n",
    "par3.set_ylabel(\"Jupiter Avg Flux\")\n",
    "# Set label for par4 (Jupiter Surface Brightness).\n",
    "par4.set_ylabel(\"Jupiter Surface Brightness\")\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Color code the axis labels for clarity:\n",
    "#   - The host left axis label is colored blue.\n",
    "#   - The parasite axes on the right are colored with their corresponding series colors.\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "host.axis[\"left\"].label.set_color('blue')      # Sun Delta (blue)\n",
    "par1.axis[\"right\"].label.set_color('red')         # Sun Apparent Magnitude (red)\n",
    "par2.axis[\"right\"].label.set_color('black')       # Jupiter Delta (black)\n",
    "par3.axis[\"right\"].label.set_color('blue')        # Jupiter Avg Flux (blue)\n",
    "par4.axis[\"right\"].label.set_color('green')       # Jupiter Surface Brightness (green)\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Customize x-axis tick locations:\n",
    "# Calculate the maximum number of days across all data sets to set the x-axis ticks.\n",
    "max_days = max(df[\"days_since_start\"].max(),\n",
    "               df_delta[\"days_since_start\"].max(),\n",
    "               df_flux[\"days_since_start\"].max(),\n",
    "               df_sb[\"days_since_start\"].max())\n",
    "# Set x-axis ticks at intervals of 100 days from 0 to max_days + 100.\n",
    "host.set_xticks(np.arange(0, max_days + 100, 100))\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Optionally, you can build a combined legend.\n",
    "# The following commented-out code shows how to collect line objects and labels,\n",
    "# then pass them to host.legend() for a unified legend.\n",
    "#\n",
    "# lines = [p1, p2, p3, p4, p5]\n",
    "# labels = [l.get_label() for l in lines]\n",
    "# host.legend(lines, labels, loc='upper left')\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "# Set a title for the overall plot.\n",
    "plt.title(\"Combined Quadruple Axis Plot (5 Y-Axes) vs. Time\")\n",
    "\n",
    "# Draw and display the final plot.\n",
    "plt.draw()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614e3dd2-e3fa-4c73-9477-0e4c0bd7fbab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# Cell 11 (Enhanced) : Diagnostic & Debugging System for DBSCAN Clusters \n",
    "#                      (Average Flux) WITH \"index_number\" COLUMN ADDED\n",
    "# -------------------------------------------------------------------------\n",
    "#\n",
    "# Description:\n",
    "#   This cell builds upon the previous diagnostics code for analyzing \n",
    "#   clustering in df_flux. The key difference here is that we now create a \n",
    "#   new column called \"index_number\", which enumerates each row in the \n",
    "#   cluster data. In both the snippet (.head(5)) and the full cluster dump, \n",
    "#   you'll see that column so you can track each row easily.\n",
    "#\n",
    "#   Otherwise, the layout, logic, and step-by-step debugging prints remain \n",
    "#   the same. We haven't removed any existing information — only added \n",
    "#   the \"index_number\" column to the cluster data output.\n",
    "#\n",
    "# Usage:\n",
    "#   - Ensure df_flux is defined with columns ['datetime','f_avg','days_since_start'].\n",
    "#   - Run this cell. The new run_dbscan_diagnostics function \n",
    "#     will provide thorough debugging output plus an \"index_number\" column.\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "# Import necessary libraries for numerical operations, data handling, plotting,\n",
    "# and clustering.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import DBSCAN  # Import the DBSCAN clustering algorithm from scikit-learn\n",
    "\n",
    "def run_dbscan_diagnostics(\n",
    "    df_flux,\n",
    "    eps=0.25,\n",
    "    min_samples=3,\n",
    "    margin_scale=0.1\n",
    "):\n",
    "    \"\"\"\n",
    "    Performs a comprehensive diagnostic analysis on df_flux for clustering \n",
    "    in the (days_since_start, f_avg) space.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_flux : pd.DataFrame\n",
    "        Must contain at least the following columns:\n",
    "           - 'datetime': Timestamps of observations.\n",
    "           - 'f_avg':    The average flux data.\n",
    "           - 'days_since_start': The time offset (in days) from a reference time.\n",
    "    eps : float\n",
    "        The maximum distance between two samples for one to be considered \n",
    "        as in the neighborhood of the other (a DBSCAN parameter).\n",
    "    min_samples : int\n",
    "        The minimum number of samples required in a neighborhood for a point to be considered \n",
    "        a core point (a DBSCAN parameter).\n",
    "    margin_scale : float\n",
    "        Fractional padding added to each cluster's min/max time when setting subplot x-limits.\n",
    "        For example, 0.1 adds 10% of the cluster's time span as padding.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "        This function prints diagnostic information and displays plots.\n",
    "    \"\"\"\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Start the diagnostic process by printing a header.\n",
    "    print(\"\\n===========================\")\n",
    "    print(\" DBSCAN DIAGNOSTIC PROCESS\")\n",
    "    print(\"===========================\\n\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # STEP 1: Basic Checks on df_flux Structure\n",
    "    print(\"STEP 1: Checking df_flux structure and required columns...\")\n",
    "    # Define the required columns that must be present in df_flux.\n",
    "    required_cols = [\"datetime\", \"f_avg\", \"days_since_start\"]\n",
    "    # Iterate through each required column and check if it exists in df_flux.\n",
    "    for c in required_cols:\n",
    "        if c not in df_flux.columns:\n",
    "            # If a column is missing, print an error and abort the function.\n",
    "            print(f\"ERROR: Missing column '{c}' in df_flux! Cannot proceed.\")\n",
    "            print(\"Suggestion: Ensure df_flux has these columns or rename appropriately.\")\n",
    "            return  # Abort execution if required columns are not present\n",
    "\n",
    "    # Print a sample of the first 5 rows of df_flux for verification.\n",
    "    print(\"\\ndf_flux sample (first 5 rows):\")\n",
    "    print(df_flux.head(5))\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # STEP 2: Validate for NaN or Infinite Values in Critical Columns\n",
    "    print(\"\\nSTEP 2: Validating for NaN or infinite values in [f_avg, days_since_start]...\")\n",
    "    # Count the number of NaN values in the 'f_avg' column.\n",
    "    nan_count_favg = df_flux[\"f_avg\"].isna().sum()\n",
    "    # Count the number of NaN values in the 'days_since_start' column.\n",
    "    nan_count_days = df_flux[\"days_since_start\"].isna().sum()\n",
    "    # If there are any NaN values, print a warning.\n",
    "    if nan_count_favg > 0 or nan_count_days > 0:\n",
    "        print(f\"WARNING: Found {nan_count_favg} NaN in 'f_avg' and {nan_count_days} in 'days_since_start'.\")\n",
    "        print(\"         This could impact DBSCAN. Suggestion: remove or fill NaNs.\")\n",
    "    else:\n",
    "        print(\"No NaN values found in the relevant columns.\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # STEP 3: Check the Range of 'days_since_start'\n",
    "    print(\"\\nSTEP 3: Checking range of days_since_start...\")\n",
    "    # Get the minimum and maximum values of 'days_since_start'.\n",
    "    days_min = df_flux[\"days_since_start\"].min()\n",
    "    days_max = df_flux[\"days_since_start\"].max()\n",
    "    print(f\"days_since_start => min: {days_min:.3f}, max: {days_max:.3f}\")\n",
    "    # If the minimum is significantly negative (and by more than a year), warn the user.\n",
    "    if days_min < 0 and abs(days_min) > 365:\n",
    "        print(\"NOTE: There's a significantly negative min time. This could indicate multiple reference points.\")\n",
    "        print(\"      If unintended, double-check your reference_time assignment.\")\n",
    "    # If the maximum value is very large, warn the user.\n",
    "    if days_max > 3000:\n",
    "        print(\"NOTE: A very large days_since_start (>3000). Are you sure your reference is correct?\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # STEP 4: Run DBSCAN Clustering on the 'days_since_start' Data\n",
    "    print(f\"\\nSTEP 4: Running DBSCAN with eps={eps} and min_samples={min_samples}...\")\n",
    "    # Reshape the 'days_since_start' data into a 2D array, as required by scikit-learn.\n",
    "    X = df_flux[\"days_since_start\"].values.reshape(-1, 1)\n",
    "    try:\n",
    "        # Create a DBSCAN object with the given eps and min_samples parameters.\n",
    "        db = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "        # Fit DBSCAN to the data and predict cluster labels for each point.\n",
    "        cluster_labels = db.fit_predict(X)\n",
    "    except Exception as e:\n",
    "        # If an error occurs, print the error and abort.\n",
    "        print(f\"ERROR: DBSCAN failed with an exception: {e}\")\n",
    "        print(\"Suggestion: verify that your data is numeric and the DBSCAN parameters are valid.\")\n",
    "        return\n",
    "\n",
    "    # Add a new column \"cluster\" to df_flux with the assigned cluster labels.\n",
    "    df_flux[\"cluster\"] = cluster_labels\n",
    "\n",
    "    # Identify the unique clusters (excluding noise points labeled as -1).\n",
    "    clusters = df_flux[df_flux[\"cluster\"] != -1][\"cluster\"].unique()\n",
    "    clusters.sort()\n",
    "\n",
    "    print(\"DBSCAN completed successfully.\")\n",
    "    print(f\"Number of data points: {len(df_flux)}\")\n",
    "    # Count the number of noise points (points with label -1).\n",
    "    n_noise = sum(cluster_labels == -1)\n",
    "    print(f\"Number of noise points: {n_noise}\")\n",
    "    print(f\"Cluster labels found (excluding -1): {clusters}\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # STEP 5: Summarize Each Cluster\n",
    "    print(\"\\nSTEP 5: Summarizing clusters:\")\n",
    "    if len(clusters) == 0:\n",
    "        print(\"No valid clusters were detected (all points might be noise).\")\n",
    "        print(\"Possible reasons or suggestions:\")\n",
    "        print(\" - Increase eps to allow more points to form a cluster.\")\n",
    "        print(\" - Decrease min_samples to allow smaller clusters.\")\n",
    "        print(\" - Verify that days_since_start is correct.\")\n",
    "        return\n",
    "\n",
    "    # Loop over each identified cluster label.\n",
    "    for label in clusters:\n",
    "        # Extract the subset of df_flux corresponding to the current cluster.\n",
    "        cdata = df_flux[df_flux[\"cluster\"] == label]\n",
    "        # Determine the minimum and maximum 'days_since_start' values for the cluster.\n",
    "        cmin = cdata[\"days_since_start\"].min()\n",
    "        cmax = cdata[\"days_since_start\"].max()\n",
    "        # Determine the size (number of points) of the cluster.\n",
    "        csize = len(cdata)\n",
    "        print(f\"  Cluster {label}: size={csize}, time-range=({cmin:.2f}, {cmax:.2f})\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # STEP 6: Create Subplots for Each Cluster with Debugging Information\n",
    "    print(\"\\nSTEP 6: Creating subplots for each cluster.\")\n",
    "    n_clusters = len(clusters)\n",
    "    # Create a figure with a subplot for each cluster.\n",
    "    # The height is set to 3 inches per cluster.\n",
    "    fig, axs = plt.subplots(\n",
    "        n_clusters, \n",
    "        1, \n",
    "        figsize=(12, 3 * n_clusters), \n",
    "        sharex=False\n",
    "    )\n",
    "    # If there is only one cluster, wrap the axis in a list for consistency.\n",
    "    if n_clusters == 1:\n",
    "        axs = [axs]\n",
    "\n",
    "    # Loop over each cluster for plotting.\n",
    "    for i, cluster_label in enumerate(clusters):\n",
    "        ax = axs[i]\n",
    "        # Select the data corresponding to the current cluster.\n",
    "        cluster_data = df_flux[df_flux[\"cluster\"] == cluster_label]\n",
    "\n",
    "        # Create a copy of the cluster data after resetting its index.\n",
    "        # This allows us to add a new column \"index_number\" without altering the original df_flux.\n",
    "        temp_for_print = cluster_data.reset_index(drop=True).copy()\n",
    "        # Add the \"index_number\" column. Here, cluster_data.index gives the original indices,\n",
    "        # so adding 1 ensures the numbers start from 1.\n",
    "        temp_for_print[\"index_number\"] = cluster_data.index + 1\n",
    "\n",
    "        # Print a header for the current cluster.\n",
    "        print(f\"\\n  -> Plotting cluster {cluster_label + 1}: {len(cluster_data)} points.\")\n",
    "\n",
    "        # Print a snippet (first 5 rows) of the cluster data, showing only selected columns.\n",
    "        # Use to_string(index=False) to skip printing the default DataFrame index.\n",
    "        snippet_data = temp_for_print.head(5)\n",
    "        print(snippet_data[[\"index_number\",\"days_since_start\",\"f_avg\"]].to_string(index=False))\n",
    "\n",
    "        # Print the full cluster data with the \"index_number\" column for complete diagnostics.\n",
    "        print(f\"\\n     Full cluster data for cluster {cluster_label}:\")\n",
    "        print(temp_for_print[[\"index_number\",\"days_since_start\",\"f_avg\"]].to_string(index=False))\n",
    "        print(\"     -------------------------\")\n",
    "\n",
    "        # Plot the cluster data: 'days_since_start' on the x-axis and 'f_avg' on the y-axis.\n",
    "        ax.scatter(\n",
    "            cluster_data[\"days_since_start\"], \n",
    "            cluster_data[\"f_avg\"],\n",
    "            color='blue', \n",
    "            alpha=0.75, \n",
    "            s=20\n",
    "        )\n",
    "\n",
    "        # Determine the x-axis limits for the plot by calculating the range of 'days_since_start'.\n",
    "        t_min = cluster_data[\"days_since_start\"].min()\n",
    "        t_max = cluster_data[\"days_since_start\"].max()\n",
    "        span  = t_max - t_min\n",
    "        if span == 0:\n",
    "            # If all points have the same time, set a default span.\n",
    "            span = 0.2\n",
    "            t_min -= 0.1\n",
    "            t_max += 0.1\n",
    "        else:\n",
    "            # Otherwise, add a margin proportional to the span.\n",
    "            margin = margin_scale * span\n",
    "            t_min -= margin\n",
    "            t_max += margin\n",
    "\n",
    "        # Apply the computed x-axis limits.\n",
    "        ax.set_xlim(t_min, t_max)\n",
    "\n",
    "        # Determine the y-axis limits for the average flux data.\n",
    "        favg_min = cluster_data[\"f_avg\"].min()\n",
    "        favg_max = cluster_data[\"f_avg\"].max()\n",
    "        y_span = favg_max - favg_min\n",
    "        if y_span == 0:\n",
    "            # Set a fallback span if all flux values are identical.\n",
    "            y_span = 1.0\n",
    "            favg_min -= 0.5\n",
    "            favg_max += 0.5\n",
    "        else:\n",
    "            # Add a margin to the y-axis limits.\n",
    "            y_margin = margin_scale * y_span\n",
    "            favg_min -= y_margin\n",
    "            favg_max += y_margin\n",
    "\n",
    "        # Apply the computed y-axis limits.\n",
    "        ax.set_ylim(favg_min, favg_max)\n",
    "        # Label the y-axis.\n",
    "        ax.set_ylabel(\"Average Flux\")\n",
    "        # Set the title for the subplot, indicating the cluster number and time range.\n",
    "        ax.set_title(f\"Cluster {cluster_label + 1}: {t_min:.2f} to {t_max:.2f} days\", fontsize=16)\n",
    "\n",
    "    # Set the x-axis label for the last subplot.\n",
    "    axs[-1].set_xlabel(\"Days Since Start\")\n",
    "    # Adjust subplot layout to avoid overlap.\n",
    "    plt.tight_layout()\n",
    "    # Display the subplots.\n",
    "    plt.show()\n",
    "\n",
    "    # Print a final message indicating the end of diagnostics.\n",
    "    print(\"\\nAll clusters plotted successfully. End of Cell 11 Diagnostics.\\n\")\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Example usage:\n",
    "# -------------------------------------------------------------------------\n",
    "# Run the DBSCAN diagnostics function on df_flux with customized parameters.\n",
    "# Here, eps=0.5, min_samples=1, and margin_scale=0.1 are passed to control clustering\n",
    "# sensitivity and plot margins.\n",
    "run_dbscan_diagnostics(df_flux, eps=0.5, min_samples=1, margin_scale=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdddccb5-6bf6-4143-95ac-c80738c69821",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212ed7b9-cda4-44fa-bc26-71c7eb96056f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1051f70-628a-4de0-be03-42d8e7624d38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4d89f0-b298-4b16-9c09-fac61357e011",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75435dc8-8c8b-4033-9b57-37c420431158",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5face325-9330-4feb-bcbf-8b490b4afc18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea93b8c3-a1e2-40f2-a0ef-96e18c7edac8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9881fb-5927-428d-b3a0-bbcf90eef09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sum of all the lines of code that have been written\n",
    "\n",
    "import nbformat\n",
    "\n",
    "# Replace with the path to your notebook file\n",
    "notebook_path = 'Main Commented.ipynb'\n",
    "\n",
    "# Read the notebook file using nbformat (version 4 is standard for recent notebooks)\n",
    "nb = nbformat.read(notebook_path, as_version=4)\n",
    "\n",
    "# Initialize a counter for code lines\n",
    "total_code_lines = 0\n",
    "\n",
    "# Loop over all cells in the notebook\n",
    "for cell in nb.cells:\n",
    "    # Process only code cells\n",
    "    if cell.cell_type == 'code':\n",
    "        # Split the cell source by newline to count the lines of code in that cell\n",
    "        lines = cell.source.splitlines()\n",
    "        total_code_lines += len(lines)\n",
    "\n",
    "print(f\"Total lines of code in the notebook: {total_code_lines}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a0c3cb-702e-41b1-a123-5cc793e4d17e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
