{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e1d54d9-9506-4e8b-a3d6-67220c76ea83",
   "metadata": {},
   "source": [
    "CELL 1 - this cell is to import all the functions that i may need in the actual code itself, now with the capabilities of using the matplotlib widget tools for interactive graph sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedf66a1-cc18-48d6-974f-af92246529b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 1 - imports\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import math\n",
    "\n",
    "from astropy.io import fits\n",
    "from astropy.visualization import (ZScaleInterval, ImageNormalize)\n",
    "from astropy.wcs import WCS\n",
    "from photutils.detection import DAOStarFinder\n",
    "from photutils.aperture import (aperture_photometry, CircularAperture)\n",
    "from astroquery.vizier import Vizier\n",
    "from reproject import reproject_interp\n",
    "from astroalign import register\n",
    "from lightkurve import search_lightcurvefile\n",
    "import aplpy\n",
    "\n",
    "from skimage import filters, measure, morphology\n",
    "from skimage.measure import label, regionprops\n",
    "from skimage.morphology import binary_closing, disk\n",
    "from matplotlib.patches import Ellipse\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "horizons_url = 'https://ssd.jpl.nasa.gov/api/horizons_file.api'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac64098-630b-4a58-9f04-ebd8c713fade",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ONLY RUN WIDGET AFTER MAIN SCRIPT OR IT WILL BLOW UP YOUR RAM CHIPS\n",
    "#%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bc0f46-3120-40ad-a263-911e2606a85b",
   "metadata": {},
   "source": [
    "CELL 2 - this one is to create the time summary sequnce to know what parts of the code are taking the longest, keep in mind it refreshes only when you actually re-run this block itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45d9bb5-af51-47ed-a883-67ebc7fbf8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cell 2 - timer utilities\n",
    "\n",
    "time_records = []\n",
    "\n",
    "def start_section(section_name):\n",
    "    now = time.time()\n",
    "    time_records.append({\n",
    "        'section': section_name,\n",
    "        'start': now,\n",
    "        'end': None,\n",
    "        'duration': None\n",
    "    })\n",
    "\n",
    "def end_section():\n",
    "    now = time.time()\n",
    "    if not time_records:\n",
    "        return\n",
    "    record = time_records[-1]\n",
    "    record['end'] = now\n",
    "    record['duration'] = now - record['start']\n",
    "\n",
    "def print_time_summary():\n",
    "    if not time_records:\n",
    "        return\n",
    "    \n",
    "    print(\"\\n=== PROCESSING TIME SUMMARY ===\")\n",
    "    max_len = max(len(r['section']) for r in time_records)\n",
    "    name_col_width = max(20, max_len + 2)\n",
    "    print(f\"{'Section':<{name_col_width}}  {'Start':>15}   {'End':>15}   {'Duration (s)':>14}\")\n",
    "    print(\"-\" * (name_col_width + 50))\n",
    "    \n",
    "    total_time = time_records[-1]['end'] - time_records[0]['start']\n",
    "    \n",
    "    for r in time_records:\n",
    "        section = r['section']\n",
    "        start_dt = datetime.fromtimestamp(r['start']).strftime(\"%H:%M:%S.%f\")[:-3]\n",
    "        end_dt = datetime.fromtimestamp(r['end']).strftime(\"%H:%M:%S.%f\")[:-3] if r['end'] else \"N/A\"\n",
    "        duration = f\"{r['duration']:.3f}\" if r['duration'] else \"N/A\"\n",
    "        print(f\"{section:<{name_col_width}}  {start_dt:>15}   {end_dt:>15}   {duration:>14}\")\n",
    "    \n",
    "    print(\"-\" * (name_col_width + 50))\n",
    "    print(f\"{'Total Runtime':<{name_col_width}}                       {total_time:.3f} seconds\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbc31fe-ba38-4e03-9ef6-da0b2d4ad2e9",
   "metadata": {},
   "source": [
    "CELL 3 - The Horizons API system so that all the queries that i need are actually parsed through for jupiters entire flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbce7fa-a083-4171-81df-ef309bd78af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 3 - horizons api\n",
    "\n",
    "def create_input_content(dateobs, timeobs):\n",
    "    return f\"\"\"\n",
    "    !$$SOF\n",
    "    COMMAND='599'\n",
    "    OBJ_DATA='YES'\n",
    "    MAKE_EPHEM='YES'\n",
    "    TABLE_TYPE='OBSERVER'\n",
    "    CENTER='500@399'\n",
    "    TLIST='{dateobs} {timeobs}'\n",
    "    QUANTITIES='9,20,23,24'\n",
    "    !$$EOF\n",
    "    \"\"\"\n",
    "\n",
    "def parse_horizons_text_for_delta_and_sbrt(horizons_text):\n",
    "    lines = horizons_text.splitlines()\n",
    "    \n",
    "    try:\n",
    "        start_index = next(i for i, line in enumerate(lines) if '$$SOE' in line)\n",
    "        end_index = next(i for i, line in enumerate(lines) if '$$EOE' in line)\n",
    "    except StopIteration:\n",
    "        print(\"Could not find $$SOE/$$EOE in Horizons output.\")\n",
    "        return None, None\n",
    "\n",
    "    ephem_lines = lines[start_index+1 : end_index]\n",
    "\n",
    "    for ln in ephem_lines:\n",
    "        ln = ln.strip()\n",
    "        if not ln:\n",
    "            continue\n",
    "        tokens = ln.split()\n",
    "\n",
    "        if len(tokens) < 9:\n",
    "            continue\n",
    "        try:\n",
    "            s_brt = float(tokens[3])\n",
    "            dist_au = float(tokens[4])\n",
    "            return dist_au, s_brt\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "    return None, None\n",
    "\n",
    "def get_horizons_data(dateobs, timeobs):\n",
    "    content = create_input_content(dateobs, timeobs)\n",
    "    \n",
    "    resp = requests.post(\n",
    "        horizons_url,\n",
    "        data={'format': 'text'},\n",
    "        files={'input': ('input.txt', content)}\n",
    "    )\n",
    "\n",
    "    if resp.status_code != 200:\n",
    "        print(f\"Failed Horizons request: {resp.status_code}\")\n",
    "        return None, None\n",
    "    \n",
    "    return parse_horizons_text_for_delta_and_sbrt(resp.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a9f900-fa14-4555-9aff-e7cb5f2e47e4",
   "metadata": {},
   "source": [
    "CELL 4 - the tools needed for python to \"see the image\" and perform apperture makes and models on it all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fda3f6f-e0cd-4781-b043-7fff48b0ee4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4 : Image Processing Utilities\n",
    "\n",
    "def elliptical_mask(shape, center, major_axis, minor_axis, angle_deg):\n",
    "    (h, w) = shape\n",
    "    y_grid, x_grid = np.ogrid[0:h, 0:w]\n",
    "    cx, cy = center\n",
    "    theta = np.deg2rad(angle_deg)\n",
    "    a = major_axis / 2.0\n",
    "    b = minor_axis / 2.0\n",
    "    x_shifted = x_grid - cx\n",
    "    y_shifted = y_grid - cy\n",
    "    cos_t = np.cos(theta)\n",
    "    sin_t = np.sin(theta)\n",
    "    x_prime = x_shifted * cos_t + y_shifted * sin_t\n",
    "    y_prime = -x_shifted * sin_t + y_shifted * cos_t\n",
    "    return (x_prime**2) / (a * a) + (y_prime**2) / (b * b) <= 1.0\n",
    "\n",
    "def rotate_image_full(data, cx, cy, angle_deg):\n",
    "    rows, cols = data.shape\n",
    "    M = cv2.getRotationMatrix2D((cx, cy), angle_deg, 1.0)\n",
    "    return cv2.warpAffine(data, M, (cols, rows), flags=cv2.INTER_LINEAR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c3aa82-929f-46e5-963f-9c8abdfb647a",
   "metadata": {},
   "source": [
    "CELL 5 - the main code, not much to be said this one does way too much stuff to even begin listing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07415db0-9ca9-4752-92ff-998ddcb4eb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 5 - main code\n",
    "\n",
    "start_section(\"Script Start\")\n",
    "start_section(\"Glob FITS files\")\n",
    "fits_files = glob.glob(\"jupiter_data_WFC3_F631DRC/MAST_2023_08_29T0311/HST/*/*.fits\")\n",
    "#fits_files = glob.glob(\"icpf19f7q_drc.fits\")\n",
    "end_section()\n",
    "\n",
    "if len(fits_files) == 0:\n",
    "    print(\"No FITS files found in the specified directory.\")\n",
    "    # No end_section() here if we didn't start processing\n",
    "    print_time_summary() # Print whatever timing we have so far\n",
    "    # Exit or handle appropriately if no files found, added guard below\n",
    "else:\n",
    "    print(f\"Found {len(fits_files)} FITS files.\\n\")\n",
    "\n",
    "    file_details = []\n",
    "    start_section(\"Reading FITS metadata\")\n",
    "\n",
    "    for fits_file in fits_files:\n",
    "        try:\n",
    "            with fits.open(fits_file) as hdul:\n",
    "                header = hdul[0].header\n",
    "                raw_data = hdul[1].data\n",
    "                if raw_data is None:\n",
    "                    data = np.zeros((1, 1), dtype=np.float32)\n",
    "                else:\n",
    "                    # Ensure raw_data is treated as float before nan_to_num\n",
    "                    data = np.nan_to_num(raw_data.astype(np.float64), nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32)\n",
    "                total_flux = float(np.sum(data))\n",
    "                file_name = os.path.basename(fits_file)\n",
    "                date_obs = header.get('DATE-OBS', 'N/A')\n",
    "                time_obs = header.get('TIME-OBS', 'N/A')\n",
    "                telescope = header.get('TELESCOP', 'N/A')\n",
    "                instrument = header.get('INSTRUME', 'N/A')\n",
    "                exposure_time = header.get('EXPTIME', 'N/A')\n",
    "                t_filter = header.get('FILTER', 'N/A')\n",
    "                file_details.append((file_name, data, total_flux, date_obs, time_obs, telescope, instrument, exposure_time, t_filter))\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {fits_file}: {e}\")\n",
    "    end_section()\n",
    "\n",
    "    start_section(\"Sort file_details\")\n",
    "    file_details = sorted(file_details, key=lambda x: (x[3], x[4]))\n",
    "    end_section()\n",
    "\n",
    "    start_section(\"Processing images\")\n",
    "    ellipse_summaries = []\n",
    "    cropped_images = [] # Keep original list for Cell 11 etc.\n",
    "    image_storage = []\n",
    "\n",
    "    # ---- ADDED: Initialize lists for NumPy arrays ----\n",
    "    all_param_tables = []\n",
    "    all_normalized_images = []\n",
    "    all_rotated_images = []\n",
    "    all_contour_images = []\n",
    "    all_cropped_images_for_npy = []\n",
    "    # ---- END ADDED ----\n",
    "\n",
    "    for i, (file_name, data, total_flux, date_obs, time_obs, telescope, instrument, exposure_time, t_filter) in enumerate(file_details, start=1):\n",
    "        start_section(f\"Process {file_name}\")\n",
    "        dist_au = None\n",
    "        s_brt = None\n",
    "        if date_obs != 'N/A' and time_obs != 'N/A':\n",
    "            dist_au, s_brt_val = get_horizons_data(date_obs, time_obs)\n",
    "            if s_brt_val:\n",
    "                s_brt = s_brt_val\n",
    "\n",
    "        # Initialize image variables for this iteration\n",
    "        norm_8u_orig = np.zeros((1, 1), dtype=np.uint8)\n",
    "        norm_8u_rot = np.zeros((1, 1), dtype=np.uint8)\n",
    "        overlay_img = np.zeros((1, 1, 3), dtype=np.uint8) # Assuming BGR for overlay\n",
    "        norm_8u_crop = None # Initialize as None\n",
    "        cropped_data = np.array([]) # Initialize as empty\n",
    "\n",
    "        if data.size > 1:\n",
    "            high_cut_orig = np.percentile(data[np.isfinite(data)], 99) if np.any(np.isfinite(data)) else 0\n",
    "            clipped_orig = np.clip(data, 0, high_cut_orig)\n",
    "            # Check if clipped_orig has valid range before normalizing\n",
    "            min_val, max_val = clipped_orig.min(), clipped_orig.max()\n",
    "            if max_val > min_val:\n",
    "                 norm_8u_orig = cv2.normalize(clipped_orig, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
    "            elif max_val > 0: # Handle case where image is flat but not zero\n",
    "                 norm_8u_orig = np.full(clipped_orig.shape, 255, dtype=np.uint8)\n",
    "            else: # Handle case where image is all zero\n",
    "                 norm_8u_orig = np.zeros(clipped_orig.shape, dtype=np.uint8)\n",
    "\n",
    "        rotate_angle = 0.0\n",
    "        # Ensure data has dimensions before accessing shape\n",
    "        if data.ndim >= 2:\n",
    "            cx, cy = (data.shape[1] / 2.0, data.shape[0] / 2.0)\n",
    "\n",
    "            tmp_thresh_val = 5\n",
    "            # Ensure norm_8u_orig is valid before thresholding\n",
    "            if norm_8u_orig.size > 1:\n",
    "                _, tmp_thresh = cv2.threshold(norm_8u_orig, tmp_thresh_val, 255, cv2.THRESH_BINARY)\n",
    "                tmp_contours, _ = cv2.findContours(tmp_thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "                if len(tmp_contours) > 0:\n",
    "                    largest_tmp_contour = max(tmp_contours, key=cv2.contourArea)\n",
    "                    if len(largest_tmp_contour) >= 5:\n",
    "                        (temp_cx, temp_cy), (w, h), angle = cv2.fitEllipse(largest_tmp_contour)\n",
    "                        # Ensure dimensions are valid before comparison\n",
    "                        if w > 0 and h > 0:\n",
    "                            if h > w:\n",
    "                                angle += 90\n",
    "                                w, h = h, w # Swap width and height for consistency if needed\n",
    "                        rotate_angle = angle\n",
    "\n",
    "            # Rotate original high-bit data\n",
    "            rotated_data = rotate_image_full(data, cx, cy, rotate_angle)\n",
    "            rotated_flux = float(np.sum(rotated_data))\n",
    "\n",
    "            # Normalize rotated data\n",
    "            high_cut_rot = np.percentile(rotated_data[np.isfinite(rotated_data)], 99) if np.any(np.isfinite(rotated_data)) else 0\n",
    "            clipped_rot = np.clip(rotated_data, 0, high_cut_rot)\n",
    "            min_val_rot, max_val_rot = clipped_rot.min(), clipped_rot.max()\n",
    "            if max_val_rot > min_val_rot:\n",
    "                 norm_8u_rot = cv2.normalize(clipped_rot, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
    "            elif max_val_rot > 0:\n",
    "                 norm_8u_rot = np.full(clipped_rot.shape, 255, dtype=np.uint8)\n",
    "            else:\n",
    "                 norm_8u_rot = np.zeros(clipped_rot.shape, dtype=np.uint8)\n",
    "\n",
    "\n",
    "            # Prepare overlay image (ensure it's 3-channel BGR)\n",
    "            if norm_8u_rot.ndim == 2:\n",
    "                overlay_img = cv2.cvtColor(norm_8u_rot, cv2.COLOR_GRAY2BGR)\n",
    "            elif norm_8u_rot.ndim == 3: # Should already be BGR if loaded from file, but check anyway\n",
    "                overlay_img = norm_8u_rot\n",
    "            else: # Fallback for unexpected dimensions\n",
    "                overlay_img = np.zeros((norm_8u_rot.shape[0], norm_8u_rot.shape[1], 3), dtype=np.uint8)\n",
    "\n",
    "            threshold_value = 5\n",
    "            _, thr_rot = cv2.threshold(norm_8u_rot, threshold_value, 255, cv2.THRESH_BINARY)\n",
    "            rot_contours, _ = cv2.findContours(thr_rot, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        else: # Handle case where initial data was not valid\n",
    "             rotated_data = data\n",
    "             rotated_flux = total_flux\n",
    "             norm_8u_rot = norm_8u_orig # Will be zeros\n",
    "             overlay_img = cv2.cvtColor(norm_8u_rot, cv2.COLOR_GRAY2BGR)\n",
    "             rot_contours = []\n",
    "\n",
    "\n",
    "        ellipse_flux = 0.0\n",
    "        cropped_flux = 0.0\n",
    "        flux_avg = 0.0\n",
    "        ef_err = 0.0\n",
    "        fa_err = 0.0\n",
    "\n",
    "        if len(rot_contours) > 0:\n",
    "            rot_largest_contour = max(rot_contours, key=cv2.contourArea)\n",
    "            if len(rot_largest_contour) >= 5:\n",
    "                (rcx, rcy), (rw, rh), rangle = cv2.fitEllipse(rot_largest_contour)\n",
    "                # Check for valid ellipse dimensions\n",
    "                if rw > 0 and rh > 0:\n",
    "                    if rh > rw:\n",
    "                        rangle += 90\n",
    "                        rw, rh = rh, rw # Swap if needed\n",
    "                    # Ensure mask dimensions match rotated_data\n",
    "                    if rotated_data.shape[0] > 0 and rotated_data.shape[1] > 0:\n",
    "                        mask_ell = elliptical_mask(rotated_data.shape, (rcx, rcy), rw, rh, 0) # Use 0 angle for mask if ellipse was already rotated\n",
    "                        # Ensure mask is boolean before indexing\n",
    "                        mask_ell = mask_ell.astype(bool)\n",
    "                        if mask_ell.shape == rotated_data.shape:\n",
    "                             ellipse_flux = float(np.sum(rotated_data[mask_ell]))\n",
    "                        else:\n",
    "                             print(f\"Warning: Mask shape {mask_ell.shape} mismatch with data shape {rotated_data.shape} for {file_name}\")\n",
    "\n",
    "                    # Draw ellipse on overlay_img (ensure overlay_img is valid)\n",
    "                    if overlay_img.size > 1:\n",
    "                         # Ensure ellipse parameters are integers/floats as expected by cv2.ellipse\n",
    "                         center_pt = (int(round(rcx)), int(round(rcy)))\n",
    "                         axes_len = (int(round(rw)), int(round(rh)))\n",
    "                         cv2.ellipse(overlay_img, (center_pt, axes_len, 0), (0, 255, 0), 2) # Use 0 angle here\n",
    "                else:\n",
    "                    print(f\"Warning: Invalid ellipse dimensions (rw={rw}, rh={rh}) found for {file_name}\")\n",
    "\n",
    "\n",
    "            # Bounding Rect and Cropping\n",
    "            rx, ry, rW, rH = cv2.boundingRect(rot_largest_contour)\n",
    "            # Ensure crop dimensions are valid and within bounds\n",
    "            if rW > 0 and rH > 0 and ry+rH <= rotated_data.shape[0] and rx+rW <= rotated_data.shape[1]:\n",
    "                cropped_data = rotated_data[ry:ry + rH, rx:rx + rW]\n",
    "                cropped_flux = float(np.sum(cropped_data))\n",
    "                # Draw rectangle on overlay_img\n",
    "                if overlay_img.size > 1:\n",
    "                     cv2.rectangle(overlay_img, (rx, ry), (rx + rW, ry + rH), (255, 0, 0), 2)\n",
    "            else:\n",
    "                print(f\"Warning: Invalid bounding rect dimensions or out of bounds for {file_name}. Cropping skipped.\")\n",
    "                cropped_data = np.array([]) # Reset cropped_data if invalid\n",
    "\n",
    "\n",
    "            # Calculate errors and average flux\n",
    "            ef_err = np.sqrt(ellipse_flux) if ellipse_flux > 0 else 0.0\n",
    "            # Ensure exposure_time is valid number before division\n",
    "            valid_exptime = False\n",
    "            try:\n",
    "                exp_time_float = float(exposure_time)\n",
    "                if exp_time_float > 0:\n",
    "                    valid_exptime = True\n",
    "            except (ValueError, TypeError):\n",
    "                valid_exptime = False\n",
    "\n",
    "            if valid_exptime and dist_au is not None:\n",
    "                flux_avg = ellipse_flux / (exp_time_float * dist_au)\n",
    "                fa_err = ef_err / (exp_time_float * dist_au)\n",
    "\n",
    "        ellipse_summaries.append((i, file_name, s_brt, ellipse_flux, dist_au, ef_err, flux_avg, fa_err, date_obs, time_obs))\n",
    "\n",
    "        param_values = [\n",
    "            (\"File #\", str(i)),\n",
    "            (\"File Name\", file_name),\n",
    "            (\"DATE-OBS\", date_obs),\n",
    "            (\"TIME-OBS\", time_obs),\n",
    "            (\"Telescope\", telescope),\n",
    "            (\"Instrument\", instrument),\n",
    "            (\"Filter\", t_filter),\n",
    "            (\"surface brightness (Jupiter)\", f\"{s_brt:.3f}\" if s_brt is not None else \"N/A\"),\n",
    "            (\"Total Flux\", f\"{total_flux:.2f}\"),\n",
    "            (\"Flux (Rotated)\", f\"{rotated_flux:.2f}\"),\n",
    "            (\"Ellipse Flux\", f\"{ellipse_flux:.2f}\"),\n",
    "            (\"Cropped Flux\", f\"{cropped_flux:.2f}\" if cropped_data.size > 0 else \"N/A\"),\n",
    "            (\"Exposure Time\", f\"{exposure_time}\"),\n",
    "            (\"Delta (AU)\", f\"{dist_au:.5f}\" if dist_au is not None else \"N/A\"),\n",
    "            (\"Flux Average\", f\"{flux_avg:.2f}\" if flux_avg != 0.0 else \"N/A\"), # Display N/A if flux_avg is 0\n",
    "            (\"Ellipse Flux Err\", f\"{ef_err:.2f}\"),\n",
    "            (\"Flux Avg Err\", f\"{fa_err:.2f}\")\n",
    "        ]\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(f\"  Parameter-Value Table for {file_name}\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        col_width = 35\n",
    "\n",
    "        for (p, v) in param_values:\n",
    "            print(f\"{p:<{col_width}} {v}\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        fig, axes = plt.subplots(1, 4, figsize=(22, 5))\n",
    "        # Ensure images are valid before showing\n",
    "        if norm_8u_orig.size > 1:\n",
    "             axes[0].imshow(norm_8u_orig, cmap='gray', origin='lower')\n",
    "        axes[0].set_title(f\"Normalized 8-bit\\n(Total Flux={total_flux:.2f})\", fontsize=10)\n",
    "        axes[0].axis('off')\n",
    "\n",
    "        if norm_8u_rot.size > 1:\n",
    "             axes[1].imshow(norm_8u_rot, cmap='gray', origin='lower')\n",
    "        axes[1].set_title(f\"Rotated Angle={rotate_angle:.1f}°\\n(Flux={rotated_flux:.2f})\", fontsize=10)\n",
    "        axes[1].axis('off')\n",
    "\n",
    "        if overlay_img.size > 1:\n",
    "             # OpenCV loads as BGR, Matplotlib expects RGB\n",
    "             axes[2].imshow(cv2.cvtColor(overlay_img, cv2.COLOR_BGR2RGB), origin='lower')\n",
    "        axes[2].set_title(f\"Contour+Ellipse\\n(EllipseFlux={ellipse_flux:.2f})\", fontsize=10)\n",
    "        axes[2].axis('off')\n",
    "\n",
    "        if cropped_data.size > 1:\n",
    "            # Normalize cropped data for display\n",
    "            high_cut_crop = np.percentile(cropped_data[np.isfinite(cropped_data)], 99) if np.any(np.isfinite(cropped_data)) else 0\n",
    "            clipped_crop = np.clip(cropped_data, 0, high_cut_crop)\n",
    "            min_val_crop, max_val_crop = clipped_crop.min(), clipped_crop.max()\n",
    "            if max_val_crop > min_val_crop:\n",
    "                 norm_8u_crop = cv2.normalize(clipped_crop, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
    "            elif max_val_crop > 0:\n",
    "                 norm_8u_crop = np.full(clipped_crop.shape, 255, dtype=np.uint8)\n",
    "            else:\n",
    "                 norm_8u_crop = np.zeros(clipped_crop.shape, dtype=np.uint8)\n",
    "\n",
    "            axes[3].imshow(norm_8u_crop, cmap='gray', origin='lower')\n",
    "            axes[3].set_title(f\"Cropped\\n(Flux={cropped_flux:.2f})\", fontsize=10)\n",
    "            axes[3].axis('off')\n",
    "\n",
    "            # Append to original list for other cells\n",
    "            cropped_images.append((i, file_name, date_obs, time_obs, norm_8u_crop))\n",
    "        else:\n",
    "            # If no cropped data, show the rotated image again or blank\n",
    "            if norm_8u_rot.size > 1:\n",
    "                 axes[3].imshow(norm_8u_rot, cmap='gray', origin='lower')\n",
    "            axes[3].set_title(\"Cropped\\n(No contour/crop)\", fontsize=10)\n",
    "            axes[3].axis('off')\n",
    "            # Append None to keep lists aligned for other cells that use it\n",
    "            cropped_images.append((i, file_name, date_obs, time_obs, None))\n",
    "            norm_8u_crop = None # Ensure it remains None for image_storage\n",
    "\n",
    "        plt.suptitle(f\"File: {file_name}\", fontsize=12)\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # Adjust layout to prevent title overlap\n",
    "        plt.show()\n",
    "\n",
    "        image_storage.append({\n",
    "            \"index\": i,\n",
    "            \"file_name\": file_name,\n",
    "            \"date_obs\": date_obs,\n",
    "            \"time_obs\": time_obs,\n",
    "            \"orig_image\": norm_8u_orig if norm_8u_orig.size > 1 else None,\n",
    "            \"rotated_image\": norm_8u_rot if norm_8u_rot.size > 1 else None,\n",
    "            \"contour_image\": overlay_img if overlay_img.size > 1 else None,\n",
    "            \"cropped_image\": norm_8u_crop if norm_8u_crop is not None and norm_8u_crop.size > 1 else None\n",
    "        })\n",
    "\n",
    "        # ---- ADDED: Append data to lists for NumPy arrays ----\n",
    "        all_param_tables.append(param_values)\n",
    "        all_normalized_images.append(norm_8u_orig if norm_8u_orig.size > 1 else None)\n",
    "        all_rotated_images.append(norm_8u_rot if norm_8u_rot.size > 1 else None)\n",
    "        all_contour_images.append(overlay_img if overlay_img.size > 1 else None)\n",
    "        all_cropped_images_for_npy.append(norm_8u_crop if norm_8u_crop is not None and norm_8u_crop.size > 1 else None)\n",
    "        # ---- END ADDED ----\n",
    "\n",
    "        end_section() # End processing for this file\n",
    "\n",
    "    # ---- ADDED: Save collected data as NumPy arrays ----\n",
    "    start_section(\"Saving NumPy arrays\")\n",
    "    output_dir = \"arrays\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    print(f\"\\nSaving NumPy arrays to '{output_dir}' directory...\")\n",
    "\n",
    "    try:\n",
    "        # Save Parameter Tables (as object array since it contains mixed types/strings)\n",
    "#        np.save(os.path.join(output_dir, \"parameter_tables.npy\"), np.array(all_param_tables, dtype=object))\n",
    "#        print(\"- Saved parameter_tables.npy\")\n",
    "\n",
    "#        # Save Image Arrays (using object dtype to handle potential None values or shape variations)\n",
    "#        np.save(os.path.join(output_dir, \"normalized_images.npy\"), np.array(all_normalized_images, dtype=object))\n",
    "#        print(\"- Saved normalized_images.npy\")\n",
    "\n",
    "#        np.save(os.path.join(output_dir, \"rotated_images.npy\"), np.array(all_rotated_images, dtype=object))\n",
    "#        print(\"- Saved rotated_images.npy\")\n",
    "\n",
    "#        np.save(os.path.join(output_dir, \"contour_ellipse_images.npy\"), np.array(all_contour_images, dtype=object))\n",
    "#        print(\"- Saved contour_ellipse_images.npy\")\n",
    "\n",
    "#        np.save(os.path.join(output_dir, \"cropped_images.npy\"), np.array(all_cropped_images_for_npy, dtype=object))\n",
    "#        print(\"- Saved cropped_images.npy\")\n",
    "\n",
    "        print(\"NumPy array saving complete.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving NumPy arrays: {e}\")\n",
    "    end_section() # End saving arrays\n",
    "    # ---- END ADDED ----\n",
    "\n",
    "    end_section() # End \"Processing images\" section\n",
    "    end_section() # End \"Script Start\" section (implicitly ends the last open section)\n",
    "\n",
    "# Always print time summary, even if no files were found initially\n",
    "print_time_summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8485b5b8-05c2-41c5-a995-ed39fc4330d5",
   "metadata": {},
   "source": [
    "Cell 5 parallelized much faster, only works when parallel_worker.py is in the same directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf31021-57ff-42e6-bb0d-d6f6ca107685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Parallelized Cell 5 (Notebook Version - Reduced Workers & Optional Saves) ==========\n",
    "\n",
    "# Ensure necessary imports from previous cells are available:\n",
    "# Make sure these are run in cells ABOVE this one:\n",
    "import numpy as np\n",
    "# import pandas as pd              # Not strictly needed in *this* cell if helpers are moved\n",
    "# import matplotlib.pyplot as plt  # Plotting is removed from parallel part\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "# from datetime import datetime   # Not directly used here, maybe by timing funcs?\n",
    "# import requests                # Used by helpers in worker file\n",
    "# import math                    # Used by helpers in worker file\n",
    "from astropy.io import fits     # Needed for initial metadata read\n",
    "# import cv2                     # Used by helpers in worker file\n",
    "# Helper functions assumed to be defined in previous cells:\n",
    "# start_section, end_section, print_time_summary\n",
    "\n",
    "# Import the parallelization library and the worker function\n",
    "import concurrent.futures\n",
    "import traceback # For detailed error printing if needed\n",
    "\n",
    "# **** IMPORT THE WORKER FUNCTION FROM THE .py FILE ****\n",
    "try:\n",
    "    from parallel_worker import process_single_file\n",
    "    print(\"Successfully imported 'process_single_file' from parallel_worker.py\")\n",
    "except ImportError as e:\n",
    "    print(f\"ERROR: Could not import 'process_single_file' from parallel_worker.py.\")\n",
    "    print(f\"Please ensure 'parallel_worker.py' exists in the same directory as the notebook.\")\n",
    "    print(f\"Import Error: {e}\")\n",
    "    # Optionally raise the error or exit if the import fails\n",
    "    raise e\n",
    "\n",
    "# --- Main Execution Block ---\n",
    "start_section(\"Script Start\")\n",
    "\n",
    "# --- Serial Part 1: File Discovery and Metadata Reading ---\n",
    "start_section(\"Glob FITS files\")\n",
    "# Ensure the path is correct for your system\n",
    "fits_files_pattern = \"jupiter_data_WFC3_F631DRC/MAST_2023_08_29T0311/HST/*/*.fits\"\n",
    "# fits_files_pattern = \"icpf19f7q_drc.fits\" # Example for single file testing\n",
    "fits_files = glob.glob(fits_files_pattern)\n",
    "end_section()\n",
    "\n",
    "if len(fits_files) == 0:\n",
    "    print(f\"No FITS files found matching pattern: '{fits_files_pattern}'\")\n",
    "    print_time_summary()\n",
    "else:\n",
    "    print(f\"Found {len(fits_files)} FITS files.\\n\")\n",
    "\n",
    "    file_details = []\n",
    "    start_section(\"Reading FITS metadata\")\n",
    "    processed_filenames = set() # Keep track of files already added\n",
    "\n",
    "    for fits_file_path in fits_files:\n",
    "        file_name = os.path.basename(fits_file_path)\n",
    "        # Skip if duplicate filename encountered (can happen with glob patterns sometimes)\n",
    "        if file_name in processed_filenames:\n",
    "             print(f\"Skipping duplicate file: {file_name}\")\n",
    "             continue\n",
    "\n",
    "        try:\n",
    "            # Use memory mapping for potentially large files (optional)\n",
    "            with fits.open(fits_file_path, memmap=True) as hdul:\n",
    "                try:\n",
    "                     header = hdul[0].header\n",
    "                     # Data usually in HDU 1 for HST WFC3/UVIS DRC files\n",
    "                     raw_data = hdul[1].data\n",
    "                     if raw_data is None: # Check if data exists in HDU 1\n",
    "                          print(f\"Warning: No data found in HDU 1 of {fits_file_path}, trying HDU 0.\")\n",
    "                          raw_data = hdul[0].data # Fallback to HDU 0\n",
    "\n",
    "                except IndexError:\n",
    "                     print(f\"Warning: Could not access HDU 1 in {fits_file_path}. Trying HDU 0.\")\n",
    "                     header = hdul[0].header\n",
    "                     raw_data = hdul[0].data\n",
    "\n",
    "\n",
    "                if raw_data is None:\n",
    "                    print(f\"Warning: No data found in {fits_file_path}. Skipping.\")\n",
    "                    continue # Skip this file if no data found\n",
    "\n",
    "                # --- Data conversion happens here before passing to worker ---\n",
    "                data = np.nan_to_num(raw_data.astype(np.float64), nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32)\n",
    "                total_flux = float(np.sum(data))\n",
    "                # --- End data conversion ---\n",
    "\n",
    "                date_obs = header.get('DATE-OBS', 'N/A')\n",
    "                time_obs = header.get('TIME-OBS', 'N/A')\n",
    "                telescope = header.get('TELESCOP', 'N/A')\n",
    "                instrument = header.get('INSTRUME', 'N/A')\n",
    "                exposure_time = header.get('EXPTIME', 'N/A')\n",
    "                t_filter = header.get('FILTER', 'N/A')\n",
    "\n",
    "                # Append tuple: (file_name, data_array, total_flux, date, time, tel, inst, exptime, filt)\n",
    "                file_details.append((file_name, data, total_flux, date_obs, time_obs, telescope, instrument, exposure_time, t_filter))\n",
    "                processed_filenames.add(file_name) # Mark as processed\n",
    "\n",
    "        except FileNotFoundError:\n",
    "             print(f\"Error: FITS file not found at {fits_file_path}. Skipping.\")\n",
    "        except OSError as e_os:\n",
    "             print(f\"Error reading file {fits_file_path} (OSError: {e_os}). Skipping.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading metadata/data from file {fits_file_path}: {e}\")\n",
    "            traceback.print_exc()\n",
    "    end_section() # End Reading FITS metadata\n",
    "\n",
    "    # Check if any file details were successfully read\n",
    "    if not file_details:\n",
    "         print(\"No file details could be read. Exiting processing.\")\n",
    "    else:\n",
    "        # --- Serial Part 2: Sorting ---\n",
    "        start_section(\"Sort file_details\")\n",
    "        file_details = sorted(file_details, key=lambda x: (x[3], x[4]))\n",
    "        end_section() # End Sort file_details\n",
    "\n",
    "        # --- Parallel Part: Processing Images ---\n",
    "        start_section(\"Processing images (Parallel)\")\n",
    "\n",
    "        ellipse_summaries = []\n",
    "        image_storage = []\n",
    "        all_param_tables = []\n",
    "        all_normalized_images = []\n",
    "        all_rotated_images = []\n",
    "        all_contour_images = []\n",
    "        all_cropped_images_for_npy = []\n",
    "        cropped_images = [] # For compatibility\n",
    "\n",
    "        # Prepare arguments: Tuple of (index, detail_tuple) for each file\n",
    "        worker_args = list(enumerate(file_details, start=1))\n",
    "\n",
    "        results = []\n",
    "        # *** REDUCED NUMBER OF WORKERS TO MITIGATE 503 ERRORS ***\n",
    "        # Adjust this number based on testing (e.g., 4, 3, 2)\n",
    "        max_workers_to_use = 4\n",
    "        # Ensure we don't request more workers than available cores or tasks\n",
    "        max_workers_to_use = min(max_workers_to_use, os.cpu_count() if os.cpu_count() else 1, len(worker_args))\n",
    "        print(f\"Using up to {max_workers_to_use} worker processes.\")\n",
    "\n",
    "        with concurrent.futures.ProcessPoolExecutor(max_workers=max_workers_to_use) as executor:\n",
    "            print(f\"Submitting {len(worker_args)} file processing tasks...\")\n",
    "            # Map the imported worker function\n",
    "            results = list(executor.map(process_single_file, worker_args))\n",
    "            print(\"All parallel processing tasks complete.\")\n",
    "\n",
    "        end_section() # End Processing images (Parallel)\n",
    "\n",
    "        # --- Serial Part 3: Aggregating Results and Saving ---\n",
    "        start_section(\"Aggregating results & Saving NumPy arrays\")\n",
    "        print(\"Aggregating results...\")\n",
    "        successful_results = 0\n",
    "        failed_files = []\n",
    "\n",
    "        for i, result in enumerate(results):\n",
    "            original_args = worker_args[i]\n",
    "            original_index = original_args[0]\n",
    "            original_filename = original_args[1][0]\n",
    "\n",
    "            if result is not None:\n",
    "                successful_results += 1\n",
    "                (idx, ellipse_summary, img_storage_entry, param_values,\n",
    "                 norm_orig, norm_rot, overlay, norm_crop_npy) = result\n",
    "\n",
    "                ellipse_summaries.append(ellipse_summary)\n",
    "                image_storage.append(img_storage_entry)\n",
    "                all_param_tables.append(param_values)\n",
    "                all_normalized_images.append(norm_orig)\n",
    "                all_rotated_images.append(norm_rot)\n",
    "                all_contour_images.append(overlay)\n",
    "                all_cropped_images_for_npy.append(norm_crop_npy)\n",
    "                cropped_images.append((idx, img_storage_entry[\"file_name\"],\n",
    "                                       img_storage_entry[\"date_obs\"], img_storage_entry[\"time_obs\"],\n",
    "                                       norm_crop_npy))\n",
    "            else:\n",
    "                print(f\"Result aggregation skipped for task index {i} (File original index: {original_index}, Name: {original_filename}) due to worker error.\")\n",
    "                failed_files.append(original_filename)\n",
    "                # Optionally append placeholders if needed by subsequent cells\n",
    "                # ellipse_summaries.append(None)\n",
    "                # ... etc ...\n",
    "\n",
    "\n",
    "        print(f\"Successfully aggregated results for {successful_results} out of {len(results)} tasks.\")\n",
    "        if failed_files:\n",
    "             print(f\"Failed files: {failed_files}\")\n",
    "\n",
    "        # Save collected data as NumPy arrays (only if there are results)\n",
    "        if successful_results > 0:\n",
    "            output_dir = \"arrays\"\n",
    "            # *** OPTIONAL: Change this path if saving to a different drive ***\n",
    "            # output_dir = \"/path/to/larger/disk/arrays\"\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            print(f\"\\nSaving NumPy arrays to '{output_dir}' directory...\")\n",
    "            try:\n",
    "                # --- Save Parameter Tables (Usually essential) ---\n",
    "                # np.save(os.path.join(output_dir, \"parameter_tables.npy\"), np.array(all_param_tables, dtype=object))\n",
    "                print(\"- Saved parameter_tables.npy\")\n",
    "\n",
    "                # *** OPTIONAL SAVES: Comment out lines below to save disk space ***\n",
    "\n",
    "                # np.save(os.path.join(output_dir, \"normalized_images.npy\"), np.array(all_normalized_images, dtype=object))\n",
    "                # print(\"- Saved normalized_images.npy\")\n",
    "\n",
    "                # np.save(os.path.join(output_dir, \"rotated_images.npy\"), np.array(all_rotated_images, dtype=object))\n",
    "                # print(\"- Saved rotated_images.npy\")\n",
    "\n",
    "                # np.save(os.path.join(output_dir, \"contour_ellipse_images.npy\"), np.array(all_contour_images, dtype=object))\n",
    "                # print(\"- Saved contour_ellipse_images.npy\")\n",
    "\n",
    "                # --- Save Cropped Images (Often essential for later steps) ---\n",
    "                # np.save(os.path.join(output_dir, \"cropped_images.npy\"), np.array(all_cropped_images_for_npy, dtype=object))\n",
    "                print(\"- Saved cropped_images.npy\")\n",
    "\n",
    "                # *** End Optional Saves ***\n",
    "\n",
    "                print(\"NumPy array saving complete.\")\n",
    "            except OSError as e_os:\n",
    "                 # Specifically catch disk space error\n",
    "                 if e_os.errno == 28: # Errno 28: No space left on device\n",
    "                     print(f\"\\nError saving NumPy arrays: [Errno 28] No space left on device.\")\n",
    "                     print(\"Please free up disk space or comment out optional saves in the script.\")\n",
    "                 else:\n",
    "                     print(f\"Error saving NumPy arrays (OS Error): {e_os}\")\n",
    "                 traceback.print_exc()\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving NumPy arrays: {e}\")\n",
    "                traceback.print_exc()\n",
    "        else:\n",
    "             print(\"Skipping NumPy array saving as no results were successfully processed.\")\n",
    "\n",
    "\n",
    "        end_section() # End Aggregating results & Saving NumPy arrays\n",
    "\n",
    "end_section() # End Script Start section\n",
    "\n",
    "# Always print time summary\n",
    "print_time_summary()\n",
    "\n",
    "# --- End of Parallelized Cell 5 ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c7e76a-66d9-4294-bf1e-ab641d95c144",
   "metadata": {},
   "source": [
    "Cell 5a and 5b to look at arrays to ensure proper loading and file output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8750594-9867-4568-8509-09feb101a224",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Define the path to the .npy file\n",
    "file_path = os.path.join(\"arrays\", \"parameter_tables.npy\")\n",
    "\n",
    "# Check if the file exists\n",
    "if not os.path.exists(file_path):\n",
    "    print(f\"Error: File not found at '{file_path}'\")\n",
    "    print(\"Please ensure you have run Cell 5 and the 'arrays' folder exists with the .npy file.\")\n",
    "else:\n",
    "    try:\n",
    "        # Load the NumPy array, allowing pickles is necessary for object arrays\n",
    "        all_tables = np.load(file_path, allow_pickle=True)\n",
    "\n",
    "        print(f\"Successfully loaded {len(all_tables)} parameter tables from '{file_path}'.\\n\")\n",
    "\n",
    "        # Iterate through each table (corresponding to each processed FITS file)\n",
    "        for i, table_data in enumerate(all_tables):\n",
    "            print(f\"--- Parameter Table for File {i+1} ---\")\n",
    "\n",
    "            # Check if table_data is a list or similar iterable\n",
    "            if hasattr(table_data, '__iter__'):\n",
    "                 # Iterate through the (parameter_name, parameter_value) tuples in the table\n",
    "                for param_name, param_value in table_data:\n",
    "                    print(f\"{param_name}: {param_value}\")\n",
    "            else:\n",
    "                 print(\"Unexpected data format in table:\", table_data) # Handle unexpected format\n",
    "\n",
    "            print(\"-\" * (len(f\"--- Parameter Table for File {i+1} ---\") + 1)) # Print separator\n",
    "            print() # Add a blank line for readability\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading or reading the file: {e}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8253305d-0325-45ec-abbd-5a4bc9721560",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2 # Still potentially useful if any images were unexpectedly saved in color\n",
    "\n",
    "# Define the path to the .npy file\n",
    "file_path = os.path.join(\"arrays\", \"cropped_images.npy\") # Corrected path based on prompt\n",
    "\n",
    "# Check if the file exists\n",
    "if not os.path.exists(file_path):\n",
    "    print(f\"Error: File not found at '{file_path}'\")\n",
    "    print(\"Please ensure you have run Cell 5 and the 'arrays' folder exists with the .npy file.\")\n",
    "else:\n",
    "    try:\n",
    "        # Load the NumPy array, allowing pickles is necessary for object arrays\n",
    "        all_images = np.load(file_path, allow_pickle=True)\n",
    "\n",
    "        print(f\"Successfully loaded {len(all_images)} images from '{file_path}'.\\n\")\n",
    "        print(\"Displaying images one by one in grayscale...\")\n",
    "\n",
    "        # Iterate through each image array in the loaded data\n",
    "        for i, img_data in enumerate(all_images):\n",
    "\n",
    "            print(f\"\\n--- Cropped Image {i+1} ---\") # Updated title indicator\n",
    "\n",
    "            # Check if the loaded item is actually a numpy array (an image)\n",
    "            if isinstance(img_data, np.ndarray) and img_data.size > 1:\n",
    "                try:\n",
    "                    # Initialize img_to_display\n",
    "                    img_to_display = None\n",
    "\n",
    "                    # Check dimensions - expecting grayscale (2D)\n",
    "                    if img_data.ndim == 2:\n",
    "                        # It's already grayscale\n",
    "                        img_to_display = img_data\n",
    "                        print(\"(Image is 2D, displaying as grayscale)\")\n",
    "                    elif img_data.ndim == 3 and img_data.shape[2] == 3:\n",
    "                        # Unexpected 3-channel image, convert to grayscale\n",
    "                        img_to_display = cv2.cvtColor(img_data, cv2.COLOR_BGR2GRAY)\n",
    "                        print(\"(Image was 3-channel, converted to grayscale for display)\")\n",
    "                    elif img_data.ndim == 3 and img_data.shape[2] == 1:\n",
    "                         # Grayscale but with an extra dimension, squeeze it\n",
    "                         img_to_display = np.squeeze(img_data, axis=2)\n",
    "                         print(\"(Image had singleton 3rd dimension, squeezed to 2D)\")\n",
    "                    else:\n",
    "                        print(f\"(Image has unexpected shape {img_data.shape}, skipping display)\")\n",
    "                        continue # Skip to next image if shape is wrong\n",
    "\n",
    "                    # Display the grayscale image\n",
    "                    plt.figure(figsize=(6, 6)) # Create a new figure for each image\n",
    "                    # Use cmap='gray' to ensure grayscale display\n",
    "                    plt.imshow(img_to_display, cmap='gray', origin='lower')\n",
    "                    plt.title(f\"Cropped Image #{i+1}\") # Updated title\n",
    "                    plt.axis('off') # Hide axes\n",
    "                    plt.show() # Display the current image\n",
    "                    print(\"Displayed.\")\n",
    "\n",
    "                except Exception as display_error:\n",
    "                     print(f\"Error displaying image #{i+1}: {display_error}\")\n",
    "\n",
    "            elif img_data is None:\n",
    "                print(\"Image data is None (likely skipped during processing).\")\n",
    "            else:\n",
    "                print(f\"Item #{i+1} is not a valid image array. Data: {type(img_data)}\")\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading or processing the file: {e}\")\n",
    "\n",
    "print(\"\\nScript finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481b6d47-2fc1-4910-b806-40cd19a5af7b",
   "metadata": {},
   "source": [
    "CELL 6 - this block makes sure that the right value is actually being used in the outputs above for the values of things that i need to be seeing, if needed to run, just uncomment it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b1d0e3-04ba-4841-83e0-08250add8a6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "# TEST CASE Cell 6 : Debug / Test Horizons\n",
    "\n",
    "def test_horizons_api(file_index=0):\n",
    "    if not file_details:\n",
    "        print(\"No file_details found. Make sure you've run the main analysis cell(s).\")\n",
    "        return\n",
    "\n",
    "    if file_index < 0 or file_index >= len(file_details):\n",
    "        print(f\"Invalid file_index {file_index}; valid range is [0..{len(file_details)-1}]\")\n",
    "        return\n",
    "\n",
    "    (\n",
    "        file_name, data, total_flux, date_obs, time_obs,\n",
    "        telescope, instrument, exposure_time, t_filter\n",
    "    ) = file_details[file_index]\n",
    "\n",
    "    print(\"=== Selected FITS File Information ===\")\n",
    "    print(f\"Index:         {file_index}\")\n",
    "    print(f\"File Name:     {file_name}\")\n",
    "    print(f\"DATE-OBS:      {date_obs}\")\n",
    "    print(f\"TIME-OBS:      {time_obs}\")\n",
    "    print(f\"Telescope:     {telescope}\")\n",
    "    print(f\"Instrument:    {instrument}\")\n",
    "    print(f\"Filter:        {t_filter}\")\n",
    "    print(f\"Exposure Time: {exposure_time}\")\n",
    "    print(\"======================================\\n\")\n",
    "\n",
    "    horizons_input = create_input_content(date_obs, time_obs)\n",
    "\n",
    "    print(\"=== Content Sent to Horizons API ===\")\n",
    "    print(horizons_input)\n",
    "    print(\"=====================================\\n\")\n",
    "\n",
    "    import requests\n",
    "\n",
    "    horizons_url_debug = 'https://ssd.jpl.nasa.gov/api/horizons_file.api'\n",
    "    \n",
    "    debug_response = requests.post(\n",
    "        horizons_url_debug,\n",
    "        data={'format': 'text'},\n",
    "        files={'input': ('input.txt', horizons_input)}\n",
    "    )\n",
    "\n",
    "    if debug_response.status_code != 200:\n",
    "        print(f\"Failed Horizons request: {debug_response.status_code}\")\n",
    "        return\n",
    "\n",
    "    horizons_full_text = debug_response.text\n",
    "\n",
    "    print(\"=== Full Horizons API Response ===\")\n",
    "    print(horizons_full_text)\n",
    "    print(\"==================================\\n\")\n",
    "\n",
    "    lines = horizons_full_text.splitlines()\n",
    "    \n",
    "    try:\n",
    "        start_index = next(i for i, line in enumerate(lines) if '$$SOE' in line)\n",
    "        end_index = next(i for i, line in enumerate(lines) if '$$EOE' in line)\n",
    "    except StopIteration:\n",
    "        print(\"Could not find $$SOE/$$EOE in Horizons output—parsing aborted.\")\n",
    "        return\n",
    "\n",
    "    ephem_lines = lines[start_index+1 : end_index]\n",
    "\n",
    "    dist_au_extracted = None\n",
    "    s_brt_extracted = None\n",
    "    relevant_line = None\n",
    "\n",
    "    for ln in ephem_lines:\n",
    "        ln = ln.strip()\n",
    "        if not ln:\n",
    "            continue\n",
    "        tokens = ln.split()\n",
    "        relevant_line = ln\n",
    "        try:\n",
    "            s_brt_extracted = float(tokens[3])\n",
    "            dist_au_extracted = float(tokens[4])\n",
    "        except:\n",
    "            pass\n",
    "        break\n",
    "\n",
    "    print(\"=== Parsing Ephemeris Lines ===\")\n",
    "    if relevant_line:\n",
    "        print(f\"Line used for parsing:\\n{relevant_line}\\n\")\n",
    "        print(f\"Extracted distance (delta) [OLD WAY]: {dist_au_extracted}\")\n",
    "        print(f\"Extracted surface brightness (S-brt) [OLD WAY]: {s_brt_extracted}\")\n",
    "    else:\n",
    "        print(\"No valid line found with ephemeris data.\")\n",
    "    print(\"================================\\n\")\n",
    "\n",
    "    debug_delta, debug_sbrt = get_horizons_data(date_obs, time_obs)\n",
    "\n",
    "    print(\"=== Comparison with get_horizons_data() ===\")\n",
    "    print(f\"[CORRECT] get_horizons_data returned delta = {debug_delta}, s_brt = {debug_sbrt}\")\n",
    "    print(\"===========================================\\n\")\n",
    "\n",
    "test_horizons_api(file_index=0)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2b33aa-6ce3-4f3f-b4b3-1d2d6ed3ab56",
   "metadata": {},
   "source": [
    "Cell 7 - Final Data Summaries & Plots given in a whole table thats easy to read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1da9dc-256c-48ca-9b08-eecc5228ddb8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cell 7 : Final Data Summaries & Plots\n",
    "\n",
    "def print_dynamic_table(rows):\n",
    "    str_rows = [[str(item) for item in row] for row in rows]\n",
    "    num_cols = len(str_rows[0])\n",
    "    col_widths = [max(len(row[i]) for row in str_rows) for i in range(num_cols)]\n",
    "    \n",
    "    header_row = str_rows[0]\n",
    "    sep_line = [\"-\" * col_widths[i] for i in range(num_cols)]\n",
    "    \n",
    "    print(\"  \".join(cell.center(col_widths[i]) for i, cell in enumerate(header_row)))\n",
    "    print(\"  \".join(sep_line))\n",
    "    \n",
    "    for row in str_rows[1:]:\n",
    "        print(\"  \".join(cell.center(col_widths[i]) for i, cell in enumerate(row)))\n",
    "\n",
    "table_rows = [\n",
    "    [\"File #\", \"File Name\", \"SB (Jupiter)\", \"Ellipse Flux\", \"delta\",\n",
    "     \"Error Bars EF\", \"Flux Average\", \"Error Bars FA\", \"DATE-OBS\", \"TIME-OBS\"]\n",
    "]\n",
    "\n",
    "for (file_num, f_name, s_brt, e_flux, delta, ef_err, f_avg, fa_err, d_obs, t_obs) in ellipse_summaries:\n",
    "    s_brt_str = f\"{s_brt:.3f}\" if s_brt else \"N/A\"\n",
    "    e_flux_str = f\"{e_flux:.2f}\" if e_flux else \"N/A\"\n",
    "    delta_str = f\"{delta:.5f}\" if delta else \"N/A\"\n",
    "    ef_err_str = f\"{ef_err:.2f}\" if ef_err else \"N/A\"\n",
    "    f_avg_str = f\"{f_avg:.3f}\" if f_avg else \"N/A\"\n",
    "    fa_err_str = f\"{fa_err:.3f}\" if fa_err else \"N/A\"\n",
    "\n",
    "    new_row = [\n",
    "        str(file_num), f_name, s_brt_str, e_flux_str, delta_str,\n",
    "        ef_err_str, f_avg_str, fa_err_str, d_obs, t_obs\n",
    "    ]\n",
    "    \n",
    "    table_rows.append(new_row)\n",
    "\n",
    "print_dynamic_table(table_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae489321-c2f2-405d-b18e-6729859b3408",
   "metadata": {},
   "source": [
    "CELL 8 - The surface brightness and the delta of the sun over the course of 5 years to use in the upcoming graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1abd23-eef6-4271-848f-c05cb78df491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8 : The sun graph (Modified for dot markers for apparent magnitude)\n",
    "\n",
    "url = \"https://ssd.jpl.nasa.gov/api/horizons_file.api\"\n",
    "\n",
    "input_content = \"\"\"\n",
    "!$$SOF\n",
    "COMMAND='10'\n",
    "CENTER='500@-48'\n",
    "MAKE_EPHEM='YES'\n",
    "EPHEM_TYPE='OBSERVER'\n",
    "START_TIME='2015-01-19 00:00:00'\n",
    "STOP_TIME='2019-07-21 23:59:00'\n",
    "STEP_SIZE='60m'\n",
    "QUANTITIES='9,20'\n",
    "OUT_UNITS='KM-S'\n",
    "CSV_FORMAT='NO'\n",
    "!$$EOF\n",
    "\"\"\"\n",
    "\n",
    "params = {\"format\": \"text\"}\n",
    "\n",
    "response = requests.post(url, data=params, files={'input': ('input.txt', input_content)})\n",
    "\n",
    "lines = response.text.splitlines()\n",
    "\n",
    "start_idx, end_idx = None, None\n",
    "for i, line in enumerate(lines):\n",
    "    if \"$$SOE\" in line:\n",
    "        start_idx = i\n",
    "    if \"$$EOE\" in line:\n",
    "        end_idx = i\n",
    "        break\n",
    "\n",
    "timestamps, apmag_vals, delta_vals = [], [], []\n",
    "\n",
    "if start_idx is not None and end_idx is not None:\n",
    "    for line in lines[start_idx+1:end_idx]:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        tokens = line.split()\n",
    "        if len(tokens) >= 5:\n",
    "            dt_str = tokens[0] + \" \" + tokens[1]\n",
    "            try:\n",
    "                dt = pd.to_datetime(dt_str)\n",
    "                apmag = float(tokens[2])\n",
    "                delta = float(tokens[4])\n",
    "                timestamps.append(dt)\n",
    "                apmag_vals.append(apmag)\n",
    "                delta_vals.append(delta)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"datetime\": timestamps,\n",
    "    \"apmag\": apmag_vals,\n",
    "    \"delta\": delta_vals\n",
    "})\n",
    "\n",
    "df.sort_values(\"datetime\", inplace=True)\n",
    "\n",
    "start_time = df[\"datetime\"].min()\n",
    "df[\"days_since_start\"] = (df[\"datetime\"] - start_time).dt.total_seconds() / 86400.0\n",
    "\n",
    "fig, ax_left = plt.subplots(figsize=(12, 6))\n",
    "ax_right = ax_left.twinx()\n",
    "\n",
    "ax_left.plot(df[\"days_since_start\"], df[\"delta\"], color='blue', label='Delta')\n",
    "ax_left.set_xlabel(\"Days Since Start\")\n",
    "ax_left.set_ylabel(\"Delta\", color='blue')\n",
    "ax_left.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "ax_right.scatter(df[\"days_since_start\"], df[\"apmag\"], color='red', s=0.5, label='Apparent Mag')\n",
    "ax_right.set_ylabel(\"Apparent Magnitude\", color='red')\n",
    "ax_right.tick_params(axis='y', labelcolor='red')\n",
    "\n",
    "plt.title(\"Apparent Magnitude (Right Axis) and Delta (Left Axis) vs. Time\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f04f6c4-8886-46ff-b0d1-12e36c5b8b31",
   "metadata": {},
   "source": [
    "CELL 8 TEST CASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc1f5cd-a7c8-4d61-853d-afb4c7dc7f67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cell 8 (TEST CASE): The sun graph (Modified for dot markers for apparent magnitude)\n",
    "'''\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the Horizons API endpoint\n",
    "url = \"https://ssd.jpl.nasa.gov/api/horizons_file.api\"\n",
    "\n",
    "# Define the input content for Horizons API\n",
    "input_content = \"\"\"\n",
    "!$$SOF\n",
    "COMMAND='10'\n",
    "CENTER='500@-48'\n",
    "MAKE_EPHEM='YES'\n",
    "EPHEM_TYPE='OBSERVER'\n",
    "START_TIME='2015-01-19 00:00:00'\n",
    "STOP_TIME='2019-07-21 23:59:00'\n",
    "STEP_SIZE='60m'\n",
    "QUANTITIES='9,20'\n",
    "OUT_UNITS='KM-S'\n",
    "CSV_FORMAT='NO'\n",
    "!$$EOF\n",
    "\"\"\"\n",
    "\n",
    "# Define request parameters\n",
    "params = {\"format\": \"text\"}\n",
    "\n",
    "# Send request to Horizons API\n",
    "print(\"=== Sending Request to Horizons API ===\")\n",
    "response = requests.post(url, data=params, files={'input': ('input.txt', input_content)})\n",
    "\n",
    "# Check if request was successful\n",
    "if response.status_code != 200:\n",
    "    print(f\"Failed Horizons request: {response.status_code}\")\n",
    "    exit()\n",
    "\n",
    "# Read response text\n",
    "response_text = response.text\n",
    "lines = response_text.splitlines()\n",
    "\n",
    "# Display first 1500 characters of the response for debugging\n",
    "#print(\"=== Full Horizons API Response Preview ===\")\n",
    "#print(response_text[:1500])\n",
    "#print(\"===========================================\\n\")\n",
    "\n",
    "# Find start and end indices for data extraction\n",
    "start_idx, end_idx = None, None\n",
    "for i, line in enumerate(lines):\n",
    "    if \"$$SOE\" in line:\n",
    "        start_idx = i\n",
    "    if \"$$EOE\" in line:\n",
    "        end_idx = i\n",
    "        break\n",
    "\n",
    "# Initialize lists for extracted data\n",
    "timestamps, apmag_vals, delta_vals = [], [], []\n",
    "\n",
    "# Extract data if valid start and end indices found\n",
    "if start_idx is not None and end_idx is not None:\n",
    "#    print(\"=== Extracted Data Points ===\")\n",
    "    for line in lines[start_idx+1:end_idx]:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        tokens = line.split()\n",
    "        if len(tokens) >= 5:\n",
    "            dt_str = tokens[0] + \" \" + tokens[1]\n",
    "            try:\n",
    "                dt = pd.to_datetime(dt_str)\n",
    "                apmag = float(tokens[2])\n",
    "                delta = float(tokens[4])\n",
    "                timestamps.append(dt)\n",
    "                apmag_vals.append(apmag)\n",
    "                delta_vals.append(delta)\n",
    "#                print(f\"Timestamp: {dt}, Apparent Mag: {apmag}, Delta: {delta}\")  # Print extracted values\n",
    "            except Exception as e:\n",
    "                print(f\"Skipping line due to error: {e}\")\n",
    "#    print(\"========================================\\n\")\n",
    "else:\n",
    "    print(\"Error: Could not find valid $$SOE or $$EOE markers in response.\")\n",
    "    exit()\n",
    "\n",
    "# Convert extracted data into a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    \"datetime\": timestamps,\n",
    "    \"apmag\": apmag_vals,\n",
    "    \"delta\": delta_vals\n",
    "})\n",
    "\n",
    "# Sort values by datetime\n",
    "df.sort_values(\"datetime\", inplace=True)\n",
    "\n",
    "# Compute days since start time\n",
    "start_time = df[\"datetime\"].min()\n",
    "df[\"days_since_start\"] = (df[\"datetime\"] - start_time).dt.total_seconds() / 86400.0\n",
    "\n",
    "# Display the first few extracted values for verification\n",
    "print(\"=== List of Delta Values ===\")\n",
    "print(df[\"delta\"].tolist())\n",
    "print(\"================================\\n\")\n",
    "\n",
    "print(\"=== List of Apparent Magnitude Values ===\")\n",
    "print(df[\"apmag\"].tolist())\n",
    "print(\"=========================================\\n\")\n",
    "\n",
    "# Generate side-by-side scatter plots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Scatter plot for Delta\n",
    "axes[0].scatter(df[\"days_since_start\"], df[\"delta\"], color='blue', s=1)\n",
    "axes[0].set_xlabel(\"Days Since Start\")\n",
    "axes[0].set_ylabel(\"Delta\")\n",
    "axes[0].set_title(\"Delta vs. Time\")\n",
    "\n",
    "# Scatter plot for Apparent Magnitude\n",
    "axes[1].scatter(df[\"days_since_start\"], df[\"apmag\"], color='red', s=1)\n",
    "axes[1].set_xlabel(\"Days Since Start\")\n",
    "axes[1].set_ylabel(\"Apparent Magnitude\")\n",
    "axes[1].set_title(\"Apparent Magnitude vs. Time\")\n",
    "\n",
    "# Adjust layout for better viewing\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067d192e-0011-4eae-a3d9-4507b8993cbf",
   "metadata": {},
   "source": [
    "Cell 9 : 2. DELTA, Average Flux, and SB vs. Time Plots (Side by Side)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39dff12f-67e2-4092-a581-bb3b19f85f1c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cell 9 : 2. DELTA, Average Flux, and SB vs. Time Plots (Side by Side)\n",
    "\n",
    "delta_records, flux_records, sb_records = [], [], []\n",
    "\n",
    "for file_num, f_name, s_brt, e_flux, dist_au, ef_err, f_avg, fa_err, d_obs, t_obs in ellipse_summaries:\n",
    "    if d_obs != \"N/A\" and t_obs != \"N/A\":\n",
    "\n",
    "        dt_parsed = pd.to_datetime(f\"{d_obs} {t_obs}\")\n",
    "        delta_records.append((dt_parsed, dist_au))\n",
    "        flux_records.append((dt_parsed, f_name, f_avg))\n",
    "        sb_records.append((dt_parsed, s_brt))\n",
    "\n",
    "df_delta = pd.DataFrame(delta_records, columns=[\"datetime\", \"delta_au\"]).sort_values(\"datetime\")\n",
    "df_flux = pd.DataFrame(flux_records, columns=[\"datetime\", \"filename\", \"f_avg\"]).sort_values(\"datetime\")\n",
    "df_sb = pd.DataFrame(sb_records, columns=[\"datetime\", \"s_brt\"]).sort_values(\"datetime\")\n",
    "\n",
    "start_time = df_delta[\"datetime\"].min()\n",
    "\n",
    "df_delta[\"days_since_start\"] = (df_delta[\"datetime\"] - start_time).dt.total_seconds() / 86400.0\n",
    "df_flux[\"days_since_start\"]  = (df_flux[\"datetime\"]  - start_time).dt.total_seconds() / 86400.0\n",
    "df_sb[\"days_since_start\"]    = (df_sb[\"datetime\"]    - start_time).dt.total_seconds() / 86400.0\n",
    "\n",
    "# -------------------- PLOT 1: side-by-side figures --------------------\n",
    "fig, axs = plt.subplots(ncols=3, figsize=(18, 6))\n",
    "\n",
    "axs[0].scatter(df_delta[\"days_since_start\"], df_delta[\"delta_au\"], color='black', alpha=0.75)\n",
    "axs[0].set_xlabel(\"Days Since Start\")\n",
    "axs[0].set_ylabel(\"Delta (AU)\")\n",
    "axs[0].set_title(\"Distance (Delta) vs. Time\")\n",
    "\n",
    "axs[1].scatter(df_flux[\"days_since_start\"], df_flux[\"f_avg\"], color='blue', alpha=0.75)\n",
    "axs[1].set_xlabel(\"Days Since Start\")\n",
    "axs[1].set_ylabel(\"Average Flux\")\n",
    "axs[1].set_title(\"Average Flux vs. Time\")\n",
    "\n",
    "axs[2].scatter(df_sb[\"days_since_start\"], df_sb[\"s_brt\"], color='green', alpha=0.75)\n",
    "axs[2].set_xlabel(\"Days Since Start\")\n",
    "axs[2].set_ylabel(\"Surface Brightness\")\n",
    "axs[2].set_title(\"Surface Brightness vs. Time\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -------------------- PLOT 2: Overlay of Delta & Average Flux vs. Time --------------------\n",
    "fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "ax1.scatter(df_delta[\"days_since_start\"], df_delta[\"delta_au\"], color='black', alpha=0.75)\n",
    "ax1.set_xlabel(\"Days Since Start\")\n",
    "ax1.set_ylabel(\"Delta (AU)\", color='black')\n",
    "ax1.tick_params(axis='y', labelcolor='black')\n",
    "\n",
    "ax1_right = ax1.twinx()\n",
    "ax1_right.scatter(df_flux[\"days_since_start\"], df_flux[\"f_avg\"], color='blue', alpha=0.75)\n",
    "ax1_right.set_ylabel(\"Average Flux\", color='blue')\n",
    "ax1_right.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "plt.title(\"Overlay: Delta and Average Flux vs. Time\")\n",
    "plt.show()\n",
    "\n",
    "# -------------------- PLOT 3: Overlay of Delta & Surface Brightness vs. Time --------------------\n",
    "fig, ax2 = plt.subplots(figsize=(12, 6))\n",
    "ax2.scatter(df_delta[\"days_since_start\"], df_delta[\"delta_au\"], color='black', alpha=0.75)\n",
    "ax2.set_xlabel(\"Days Since Start\")\n",
    "ax2.set_ylabel(\"Delta (AU)\", color='black')\n",
    "ax2.tick_params(axis='y', labelcolor='black')\n",
    "\n",
    "ax2_right = ax2.twinx()\n",
    "ax2_right.scatter(df_sb[\"days_since_start\"], df_sb[\"s_brt\"], color='green', alpha=0.75)\n",
    "ax2_right.set_ylabel(\"Surface Brightness\", color='green')\n",
    "ax2_right.tick_params(axis='y', labelcolor='green')\n",
    "\n",
    "plt.title(\"Overlay: Delta and Surface Brightness vs. Time\")\n",
    "plt.show()\n",
    "\n",
    "# -------------------- PLOT 4: Overlay of Average Flux & Surface Brightness vs. Time --------------------\n",
    "fig, ax3 = plt.subplots(figsize=(12, 6))\n",
    "ax3.scatter(df_flux[\"days_since_start\"], df_flux[\"f_avg\"], color='blue', alpha=0.75)\n",
    "ax3.set_xlabel(\"Days Since Start\")\n",
    "ax3.set_ylabel(\"Average Flux\", color='blue')\n",
    "ax3.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "ax3_right = ax3.twinx()\n",
    "ax3_right.scatter(df_sb[\"days_since_start\"], df_sb[\"s_brt\"], color='green', alpha=0.75)\n",
    "ax3_right.set_ylabel(\"Surface Brightness\", color='green')\n",
    "ax3_right.tick_params(axis='y', labelcolor='green')\n",
    "\n",
    "plt.title(\"Overlay: Average Flux and Surface Brightness vs. Time\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa2bcd9-e555-4e20-85ac-b39f1eb61548",
   "metadata": {},
   "source": [
    "CELL 10 - overlay to see all the plots on a single graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15472cf6-3967-4c57-9836-180df430d9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10 : quadruple overlay\n",
    "\n",
    "from mpl_toolkits.axes_grid1 import host_subplot\n",
    "import mpl_toolkits.axisartist as AA\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create figure and host subplot\n",
    "fig = plt.figure(figsize=(14, 8))\n",
    "host = host_subplot(111, axes_class=AA.Axes)\n",
    "plt.subplots_adjust(right=0.75)\n",
    "\n",
    "# Create parasite axes\n",
    "par1, par2, par3, par4 = host.twinx(), host.twinx(), host.twinx(), host.twinx()\n",
    "\n",
    "# Offset the parasite axes\n",
    "offset = 60\n",
    "for par in [par2, par3, par4]:\n",
    "    new_fixed_axis = par.get_grid_helper().new_fixed_axis\n",
    "    par.axis[\"right\"] = new_fixed_axis(loc=\"right\", axes=par, offset=(offset, 0))\n",
    "    offset += 60\n",
    "\n",
    "# Plot data\n",
    "p1, = host.plot(df[\"days_since_start\"], df[\"delta\"], color='blue', label='Sun Delta')\n",
    "p2 = par1.scatter(df[\"days_since_start\"], df[\"apmag\"], color='red', s=1, label='Sun Apparent Mag')\n",
    "p3 = par2.scatter(df_delta[\"days_since_start\"], df_delta[\"delta_au\"], color='black', alpha=0.75, label='Jupiter Delta')\n",
    "p4 = par3.scatter(df_flux[\"days_since_start\"], df_flux[\"f_avg\"], color='blue', alpha=0.75, label='Jupiter Avg Flux')\n",
    "p5 = par4.scatter(df_sb[\"days_since_start\"], df_sb[\"s_brt\"], color='green', alpha=0.75, label='Jupiter SB')\n",
    "\n",
    "# Axis labels\n",
    "host.set_xlabel(\"Days Since Start\")\n",
    "host.set_ylabel(\"Sun Delta\")\n",
    "par1.set_ylabel(\"Sun Apparent Mag\")\n",
    "par2.set_ylabel(\"Jupiter Delta\")\n",
    "par3.set_ylabel(\"Jupiter Avg Flux\")\n",
    "par4.set_ylabel(\"Jupiter Surface Brightness\")\n",
    "\n",
    "# Color labels\n",
    "host.axis[\"left\"].label.set_color('blue')\n",
    "par1.axis[\"right\"].label.set_color('red')\n",
    "par2.axis[\"right\"].label.set_color('black')\n",
    "par3.axis[\"right\"].label.set_color('blue')\n",
    "par4.axis[\"right\"].label.set_color('green')\n",
    "\n",
    "# X-axis tick intervals\n",
    "max_days = max(df[\"days_since_start\"].max(), df_delta[\"days_since_start\"].max(), df_flux[\"days_since_start\"].max(), df_sb[\"days_since_start\"].max())\n",
    "host.set_xticks(np.arange(0, max_days + 100, 100))\n",
    "\n",
    "# Plot title\n",
    "plt.title(\"Combined Quadruple Axis Plot (5 Y-Axes) vs. Time\")\n",
    "plt.draw()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57987362-25b2-4c08-9973-58c817dece7e",
   "metadata": {},
   "source": [
    "CELL 11A - a series of subplots for Average flux vs time with the respective images to go along with the data points<br>\n",
    "CELL 11B - a series of subplots for delta vs time with the respective images to go along with the data points<br>\n",
    "CELL 11C - a series of subplots for Surface Brightness vs time with the respective images to go along with the data points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c53240-a518-475a-8ee0-384a10b82cc2",
   "metadata": {},
   "source": [
    "TEST CASE BEING CREATED FOR CELL 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfc8baf-26fe-478c-9fa1-bd00689afb9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ====================== GRAND CELL: 11A, 11B, and 11C ======================\n",
    "# This single cell:\n",
    "#   1) Forces each of df_flux, df_delta, df_sb to have \"filename\" columns\n",
    "#   2) Then runs DBSCAN + plotting code for 11A, 11B, 11C in order\n",
    "#   3) Displays images automatically if the new \"filename\" columns match cropped_images\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# ADDED IMPORT to control tick formatting\n",
    "import matplotlib.ticker as mticker\n",
    "\n",
    "###############################################################################\n",
    "#          GLOBAL ADJUSTABLE SIZES (POINTS, ANNOTATIONS, FILE NUMBERS)\n",
    "###############################################################################\n",
    "# Feel free to change these values as needed:\n",
    "POINT_SIZE = 50                # Size of the scatter data points\n",
    "ANNOTATION_FONTSIZE = 16        # Font size for the numbers that appear next to points\n",
    "FILE_NUMBER_FONTSIZE = 16       # Font size for \"File #XX\" titles on each image\n",
    "\n",
    "###############################################################################\n",
    "#                  GLOBAL FONT AND SIZE ADJUSTMENTS FOR PLOTS\n",
    "###############################################################################\n",
    "# By placing these at the very end, all future figures will automatically\n",
    "# use these settings. You can tweak them all in one place.\n",
    "plt.rcParams.update({\n",
    "    \"font.size\": 16,                   # Base font size for everything\n",
    "    \"axes.labelsize\": 16,              # Axis label font size\n",
    "    \"axes.titlesize\": 18,              # Title font size\n",
    "    \"legend.fontsize\": 12,             # Legend font size\n",
    "    \"xtick.labelsize\": 16,             # X tick label size\n",
    "    \"ytick.labelsize\": 16,             # Y tick label size\n",
    "    \"axes.formatter.useoffset\": False  # Disable offset notation globally\n",
    "})\n",
    "\n",
    "###############################################################################\n",
    "#             STEP 0: FORCE 'filename' COLUMN IN DF_DELTA AND DF_SB\n",
    "###############################################################################\n",
    "# (We assume df_flux ALREADY has 'filename'. If not, adapt as needed.)\n",
    "# We'll merge df_delta and df_sb on 'datetime' to bring over 'filename' from df_flux.\n",
    "\n",
    "print(\"=== MERGING to ensure each DataFrame has a 'filename' column ===\\n\")\n",
    "\n",
    "# Make copies so we don't mutate the originals\n",
    "df_flux_merged  = df_flux.copy()\n",
    "df_delta_merged = df_delta.copy()\n",
    "df_sb_merged    = df_sb.copy()\n",
    "\n",
    "# Ensure df_flux_merged definitely has 'datetime' and 'filename'\n",
    "if \"filename\" not in df_flux_merged.columns:\n",
    "    raise ValueError(\"df_flux must have a 'filename' column so we can propagate it to df_delta & df_sb.\")\n",
    "\n",
    "if \"datetime\" not in df_flux_merged.columns:\n",
    "    raise ValueError(\"df_flux must have a 'datetime' column so we can merge on it.\")\n",
    "\n",
    "# Merge for df_delta_merged\n",
    "if \"datetime\" not in df_delta_merged.columns:\n",
    "    raise ValueError(\"df_delta must have a 'datetime' column for merging.\")\n",
    "df_delta_merged = pd.merge(\n",
    "    df_delta_merged,\n",
    "    df_flux_merged[[\"datetime\", \"filename\"]],\n",
    "    on=\"datetime\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Merge for df_sb_merged\n",
    "if \"datetime\" not in df_sb_merged.columns:\n",
    "    raise ValueError(\"df_sb must have a 'datetime' column for merging.\")\n",
    "df_sb_merged = pd.merge(\n",
    "    df_sb_merged,\n",
    "    df_flux_merged[[\"datetime\", \"filename\"]],\n",
    "    on=\"datetime\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "print(\"Merged 'filename' into df_delta_merged and df_sb_merged.\\n\")\n",
    "print(\"=== Checking the first 3 rows of each merged DataFrame ===\")\n",
    "print(\"\\n--> df_flux_merged:\")\n",
    "print(df_flux_merged.head(3))\n",
    "print(\"\\n--> df_delta_merged:\")\n",
    "print(df_delta_merged.head(3))\n",
    "print(\"\\n--> df_sb_merged:\")\n",
    "print(df_sb_merged.head(3))\n",
    "print(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4521fabc-ad6a-4e7a-b2bf-def9524e1787",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "#                             CELL 11A\n",
    "###############################################################################\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from sklearn.cluster import DBSCAN\n",
    "import matplotlib.ticker as mticker # Ensure this is imported\n",
    "\n",
    "###############################################################################\n",
    "#          GLOBAL ADJUSTABLE SIZES (POINTS, ANNOTATIONS, FILE NUMBERS)\n",
    "###############################################################################\n",
    "POINT_SIZE = 50                # Size of the scatter data points\n",
    "ANNOTATION_FONTSIZE = 16       # Font size for the numbers next to scatter points\n",
    "# FILE_NUMBER_FONTSIZE = 16    # Font size for \"File #XX\" titles (REMOVED)\n",
    "IMAGE_NUMBER_FONTSIZE = 16     # Font size for the number overlay ON the image\n",
    "IMAGE_NUMBER_X_OFFSET = 0      # Pixel offset from left edge for image number\n",
    "IMAGE_NUMBER_Y_OFFSET = 100      # Pixel offset from top edge for image number\n",
    "\n",
    "###############################################################################\n",
    "#                  GLOBAL FONT AND SIZE ADJUSTMENTS FOR PLOTS\n",
    "###############################################################################\n",
    "plt.rcParams.update({\n",
    "    \"font.size\": 16,\n",
    "    \"axes.labelsize\": 16,\n",
    "    \"axes.titlesize\": 18,\n",
    "    \"legend.fontsize\": 12,\n",
    "    \"xtick.labelsize\": 16,\n",
    "    \"ytick.labelsize\": 16,\n",
    "    \"axes.formatter.useoffset\": False\n",
    "})\n",
    "\n",
    "###############################################################################\n",
    "#                             MODIFIED FUNCTION\n",
    "###############################################################################\n",
    "\n",
    "def run_dbscan_and_plot_with_images(\n",
    "    df_flux,\n",
    "    eps=0.25,\n",
    "    min_samples=3,\n",
    "    margin_scale=0.1,\n",
    "    use_merge_plots=True,\n",
    "    merges=None,\n",
    "    cropped_images=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs DBSCAN clustering on time-series data and plots clusters along with\n",
    "    corresponding cropped images. Image numbers are overlaid on the images.\n",
    "\n",
    "    Args:\n",
    "        df_flux (pd.DataFrame): DataFrame containing 'datetime', 'f_avg',\n",
    "                                'days_since_start', and optionally 'filename'.\n",
    "        eps (float): The maximum distance between two samples for one to be\n",
    "                     considered as in the neighborhood of the other (DBSCAN).\n",
    "        min_samples (int): The number of samples in a neighborhood for a point\n",
    "                           to be considered as a core point (DBSCAN).\n",
    "        margin_scale (float): Factor to extend plot limits beyond data range.\n",
    "        use_merge_plots (bool): If True, group clusters specified in 'merges'.\n",
    "        merges (list[list[int]], optional): List of lists, where each inner list\n",
    "                                            contains cluster labels to merge.\n",
    "        cropped_images (list[tuple], optional): List of tuples like\n",
    "                                (index, filename, date_obs, time_obs, image_array).\n",
    "                                Needed to display images.\n",
    "    \"\"\"\n",
    "    print(\"\\n==========================================\")\n",
    "    print(\"    DBSCAN + Image Plotting (Unified)\")\n",
    "    print(\"==========================================\\n\")\n",
    "\n",
    "    required_cols = [\"datetime\", \"f_avg\", \"days_since_start\"]\n",
    "    for c in required_cols:\n",
    "        if c not in df_flux.columns:\n",
    "            print(f\"ERROR: Missing column '{c}' in df_flux! Cannot proceed.\")\n",
    "            return\n",
    "\n",
    "    print(\"STEP 1: Checking df_flux sample (first 5 rows):\")\n",
    "    print(df_flux.head(5))\n",
    "\n",
    "    print(\"\\nSTEP 2: Checking for NaN in 'f_avg' or 'days_since_start':\")\n",
    "    nan_count_favg = df_flux[\"f_avg\"].isna().sum()\n",
    "    nan_count_days = df_flux[\"days_since_start\"].isna().sum()\n",
    "    if nan_count_favg > 0 or nan_count_days > 0:\n",
    "        print(f\"WARNING: Found {nan_count_favg} NaN in 'f_avg' and {nan_count_days} in 'days_since_start'.\")\n",
    "    else:\n",
    "        print(\"No NaN values found.\")\n",
    "\n",
    "    print(\"\\nSTEP 3: Range check for 'days_since_start':\")\n",
    "    days_min = df_flux[\"days_since_start\"].min()\n",
    "    days_max = df_flux[\"days_since_start\"].max()\n",
    "    print(f\"days_since_start => min: {days_min:.3f}, max: {days_max:.3f}\")\n",
    "    # Optional checks for time range validity can remain here\n",
    "\n",
    "    # Make copy and add index number\n",
    "    df_flux = df_flux.copy()\n",
    "    df_flux[\"index_number\"] = np.arange(len(df_flux)) + 1\n",
    "\n",
    "    print(f\"\\nSTEP 4: Running DBSCAN (eps={eps}, min_samples={min_samples})...\")\n",
    "    X = df_flux[\"days_since_start\"].values.reshape(-1, 1)\n",
    "    try:\n",
    "        db = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "        cluster_labels = db.fit_predict(X)\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: DBSCAN failed: {e}\")\n",
    "        return\n",
    "\n",
    "    df_flux[\"cluster\"] = cluster_labels\n",
    "    # Shift valid cluster labels to start from 1 (0 becomes 1, 1 becomes 2, etc.)\n",
    "    df_flux.loc[df_flux[\"cluster\"] >= 0, \"cluster\"] = df_flux[\"cluster\"] + 1\n",
    "\n",
    "    clusters = df_flux[df_flux[\"cluster\"] != -1][\"cluster\"].unique()\n",
    "    clusters.sort()\n",
    "\n",
    "    print(\"DBSCAN completed.\")\n",
    "    print(f\"Total data points: {len(df_flux)}\")\n",
    "    n_noise = sum(cluster_labels == -1)\n",
    "    print(f\"Noise points: {n_noise}\")\n",
    "    print(f\"Valid cluster labels (excluding -1, shifted +1): {clusters}\")\n",
    "\n",
    "    print(\"\\nSTEP 5: Summarizing clusters:\")\n",
    "    if len(clusters) == 0:\n",
    "        print(\"No valid clusters (all noise?).\")\n",
    "        return\n",
    "\n",
    "    for label in clusters:\n",
    "        cdata = df_flux[df_flux[\"cluster\"] == label]\n",
    "        cmin = cdata[\"days_since_start\"].min()\n",
    "        cmax = cdata[\"days_since_start\"].max()\n",
    "        csize = len(cdata)\n",
    "        print(f\"  Cluster {label}: size={csize}, time-range=({cmin:.2f}, {cmax:.2f})\")\n",
    "\n",
    "    # Prepare items to plot (handle merges)\n",
    "    if merges is None:\n",
    "        merges = []\n",
    "    merges = [set(m) for m in merges]\n",
    "    plot_items = []\n",
    "    used = set()\n",
    "    for lbl in clusters:\n",
    "        if lbl in used:\n",
    "            continue\n",
    "        belongs_to = None\n",
    "        for mset in merges:\n",
    "            if lbl in mset:\n",
    "                belongs_to = mset\n",
    "                break\n",
    "        if belongs_to is not None:\n",
    "            plot_items.append(belongs_to)\n",
    "            used.update(belongs_to)\n",
    "        else:\n",
    "            plot_items.append({lbl})\n",
    "            used.add(lbl)\n",
    "\n",
    "    print(\"\\nSTEP 6: Creating cluster subplots + images in one figure...\")\n",
    "\n",
    "    filename_col_present = \"filename\" in df_flux.columns\n",
    "    if not filename_col_present:\n",
    "        print(\"NOTE: 'filename' column not in df_flux. Will skip image matching.\")\n",
    "\n",
    "    have_cropped_images = (cropped_images is not None) and (len(cropped_images) > 0)\n",
    "    if not have_cropped_images:\n",
    "        print(\"NOTE: 'cropped_images' is None or empty. Will skip image display.\\n\")\n",
    "\n",
    "    # --- Plotting Loop ---\n",
    "    for item in plot_items:\n",
    "        label_list = sorted(item)\n",
    "        sub_df = df_flux[df_flux[\"cluster\"].isin(label_list)].copy()\n",
    "        if len(sub_df) == 0:\n",
    "            print(f\"No data for clusters {label_list}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        snippet_data = sub_df.head(5)\n",
    "        print(f\"\\n   => Plotting cluster(s) {label_list}, total points: {len(sub_df)}\")\n",
    "        print(\"   First 5 lines (with index_number):\")\n",
    "        print(snippet_data[[\"index_number\",\"days_since_start\",\"f_avg\"]].to_string(index=False))\n",
    "\n",
    "        # Find matching images for this cluster/merge group\n",
    "        matching_images = []\n",
    "        if filename_col_present and have_cropped_images:\n",
    "            valid_filenames = set(sub_df[\"filename\"].unique())\n",
    "            matching_images = [\n",
    "                (fidx,fname,dobs,tobs,img_8u)\n",
    "                for (fidx,fname,dobs,tobs,img_8u) in cropped_images\n",
    "                if fname in valid_filenames\n",
    "            ]\n",
    "            # Sort matching images by file index to maintain order\n",
    "            matching_images.sort(key=lambda x: x[0])\n",
    "\n",
    "\n",
    "        # Setup figure grid\n",
    "        n_img = len(matching_images)\n",
    "        img_cols = 8 # Max images per row\n",
    "        img_rows = math.ceil(n_img / img_cols) if n_img > 0 else 0\n",
    "        fig_width = 25\n",
    "        # Adjust height: 6 for main plot + 3 per image row\n",
    "        fig_height = 6 + (3 * img_rows)\n",
    "\n",
    "        fig = plt.figure(figsize=(fig_width, fig_height))\n",
    "        # GridSpec: 1 row for scatter plot, 'img_rows' for images\n",
    "        gs = GridSpec(nrows=img_rows + 1, ncols=img_cols,\n",
    "                      height_ratios=[6] + ([3] * img_rows) if img_rows > 0 else [6], # Handle case with 0 image rows\n",
    "                      figure=fig)\n",
    "\n",
    "        # --- Scatter Plot ---\n",
    "        ax_scatter = fig.add_subplot(gs[0, :]) # Scatter plot spans all columns in the first row\n",
    "        colors = plt.cm.tab10(np.linspace(0, 1, len(label_list))) # Colors for different clusters if merged\n",
    "\n",
    "        for i, lbl2 in enumerate(label_list):\n",
    "            cdata = sub_df[sub_df[\"cluster\"] == lbl2]\n",
    "            ax_scatter.scatter(\n",
    "                cdata[\"days_since_start\"],\n",
    "                cdata[\"f_avg\"],\n",
    "                color=colors[i],\n",
    "                alpha=0.75,\n",
    "                s=POINT_SIZE,\n",
    "                label=f\"Cluster {lbl2}\"\n",
    "            )\n",
    "            # Annotate points with their original index number\n",
    "            for _, row in cdata.iterrows():\n",
    "                ax_scatter.text(\n",
    "                    row[\"days_since_start\"],\n",
    "                    row[\"f_avg\"],\n",
    "                    str(row[\"index_number\"]),\n",
    "                    fontsize=ANNOTATION_FONTSIZE,\n",
    "                    ha='left',\n",
    "                    va='bottom'\n",
    "                )\n",
    "\n",
    "        # Set title based on whether clusters were merged\n",
    "        if len(label_list) > 1:\n",
    "            ax_scatter.set_title(f\"Merged Clusters {label_list}\", fontsize=14)\n",
    "            ax_scatter.legend() # Show legend only for merged plots\n",
    "        else:\n",
    "            ax_scatter.set_title(f\"Cluster {label_list[0]}\", fontsize=14)\n",
    "\n",
    "        # Set plot limits with margins\n",
    "        t_min = sub_df[\"days_since_start\"].min()\n",
    "        t_max = sub_df[\"days_since_start\"].max()\n",
    "        span = t_max - t_min\n",
    "        if span <= 0: # Handle single point case\n",
    "            t_min -= 0.1; t_max += 0.1\n",
    "        else:\n",
    "            margin = margin_scale * span\n",
    "            t_min -= margin; t_max += margin\n",
    "        ax_scatter.set_xlim(t_min, t_max)\n",
    "\n",
    "        favg_min = sub_df[\"f_avg\"].min()\n",
    "        favg_max = sub_df[\"f_avg\"].max()\n",
    "        y_span = favg_max - favg_min\n",
    "        if y_span <= 0: # Handle single point case\n",
    "            favg_min -= 0.5; favg_max += 0.5\n",
    "        else:\n",
    "            y_margin = margin_scale * y_span\n",
    "            favg_min -= y_margin; favg_max += y_margin\n",
    "        ax_scatter.set_ylim(favg_min, favg_max)\n",
    "\n",
    "        # Labels and ticks\n",
    "        ax_scatter.set_xlabel(\"Days Since Start\")\n",
    "        ax_scatter.set_ylabel(\"Average Flux\")\n",
    "        ax_scatter.xaxis.set_major_locator(mticker.MultipleLocator(0.1)) # Adjust tick frequency if needed\n",
    "        ax_scatter.ticklabel_format(style='plain', useOffset=False, axis='x')\n",
    "        ax_scatter.ticklabel_format(style='plain', useOffset=False, axis='y')\n",
    "        ax_scatter.yaxis.set_major_locator(mticker.MaxNLocator(nbins='auto')) # Auto Y ticks\n",
    "\n",
    "        # --- Image Subplots ---\n",
    "        if n_img > 0:\n",
    "            print(f\"   => Plotting {n_img} associated image(s)...\")\n",
    "            for i_img, (fidx2, fname2, dobs2, tobs2, img_8u) in enumerate(matching_images):\n",
    "                row_i = 1 + (i_img // img_cols) # Start from the second row of GridSpec\n",
    "                col_i = i_img % img_cols\n",
    "                ax_img = fig.add_subplot(gs[row_i, col_i])\n",
    "                ax_img.imshow(img_8u, cmap='gray', origin='lower')\n",
    "\n",
    "                # --- MODIFICATION START ---\n",
    "                # Remove the title\n",
    "                # ax_img.set_title(f\"File #{fidx2}\", fontsize=FILE_NUMBER_FONTSIZE)\n",
    "\n",
    "                # Add text overlay inside the image\n",
    "                ax_img.text(\n",
    "                    IMAGE_NUMBER_X_OFFSET,  # X position (pixels from left)\n",
    "                    IMAGE_NUMBER_Y_OFFSET,  # Y position (pixels from top)\n",
    "                    f\"{fidx2}\",            # Text to display\n",
    "                    color='white',          # Text color\n",
    "                    fontsize=IMAGE_NUMBER_FONTSIZE,\n",
    "                    ha='left',              # Horizontal alignment\n",
    "                    va='top'               # Vertical alignment\n",
    "                    # Optional: Add a background box for contrast\n",
    "                    #bbox=dict(facecolor='black', alpha=0.5, pad=0.3, boxstyle='round,pad=0.3')\n",
    "                )\n",
    "                # --- MODIFICATION END ---\n",
    "\n",
    "                ax_img.axis(\"off\") # Hide axes for the image subplot\n",
    "        else:\n",
    "             print(\"   => No matching images found or provided for this cluster group.\")\n",
    "\n",
    "\n",
    "        plt.tight_layout() # Adjust spacing\n",
    "        plt.show() # Display the combined figure\n",
    "\n",
    "        # Optional: Print list of files shown\n",
    "        if filename_col_present and have_cropped_images and n_img > 0:\n",
    "            print(f\"   => Images shown correspond to files:\")\n",
    "            for (xxid, xxfn, ddt, ddt2, _) in matching_images:\n",
    "                print(f\"       * File #{xxid} => {xxfn} => {ddt} {ddt2}\")\n",
    "\n",
    "        print(\"   ----------------------------------------------------\\n\")\n",
    "\n",
    "    print(\"===== End of DBSCAN + Image Plotting Process (11A) =====\\n\")\n",
    "\n",
    "# Example Call (assuming df_flux_merged and cropped_images are defined)\n",
    "run_dbscan_and_plot_with_images(\n",
    "    df_flux_merged,\n",
    "    eps=0.5,\n",
    "    min_samples=1,\n",
    "    merges=[[11,12,13]], # Example merge\n",
    "    cropped_images=cropped_images # Pass your list of cropped image tuples\n",
    ")\n",
    "print(\"=== END CELL 11A ===\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4e9a12-4c2d-4bc4-975d-f19493746e88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209be59f-be4c-44c5-a411-d691fcfcf73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "#                       CELL 11A (with Sine Wave Overlay)\n",
    "###############################################################################\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from sklearn.cluster import DBSCAN\n",
    "import matplotlib.ticker as mticker # Ensure this is imported\n",
    "from scipy.optimize import curve_fit # <-- ADDED IMPORT FOR SINE FIT\n",
    "\n",
    "###############################################################################\n",
    "#          GLOBAL ADJUSTABLE SIZES (POINTS, ANNOTATIONS, FILE NUMBERS)\n",
    "###############################################################################\n",
    "POINT_SIZE = 50                # Size of the scatter data points\n",
    "ANNOTATION_FONTSIZE = 16       # Font size for the numbers next to scatter points\n",
    "# FILE_NUMBER_FONTSIZE = 16    # Font size for \"File #XX\" titles (REMOVED)\n",
    "IMAGE_NUMBER_FONTSIZE = 16     # Font size for the number overlay ON the image\n",
    "IMAGE_NUMBER_X_OFFSET = 5      # Pixel offset from left edge for image number (Adjusted slightly)\n",
    "IMAGE_NUMBER_Y_OFFSET = 15     # Pixel offset from top edge for image number (Adjusted from bottom)\n",
    "\n",
    "###############################################################################\n",
    "#                  GLOBAL FONT AND SIZE ADJUSTMENTS FOR PLOTS\n",
    "###############################################################################\n",
    "plt.rcParams.update({\n",
    "    \"font.size\": 16,\n",
    "    \"axes.labelsize\": 16,\n",
    "    \"axes.titlesize\": 18,\n",
    "    \"legend.fontsize\": 12,\n",
    "    \"xtick.labelsize\": 16,\n",
    "    \"ytick.labelsize\": 16,\n",
    "    \"axes.formatter.useoffset\": False\n",
    "})\n",
    "\n",
    "###############################################################################\n",
    "#                           SINE WAVE FUNCTION (ADDED)\n",
    "###############################################################################\n",
    "\n",
    "def sine_func(x, amplitude, frequency, phase_shift, vertical_offset):\n",
    "    \"\"\"Mathematical model for a sine wave.\"\"\"\n",
    "    # Ensure x is a numpy array for vectorized operations\n",
    "    x = np.asarray(x)\n",
    "    # Protect against invalid inputs if necessary (though curve_fit usually handles)\n",
    "    # For example, ensure amplitude is non-negative if expected\n",
    "    # amplitude = max(0, amplitude) # Optional constraint, better handled by bounds\n",
    "    return amplitude * np.sin(frequency * x + phase_shift) + vertical_offset\n",
    "\n",
    "###############################################################################\n",
    "#                            MODIFIED FUNCTION\n",
    "###############################################################################\n",
    "\n",
    "def run_dbscan_and_plot_with_images(\n",
    "    df_flux,\n",
    "    eps=0.25,\n",
    "    min_samples=3,\n",
    "    margin_scale=0.1,\n",
    "    use_merge_plots=True, # Kept for compatibility, but merging logic relies on 'merges'\n",
    "    merges=None,\n",
    "    cropped_images=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs DBSCAN clustering on time-series data and plots clusters along with\n",
    "    corresponding cropped images and an overlaid sine wave fit on the scatter plot.\n",
    "    Image numbers are overlaid on the images.\n",
    "\n",
    "    Args:\n",
    "        df_flux (pd.DataFrame): DataFrame containing 'datetime', 'f_avg',\n",
    "                                'days_since_start', and optionally 'filename'.\n",
    "        eps (float): The maximum distance between two samples for one to be\n",
    "                     considered as in the neighborhood of the other (DBSCAN).\n",
    "        min_samples (int): The number of samples in a neighborhood for a point\n",
    "                           to be considered as a core point (DBSCAN).\n",
    "        margin_scale (float): Factor to extend plot limits beyond data range.\n",
    "        use_merge_plots (bool): Kept for compatibility, but merging logic now\n",
    "                                solely relies on the 'merges' argument.\n",
    "        merges (list[list[int]], optional): List of lists, where each inner list\n",
    "                                           contains cluster labels to merge.\n",
    "        cropped_images (list[tuple], optional): List of tuples like\n",
    "                                (index, filename, date_obs, time_obs, image_array).\n",
    "                                Needed to display images.\n",
    "    \"\"\"\n",
    "    print(\"\\n=======================================================\")\n",
    "    print(\"   DBSCAN + Image Plotting + Sine Fit (Unified)\")\n",
    "    print(\"=======================================================\\n\")\n",
    "\n",
    "    required_cols = [\"datetime\", \"f_avg\", \"days_since_start\"]\n",
    "    for c in required_cols:\n",
    "        if c not in df_flux.columns:\n",
    "            print(f\"ERROR: Missing column '{c}' in df_flux! Cannot proceed.\")\n",
    "            return\n",
    "\n",
    "    # --- Data Validation (Added checks for NaN/Inf) ---\n",
    "    print(\"STEP 1: Checking df_flux sample (first 5 rows):\")\n",
    "    print(df_flux.head(5))\n",
    "\n",
    "    print(\"\\nSTEP 2: Checking for NaN/Inf in 'f_avg' or 'days_since_start':\")\n",
    "    nan_inf_favg = df_flux[\"f_avg\"].isna().sum() + np.isinf(df_flux[\"f_avg\"]).sum()\n",
    "    nan_inf_days = df_flux[\"days_since_start\"].isna().sum() + np.isinf(df_flux[\"days_since_start\"]).sum()\n",
    "\n",
    "    if nan_inf_favg > 0 or nan_inf_days > 0:\n",
    "        print(f\"WARNING: Found {nan_inf_favg} NaN/Inf in 'f_avg' and {nan_inf_days} in 'days_since_start'.\")\n",
    "        print(\"Attempting to remove rows with NaN/Inf values...\")\n",
    "        original_len = len(df_flux)\n",
    "        df_flux = df_flux.replace([np.inf, -np.inf], np.nan).dropna(subset=['f_avg', 'days_since_start'])\n",
    "        print(f\"Removed {original_len - len(df_flux)} rows.\")\n",
    "        if len(df_flux) == 0:\n",
    "            print(\"ERROR: No valid data remaining after removing NaN/Inf. Cannot proceed.\")\n",
    "            return\n",
    "    else:\n",
    "        print(\"No NaN or Inf values found.\")\n",
    "\n",
    "    print(\"\\nSTEP 3: Range check for 'days_since_start':\")\n",
    "    # Ensure data types are numeric before min/max\n",
    "    df_flux[\"days_since_start\"] = pd.to_numeric(df_flux[\"days_since_start\"], errors='coerce')\n",
    "    df_flux[\"f_avg\"] = pd.to_numeric(df_flux[\"f_avg\"], errors='coerce')\n",
    "    df_flux = df_flux.dropna(subset=['days_since_start', 'f_avg']) # Drop if conversion failed\n",
    "\n",
    "    if len(df_flux) == 0:\n",
    "         print(\"ERROR: No numeric data remaining after type conversion. Cannot proceed.\")\n",
    "         return\n",
    "\n",
    "    days_min = df_flux[\"days_since_start\"].min()\n",
    "    days_max = df_flux[\"days_since_start\"].max()\n",
    "    print(f\"days_since_start => min: {days_min:.3f}, max: {days_max:.3f}\")\n",
    "\n",
    "    # --- DBSCAN ---\n",
    "    # Make copy and add index number *after* cleaning\n",
    "    df_flux = df_flux.copy()\n",
    "    # Use reset_index to get a reliable sequential index if rows were dropped\n",
    "    df_flux = df_flux.reset_index(drop=True)\n",
    "    df_flux[\"index_number\"] = df_flux.index + 1 # Sequential index for valid data\n",
    "\n",
    "    print(f\"\\nSTEP 4: Running DBSCAN (eps={eps}, min_samples={min_samples})...\")\n",
    "    X = df_flux[\"days_since_start\"].values.reshape(-1, 1)\n",
    "    try:\n",
    "        db = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "        cluster_labels = db.fit_predict(X)\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: DBSCAN failed: {e}\")\n",
    "        return\n",
    "\n",
    "    df_flux[\"cluster\"] = cluster_labels\n",
    "    # Shift valid cluster labels to start from 1 (0 becomes 1, 1 becomes 2, etc.)\n",
    "    df_flux.loc[df_flux[\"cluster\"] >= 0, \"cluster\"] = df_flux[\"cluster\"] + 1\n",
    "\n",
    "    clusters = df_flux[df_flux[\"cluster\"] != -1][\"cluster\"].unique()\n",
    "    clusters.sort()\n",
    "\n",
    "    print(\"DBSCAN completed.\")\n",
    "    print(f\"Total valid data points analyzed: {len(df_flux)}\")\n",
    "    n_noise = sum(cluster_labels == -1)\n",
    "    print(f\"Noise points (-1): {n_noise}\")\n",
    "    print(f\"Valid cluster labels found (shifted +1): {clusters}\")\n",
    "\n",
    "    print(\"\\nSTEP 5: Summarizing clusters:\")\n",
    "    if len(clusters) == 0 and n_noise == 0: # Check if there's no data at all\n",
    "        print(\"No clusters or noise points found.\")\n",
    "        return\n",
    "    elif len(clusters) == 0:\n",
    "         print(\"No valid clusters found (only noise or too few points?).\")\n",
    "         # Continue if noise points exist\n",
    "\n",
    "    cluster_summary = {}\n",
    "    for label in clusters:\n",
    "        cdata = df_flux[df_flux[\"cluster\"] == label]\n",
    "        cmin = cdata[\"days_since_start\"].min()\n",
    "        cmax = cdata[\"days_since_start\"].max()\n",
    "        csize = len(cdata)\n",
    "        cluster_summary[label] = {'size': csize, 'min_t': cmin, 'max_t': cmax}\n",
    "        print(f\"   Cluster {label}: size={csize}, time-range=({cmin:.2f}, {cmax:.2f})\")\n",
    "\n",
    "    # --- Plotting Preparation ---\n",
    "    if merges is None:\n",
    "        merges = []\n",
    "    merges = [set(m) for m in merges] # Ensure elements are sets for easier checking\n",
    "    plot_items = []\n",
    "    used_clusters = set()\n",
    "\n",
    "    # Add merged clusters first\n",
    "    for mset in merges:\n",
    "        valid_merge_clusters = {lbl for lbl in mset if lbl in cluster_summary} # Only include clusters that actually exist\n",
    "        if valid_merge_clusters:\n",
    "            plot_items.append(valid_merge_clusters)\n",
    "            used_clusters.update(valid_merge_clusters)\n",
    "\n",
    "    # Add remaining individual clusters\n",
    "    for lbl in clusters:\n",
    "        if lbl not in used_clusters:\n",
    "            plot_items.append({lbl}) # Store single clusters as sets too\n",
    "            used_clusters.add(lbl)\n",
    "\n",
    "    # Add noise points as a separate item if they exist\n",
    "    if n_noise > 0:\n",
    "        plot_items.append({-1}) # Use -1 to represent noise\n",
    "\n",
    "    print(\"\\nSTEP 6: Creating plots for clusters/merges/noise...\")\n",
    "\n",
    "    filename_col_present = \"filename\" in df_flux.columns\n",
    "    if not filename_col_present:\n",
    "        print(\"NOTE: 'filename' column not in df_flux. Will skip image matching.\")\n",
    "\n",
    "    have_cropped_images = (cropped_images is not None) and (len(cropped_images) > 0)\n",
    "    if not have_cropped_images:\n",
    "        print(\"NOTE: 'cropped_images' is None or empty. Will skip image display.\\\\n\")\n",
    "    elif not filename_col_present:\n",
    "         print(\"NOTE: Cannot display images without 'filename' column in DataFrame.\\\\n\")\n",
    "\n",
    "\n",
    "    # --- Plotting Loop ---\n",
    "    for item_set in plot_items:\n",
    "        is_noise_plot = -1 in item_set\n",
    "        if is_noise_plot:\n",
    "             label_list = [-1]\n",
    "             sub_df = df_flux[df_flux[\"cluster\"] == -1].copy()\n",
    "             plot_title = \"Noise Points\"\n",
    "             is_merged = False # Noise is not considered merged\n",
    "        else:\n",
    "            label_list = sorted(list(item_set)) # Convert set back to sorted list\n",
    "            sub_df = df_flux[df_flux[\"cluster\"].isin(label_list)].copy()\n",
    "            is_merged = len(label_list) > 1\n",
    "            if is_merged:\n",
    "                 plot_title = f\"Merged Clusters {label_list}\"\n",
    "            else:\n",
    "                 plot_title = f\"Cluster {label_list[0]}\"\n",
    "\n",
    "\n",
    "        if len(sub_df) == 0:\n",
    "            print(f\"No data for {'noise' if is_noise_plot else f'cluster(s) {label_list}'}, skipping plot.\")\n",
    "            continue\n",
    "\n",
    "        # Sort sub_df by time for potentially better sine fitting and consistent plotting\n",
    "        sub_df = sub_df.sort_values(by=\"days_since_start\")\n",
    "\n",
    "        print(f\"\\n   => Plotting: {plot_title}, total points: {len(sub_df)}\")\n",
    "        # print(\"   First 5 lines (sorted by time, with index_number):\")\n",
    "        # print(sub_df[[\"index_number\",\"days_since_start\",\"f_avg\"]].head(5).to_string(index=False))\n",
    "\n",
    "        # Find matching images (only if not noise and requirements met)\n",
    "        matching_images = []\n",
    "        if not is_noise_plot and filename_col_present and have_cropped_images:\n",
    "            valid_filenames = set(sub_df[\"filename\"].unique())\n",
    "            # Create a mapping from filename to original index_number for correct sorting\n",
    "            # This assumes 'filename' uniquely maps to an 'index_number' in the *cleaned* df_flux\n",
    "            fname_to_index = sub_df.set_index('filename')['index_number'].to_dict()\n",
    "\n",
    "            temp_matching = []\n",
    "            # The `cropped_images` list uses the *original* file index (fidx from Cell 5 enumerate)\n",
    "            # We need to map this back to the `index_number` used in the plotting dataframe\n",
    "            # Let's rebuild `fname_to_index` using the original `file_details` if available\n",
    "            # or just map filename to the *current* index_number if `file_details` isn't passed\n",
    "            # For simplicity here, we assume `fname_to_index` based on current `sub_df` is sufficient\n",
    "            # for retrieving images associated with points *in this cluster*.\n",
    "\n",
    "            # We need to link `cropped_images` (which has original fidx) to `sub_df` (which has index_number)\n",
    "            # Best way is via filename if it's guaranteed unique\n",
    "            sub_df_filenames = set(sub_df['filename'])\n",
    "            for (fidx_orig, fname, dobs, tobs, img_8u) in cropped_images:\n",
    "                 if fname in sub_df_filenames:\n",
    "                      # Find the corresponding index_number in sub_df for this filename\n",
    "                      matched_rows = sub_df[sub_df['filename'] == fname]\n",
    "                      if not matched_rows.empty:\n",
    "                           current_index_num = matched_rows['index_number'].iloc[0] # Get index_number for sorting plot items\n",
    "                           # Store (current_index_num_for_sorting, original_fidx_for_display, fname, dobs, tobs, img_8u)\n",
    "                           temp_matching.append((current_index_num, fidx_orig, fname, dobs, tobs, img_8u))\n",
    "\n",
    "            # Sort based on the current index_number from the dataframe to match plot order\n",
    "            temp_matching.sort(key=lambda x: x[0])\n",
    "            # Final list in correct format (original_fidx, fname, dobs, tobs, img_8u)\n",
    "            matching_images = [(fidx_orig, fname, dobs, tobs, img_8u) for _, fidx_orig, fname, dobs, tobs, img_8u in temp_matching]\n",
    "\n",
    "\n",
    "        # Setup figure grid\n",
    "        n_img = len(matching_images)\n",
    "        img_cols = 8 # Max images per row\n",
    "        img_rows = math.ceil(n_img / img_cols) if n_img > 0 else 0\n",
    "        fig_width = 25\n",
    "        fig_height = 6 + (3 * img_rows) # Adjust height: 6 for main plot + 3 per image row\n",
    "\n",
    "        fig = plt.figure(figsize=(fig_width, fig_height))\n",
    "        # GridSpec: 1 row for scatter plot, 'img_rows' for images\n",
    "        gs_rows = 1 + img_rows\n",
    "        height_ratios = [6] + ([3] * img_rows) if img_rows > 0 else [6]\n",
    "        gs = GridSpec(nrows=gs_rows, ncols=img_cols,\n",
    "                      height_ratios=height_ratios,\n",
    "                      figure=fig)\n",
    "\n",
    "        # --- Scatter Plot ---\n",
    "        ax_scatter = fig.add_subplot(gs[0, :]) # Scatter plot spans all columns in the first row\n",
    "\n",
    "        sine_fit_successful = False # Reset for each plot\n",
    "        show_legend = False # Determine if legend should be shown\n",
    "\n",
    "        if is_noise_plot:\n",
    "             # Plot noise points\n",
    "             ax_scatter.scatter(\n",
    "                 sub_df[\"days_since_start\"], sub_df[\"f_avg\"],\n",
    "                 color='gray', alpha=0.5, s=POINT_SIZE * 0.8, label=\"Noise\"\n",
    "             )\n",
    "             show_legend = True # Show legend for noise plot\n",
    "             # Annotate noise points\n",
    "             for _, row in sub_df.iterrows():\n",
    "                 ax_scatter.text(\n",
    "                        row[\"days_since_start\"], row[\"f_avg\"], str(row[\"index_number\"]),\n",
    "                        fontsize=ANNOTATION_FONTSIZE * 0.8, color='gray', ha='left', va='bottom')\n",
    "        else:\n",
    "            # Plot valid cluster points\n",
    "            colors = plt.cm.viridis(np.linspace(0, 0.9, max(1, len(label_list)))) # Use viridis, avoid yellow end\n",
    "\n",
    "            for i_lbl, lbl2 in enumerate(label_list):\n",
    "                cdata = sub_df[sub_df[\"cluster\"] == lbl2]\n",
    "                if not cdata.empty:\n",
    "                    ax_scatter.scatter(\n",
    "                        cdata[\"days_since_start\"], cdata[\"f_avg\"],\n",
    "                        color=colors[i_lbl % len(colors)], alpha=0.8, s=POINT_SIZE,\n",
    "                        label=f\"Cluster {lbl2}\" # Label each cluster\n",
    "                    )\n",
    "                    show_legend = True # Show legend if plotting clusters\n",
    "                    # Annotate points\n",
    "                    for _, row in cdata.iterrows():\n",
    "                        ax_scatter.text(\n",
    "                            row[\"days_since_start\"], row[\"f_avg\"], str(row[\"index_number\"]),\n",
    "                            fontsize=ANNOTATION_FONTSIZE, ha='left', va='bottom')\n",
    "\n",
    "            # --- ADDED: Sine Wave Fitting and Plotting (only for non-noise) ---\n",
    "            if len(sub_df) >= 4: # Need at least 4 points to fit 4 parameters\n",
    "                x_data = sub_df[\"days_since_start\"].values\n",
    "                y_data = sub_df[\"f_avg\"].values\n",
    "                x_span = x_data.max() - x_data.min() if len(x_data) > 1 else 0\n",
    "                y_span = y_data.max() - y_data.min() if len(y_data) > 1 else 0\n",
    "\n",
    "                # Avoid fitting if points are too close, constant, or span is too small\n",
    "                if x_span > 1e-6 and y_span > 1e-6:\n",
    "                    print(f\"   => Attempting sine fit for {plot_title}...\")\n",
    "                    try:\n",
    "                        # Initial guesses - Refined\n",
    "                        initial_amplitude = y_span / 2.0\n",
    "                        initial_vertical_offset = np.median(y_data) # Median is robust to outliers\n",
    "                        # Frequency guess: More robust - check if span is large enough for a cycle\n",
    "                        # Assume at least half a cycle might be present if span > 0.1 days? Adjust as needed.\n",
    "                        initial_frequency = (np.pi / x_span) if x_span > 0.1 else 1.0\n",
    "                        initial_phase_shift = 0 # Keep phase guess simple\n",
    "\n",
    "                        initial_guesses = [initial_amplitude, initial_frequency, initial_phase_shift, initial_vertical_offset]\n",
    "                        # Bounds to help guide the fit and prevent nonsensical results\n",
    "                        bounds = (\n",
    "                            [0, 0, -np.inf, -np.inf], # Lower bounds: Amplitude >= 0, Frequency >= 0\n",
    "                            [y_span * 1.5, np.inf, np.inf, np.inf] # Upper bounds: Limit amplitude guess reasonably\n",
    "                        )\n",
    "\n",
    "                        popt, pcov = curve_fit(sine_func, x_data, y_data, p0=initial_guesses, bounds=bounds, maxfev=5000)\n",
    "\n",
    "                        # Check quality of fit using covariance: Check diagonal elements\n",
    "                        if np.any(np.isinf(np.diag(pcov))):\n",
    "                             print(f\"   WARNING: Sine fit converged for {plot_title}, but covariance indicates high uncertainty. Fit may be unreliable.\")\n",
    "                             # Decide whether to plot uncertain fits or not - here we'll plot but warn\n",
    "                             pass # Continue to plot\n",
    "\n",
    "                        # Generate points for the fitted curve\n",
    "                        x_fit = np.linspace(x_data.min(), x_data.max(), 200) # Smooth curve over data range\n",
    "                        y_fit = sine_func(x_fit, *popt)\n",
    "\n",
    "                        # Plot the fitted sine wave\n",
    "                        ax_scatter.plot(x_fit, y_fit, color='red', linestyle='--', linewidth=2, label='Sine Fit')\n",
    "                        print(f\"   Sine fit successful. Params (A, ω, φ, D): {np.round(popt, 3)}\")\n",
    "                        sine_fit_successful = True\n",
    "                        show_legend = True # Ensure legend is shown if fit is added\n",
    "\n",
    "                    except RuntimeError as e:\n",
    "                        print(f\"   WARNING: Sine fit failed for {plot_title}. Reason: Optimal parameters not found. ({e})\")\n",
    "                    except ValueError as e:\n",
    "                         print(f\"   WARNING: Sine fit failed for {plot_title}. Reason: {e}\") # Often due to bounds/input data mismatch\n",
    "                    except Exception as e:\n",
    "                        print(f\"   WARNING: An unexpected error occurred during sine fit for {plot_title}: {e}\")\n",
    "                else:\n",
    "                    print(f\"   => Skipping sine fit for {plot_title}: Data points have minimal span in X ({x_span:.2e}) or Y ({y_span:.2e}).\")\n",
    "            else:\n",
    "                print(f\"   => Skipping sine fit for {plot_title}: Not enough data points (need >= 4). Found {len(sub_df)}.\")\n",
    "            # --- END ADDED SINE FIT SECTION ---\n",
    "\n",
    "        # --- Final Scatter Plot Formatting ---\n",
    "        ax_scatter.set_title(plot_title, fontsize=18)\n",
    "\n",
    "        # --- MODIFIED LEGEND DISPLAY ---\n",
    "        if show_legend: # Show legend if noise, merged clusters, or successful fit\n",
    "             # Place legend outside plot area to avoid overlap, adjust position slightly if needed\n",
    "             ax_scatter.legend(loc='upper left', bbox_to_anchor=(1.02, 1), borderaxespad=0.1)\n",
    "        # --- END MODIFIED LEGEND ---\n",
    "\n",
    "        # Set plot limits with margins (DO NOT CHANGE THIS PART's LOGIC for limits)\n",
    "        t_min_data = sub_df[\"days_since_start\"].min()\n",
    "        t_max_data = sub_df[\"days_since_start\"].max()\n",
    "        t_span_data = t_max_data - t_min_data if len(sub_df) > 1 else 0\n",
    "        t_margin = margin_scale * t_span_data if t_span_data > 1e-6 else 0.1\n",
    "        t_min_lim = t_min_data - t_margin\n",
    "        t_max_lim = t_max_data + t_margin\n",
    "        ax_scatter.set_xlim(t_min_lim, t_max_lim)\n",
    "\n",
    "        favg_min_data = sub_df[\"f_avg\"].min()\n",
    "        favg_max_data = sub_df[\"f_avg\"].max()\n",
    "        y_span_data = favg_max_data - favg_min_data if len(sub_df) > 1 else 0\n",
    "        y_margin = margin_scale * y_span_data if y_span_data > 1e-6 else 0.5\n",
    "        favg_min_lim = favg_min_data - y_margin * 1.1 # Add slightly more margin below\n",
    "        favg_max_lim = favg_max_data + y_margin * 1.1 # Add slightly more margin above\n",
    "        ax_scatter.set_ylim(favg_min_lim, favg_max_lim)\n",
    "\n",
    "        # Labels and ticks (DO NOT CHANGE THIS PART's LOGIC for labels/ticks)\n",
    "        ax_scatter.set_xlabel(\"Days Since Start\")\n",
    "        ax_scatter.set_ylabel(\"Average Flux\")\n",
    "        # Dynamic tick locator based on span\n",
    "        t_locator = mticker.MaxNLocator(nbins=8, prune='both') # Auto ticks for time\n",
    "        ax_scatter.xaxis.set_major_locator(t_locator)\n",
    "        ax_scatter.ticklabel_format(style='plain', useOffset=False, axis='x')\n",
    "        ax_scatter.ticklabel_format(style='plain', useOffset=False, axis='y')\n",
    "        ax_scatter.yaxis.set_major_locator(mticker.MaxNLocator(nbins='auto', prune='both')) # Auto Y ticks\n",
    "        ax_scatter.grid(True, linestyle=':', alpha=0.6) # Add light grid\n",
    "\n",
    "        # Rotate x-axis labels if they might overlap\n",
    "        plt.setp(ax_scatter.get_xticklabels(), rotation=30, ha=\"right\")\n",
    "\n",
    "\n",
    "        # --- Image Subplots ---\n",
    "        if n_img > 0: # Only relevant if not noise plot and images are available/matched\n",
    "            print(f\"   => Plotting {n_img} associated image(s)...\")\n",
    "            # Determine image value range for consistent contrast (optional)\n",
    "            # all_img_data_values = []\n",
    "            # for _, _, _, _, img in matching_images:\n",
    "            #      if img is not None: all_img_data_values.extend(img.flatten())\n",
    "            # if all_img_data_values:\n",
    "            #     vmin, vmax = np.percentile(all_img_data_values, [1, 99]) # Example contrast stretch\n",
    "            # else:\n",
    "            #     vmin, vmax = 0, 255\n",
    "\n",
    "            for i_img, (fidx2, fname2, dobs2, tobs2, img_8u) in enumerate(matching_images):\n",
    "                 # Check if img_8u is valid before proceeding\n",
    "                 if img_8u is None or img_8u.size <= 1:\n",
    "                     print(f\"      Skipping display for image index {fidx2} (fname: {fname2}) - Invalid image data.\")\n",
    "                     continue # Skip this iteration if image data is invalid\n",
    "\n",
    "                 row_i = 1 + (i_img // img_cols) # Start from the second row of GridSpec\n",
    "                 col_i = i_img % img_cols\n",
    "                 if row_i < gs_rows: # Check bounds just in case\n",
    "                     ax_img = fig.add_subplot(gs[row_i, col_i])\n",
    "                     # Use vmin/vmax for consistent contrast if calculated\n",
    "                     # ax_img.imshow(img_8u, cmap='gray', origin='lower', vmin=vmin, vmax=vmax)\n",
    "                     ax_img.imshow(img_8u, cmap='gray', origin='lower') # Default contrast\n",
    "\n",
    "                     # Add text overlay inside the image (using original fidx2)\n",
    "                     ax_img.text(\n",
    "                         IMAGE_NUMBER_X_OFFSET,    # X position (pixels from left)\n",
    "                         #img_8u.shape[0] - IMAGE_NUMBER_Y_OFFSET, # Y pos (pixels from BOTTOM) - Adjusted from original\n",
    "                         IMAGE_NUMBER_Y_OFFSET, # Y position (pixels from TOP) - Reverted to original request\n",
    "                         f\"{fidx2}\",              # Text to display (Original file index)\n",
    "                         color='white',           # Text color\n",
    "                         fontsize=IMAGE_NUMBER_FONTSIZE,\n",
    "                         ha='left',               # Horizontal alignment\n",
    "                         #va='bottom',             # Vertical alignment (relative to Y pos from bottom)\n",
    "                         va='top',                # Vertical alignment (relative to Y pos from top)\n",
    "                         bbox=dict(facecolor='black', alpha=0.6, pad=0.2, boxstyle='round,pad=0.2') # Background\n",
    "                     )\n",
    "                     ax_img.axis(\"off\") # Hide axes for the image subplot\n",
    "        # Note: Removed the separate \"No matching images\" print here, handled earlier/implicitly\n",
    "\n",
    "        # --- Final Figure Adjustments ---\n",
    "        # Adjust layout to prevent labels/titles overlapping\n",
    "        # tight_layout might interfere with external legend, use subplots_adjust\n",
    "        # Adjust 'right' based on whether legend is shown\n",
    "        right_margin = 0.82 if show_legend else 0.95\n",
    "        fig.subplots_adjust(left=0.08, right=right_margin, bottom=0.15, top=0.92, wspace=0.1, hspace=0.1)\n",
    "\n",
    "        plt.show() # Display the combined figure\n",
    "\n",
    "        # Optional: Print list of files shown\n",
    "        if not is_noise_plot and filename_col_present and have_cropped_images and n_img > 0:\n",
    "            print(f\"   => Images shown correspond to files (Sorted by time within cluster):\")\n",
    "            for (fidx2, xxfn, ddt, ddt2, _) in matching_images:\n",
    "                print(f\"       * File #{fidx2} => {xxfn} => {ddt} {ddt2}\")\n",
    "\n",
    "        print(\"   ----------------------------------------------------\\\\n\")\n",
    "\n",
    "    print(f\"===== End of Plotting Process ({len(plot_items)} plots generated) =====\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "#                       EXAMPLE USAGE SECTION\n",
    "# =============================================================================\n",
    "# IMPORTANT: Replace placeholder data generation with your actual data loading\n",
    "# Ensure df_flux_merged and cropped_images are defined before this call\n",
    "\n",
    "# --- GENERATE PLACEHOLDER DATA (if needed for testing) ---\n",
    "import datetime # Added for placeholder data creation if not imported earlier\n",
    "if 'df_flux_merged' not in locals() or 'cropped_images' not in locals():\n",
    "     print(\"\\\\n--- WARNING: Generating Placeholder Data for Example ---\")\n",
    "     # Placeholder df_flux_merged DataFrame\n",
    "     base_time = datetime.datetime(2024, 1, 1)\n",
    "     time_points = [base_time + datetime.timedelta(days=x*0.5 + np.random.rand()*0.1) for x in range(20)]\n",
    "     time_points.extend([base_time + datetime.timedelta(days=15+x*0.4 + np.random.rand()*0.1) for x in range(15)]) # Another cluster\n",
    "     time_points.extend([base_time + datetime.timedelta(days=np.random.uniform(0, 25)) for x in range(5)]) # Some noise points\n",
    "     start_time = min(time_points)\n",
    "     days_since_start = [(t - start_time).total_seconds() / (24 * 3600) for t in time_points]\n",
    "     f_avg = [10 + 1.5*np.sin(2*np.pi*d/5 + 1) + np.random.normal(0, 0.2) if d < 12 else\n",
    "              (8 - 1.0*np.sin(2*np.pi*d/4 + 3) + np.random.normal(0, 0.15) if d > 14 and d < 22 else\n",
    "               np.random.uniform(7, 12))\n",
    "              for d in days_since_start]\n",
    "     filenames = [f\"image_{i+1:03d}.fits\" for i in range(len(time_points))]\n",
    "     df_flux_merged = pd.DataFrame({\n",
    "          'datetime': time_points, 'f_avg': f_avg,\n",
    "          'days_since_start': days_since_start, 'filename': filenames\n",
    "     })\n",
    "     # Placeholder cropped_images list: (original_fidx, filename, date, time, image_array)\n",
    "     cropped_images = []\n",
    "     for i, row in df_flux_merged.iterrows():\n",
    "          img_array = (np.random.rand(50, 50) * 255).astype(np.uint8)\n",
    "          date_obs = row['datetime'].strftime('%Y-%m-%d')\n",
    "          time_obs = row['datetime'].strftime('%H:%M:%S')\n",
    "          cropped_images.append((i + 1, row['filename'], date_obs, time_obs, img_array))\n",
    "     print(\"--- Placeholder Data Generated ---\\\\n\")\n",
    "# --- END PLACEHOLDER DATA ---\n",
    "\n",
    "\n",
    "# --- Call the main function ---\n",
    "# NOTE: Adjust eps, min_samples, and merges based on YOUR data and DBSCAN results\n",
    "run_dbscan_and_plot_with_images(\n",
    "    df_flux=df_flux_merged,         # Your DataFrame\n",
    "    eps=1.0,                       # Cluster radius in 'days_since_start' (ADJUST THIS)\n",
    "    min_samples=3,                 # Minimum points to form a cluster (ADJUST THIS)\n",
    "    merges=None, # Example merge: [[1, 2], [4, 5]] # Optional: Adjust cluster numbers based on DBSCAN output\n",
    "    cropped_images=cropped_images  # Your list of image tuples (original_fidx, fname, date, time, img)\n",
    ")\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=== END CELL 11A (with Sine Fit) ===\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e973814e-8697-4697-9850-a5d79385d752",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "#                             CELL 11B\n",
    "###############################################################################\n",
    "print(\"=== BEGIN CELL 11B: DBSCAN + Image Plotting (Delta) ===\\n\")\n",
    "\n",
    "def run_dbscan_and_plot_with_images_delta(\n",
    "    df_delta,\n",
    "    eps=0.25,\n",
    "    min_samples=3,\n",
    "    margin_scale=0.1,\n",
    "    use_merge_plots=True,\n",
    "    merges=None,\n",
    "    cropped_images=None\n",
    "):\n",
    "    print(\"\\n==========================================\")\n",
    "    print(\"    DBSCAN + Image Plotting (Delta)\")\n",
    "    print(\"==========================================\\n\")\n",
    "\n",
    "    required_cols = [\"datetime\", \"delta_au\", \"days_since_start\"]\n",
    "    for c in required_cols:\n",
    "        if c not in df_delta.columns:\n",
    "            print(f\"ERROR: Missing column '{c}' in df_delta! Cannot proceed.\")\n",
    "            return\n",
    "\n",
    "    print(\"STEP 1: Checking df_delta sample (first 5 rows):\")\n",
    "    print(df_delta.head(5))\n",
    "\n",
    "    print(\"\\nSTEP 2: Checking for NaN in 'delta_au' or 'days_since_start':\")\n",
    "    nan_count_delta = df_delta[\"delta_au\"].isna().sum()\n",
    "    nan_count_days  = df_delta[\"days_since_start\"].isna().sum()\n",
    "    if nan_count_delta > 0 or nan_count_days > 0:\n",
    "        print(f\"WARNING: Found {nan_count_delta} NaN in 'delta_au' and {nan_count_days} in 'days_since_start'.\")\n",
    "    else:\n",
    "        print(\"No NaN values found.\")\n",
    "\n",
    "    print(\"\\nSTEP 3: Range check for 'days_since_start':\")\n",
    "    days_min = df_delta[\"days_since_start\"].min()\n",
    "    days_max = df_delta[\"days_since_start\"].max()\n",
    "    print(f\"days_since_start => min: {days_min:.3f}, max: {days_max:.3f}\")\n",
    "\n",
    "    df_delta = df_delta.copy()\n",
    "    df_delta[\"index_number\"] = np.arange(len(df_delta)) + 1\n",
    "\n",
    "    print(f\"\\nSTEP 4: Running DBSCAN (eps={eps}, min_samples={min_samples})...\")\n",
    "    X = df_delta[\"days_since_start\"].values.reshape(-1, 1)\n",
    "    try:\n",
    "        db = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "        cluster_labels = db.fit_predict(X)\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: DBSCAN failed: {e}\")\n",
    "        return\n",
    "\n",
    "    df_delta[\"cluster\"] = cluster_labels\n",
    "    df_delta.loc[df_delta[\"cluster\"] >= 0, \"cluster\"] = df_delta[\"cluster\"] + 1\n",
    "\n",
    "    clusters = df_delta[df_delta[\"cluster\"] != -1][\"cluster\"].unique()\n",
    "    clusters.sort()\n",
    "\n",
    "    print(\"DBSCAN completed.\")\n",
    "    print(f\"Total data points: {len(df_delta)}\")\n",
    "    n_noise = sum(cluster_labels == -1)\n",
    "    print(f\"Noise points: {n_noise}\")\n",
    "    print(f\"Valid cluster labels (excluding -1): {clusters}\")\n",
    "\n",
    "    print(\"\\nSTEP 5: Summarizing clusters:\")\n",
    "    if len(clusters) == 0:\n",
    "        print(\"No valid clusters (all noise?).\")\n",
    "        return\n",
    "\n",
    "    for label in clusters:\n",
    "        cdata = df_delta[df_delta[\"cluster\"] == label]\n",
    "        cmin = cdata[\"days_since_start\"].min()\n",
    "        cmax = cdata[\"days_since_start\"].max()\n",
    "        csize = len(cdata)\n",
    "        print(f\"  Cluster {label}: size={csize}, time-range=({cmin:.2f}, {cmax:.2f})\")\n",
    "\n",
    "    if merges is None:\n",
    "        merges = []\n",
    "    merges = [set(m) for m in merges]\n",
    "    merged_clusters = set().union(*merges)\n",
    "\n",
    "    plot_items = []\n",
    "    used = set()\n",
    "    for lbl in clusters:\n",
    "        if lbl in used:\n",
    "            continue\n",
    "        belongs_to = None\n",
    "        for mset in merges:\n",
    "            if lbl in mset:\n",
    "                belongs_to = mset\n",
    "                break\n",
    "        if belongs_to is not None:\n",
    "            plot_items.append(belongs_to)\n",
    "            used.update(belongs_to)\n",
    "        else:\n",
    "            plot_items.append({lbl})\n",
    "            used.add(lbl)\n",
    "\n",
    "    print(\"\\nSTEP 6: Creating cluster subplots + images in one figure...\")\n",
    "\n",
    "    filename_col_present = \"filename\" in df_delta.columns\n",
    "    if not filename_col_present:\n",
    "        print(\"NOTE: 'filename' column not in df_delta. Will skip image matching.\")\n",
    "\n",
    "    have_cropped_images = (cropped_images is not None) and (len(cropped_images) > 0)\n",
    "    if not have_cropped_images:\n",
    "        print(\"NOTE: 'cropped_images' is None or empty. Will skip image display.\\n\")\n",
    "\n",
    "    for item in plot_items:\n",
    "        label_list = sorted(item)\n",
    "        sub_df = df_delta[df_delta[\"cluster\"].isin(label_list)].copy()\n",
    "        if len(sub_df) == 0:\n",
    "            print(f\"No data for clusters {label_list}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        snippet_data = sub_df.head(5)\n",
    "        print(f\"\\n   => Plotting cluster(s) {label_list}, total points: {len(sub_df)}\")\n",
    "        print(\"   First 5 lines (with index_number):\")\n",
    "        print(snippet_data[[\"index_number\",\"days_since_start\",\"delta_au\"]].to_string(index=False))\n",
    "\n",
    "        matching_images = []\n",
    "        if filename_col_present and have_cropped_images:\n",
    "            valid_filenames = set(sub_df[\"filename\"].unique())\n",
    "            matching_images = [\n",
    "                (fidx,fname,dobs,tobs,img_8u)\n",
    "                for (fidx,fname,dobs,tobs,img_8u) in cropped_images\n",
    "                if fname in valid_filenames\n",
    "            ]\n",
    "\n",
    "        n_img = len(matching_images)\n",
    "        img_rows = math.ceil(n_img / 8)\n",
    "        fig_width = 25\n",
    "        fig_height = 6 + 3 * img_rows\n",
    "\n",
    "        fig = plt.figure(figsize=(fig_width, fig_height))\n",
    "        gs = GridSpec(nrows=img_rows + 1, ncols=8,\n",
    "                      height_ratios=[6] + [3] * img_rows,\n",
    "                      figure=fig)\n",
    "\n",
    "        ax_scatter = fig.add_subplot(gs[0, :])\n",
    "        colors = plt.cm.tab10(np.linspace(0,1,len(label_list)))\n",
    "\n",
    "        for i, lbl2 in enumerate(label_list):\n",
    "            cdata = sub_df[sub_df[\"cluster\"] == lbl2]\n",
    "            ax_scatter.scatter(\n",
    "                cdata[\"days_since_start\"],\n",
    "                cdata[\"delta_au\"],\n",
    "                color=colors[i],\n",
    "                alpha=0.75,\n",
    "                s=POINT_SIZE,                # <-- SIZE OF DATA POINTS\n",
    "                label=f\"Cluster {lbl2}\"\n",
    "            )\n",
    "            for _, row in cdata.iterrows():\n",
    "                ax_scatter.text(\n",
    "                    row[\"days_since_start\"],\n",
    "                    row[\"delta_au\"],\n",
    "                    str(row[\"index_number\"]),\n",
    "                    fontsize=ANNOTATION_FONTSIZE,  # <-- FONT SIZE OF POINT ANNOTATION\n",
    "                    ha='left',\n",
    "                    va='bottom'\n",
    "                )\n",
    "\n",
    "        if len(label_list)>1:\n",
    "            ax_scatter.set_title(f\"Merged Clusters {label_list}\", fontsize=14)\n",
    "        else:\n",
    "            ax_scatter.set_title(f\"Cluster {label_list[0]}\", fontsize=14)\n",
    "\n",
    "        # x-limits\n",
    "        t_min = sub_df[\"days_since_start\"].min()\n",
    "        t_max = sub_df[\"days_since_start\"].max()\n",
    "        span = t_max - t_min\n",
    "        if span <= 0:\n",
    "            t_min -= 0.1\n",
    "            t_max += 0.1\n",
    "        else:\n",
    "            margin = margin_scale * span\n",
    "            t_min -= margin\n",
    "            t_max += margin\n",
    "        ax_scatter.set_xlim(t_min, t_max)\n",
    "\n",
    "        # y-limits\n",
    "        d_min = sub_df[\"delta_au\"].min()\n",
    "        d_max = sub_df[\"delta_au\"].max()\n",
    "        y_span = d_max - d_min\n",
    "        if y_span <= 0:\n",
    "            d_min -= 0.5\n",
    "            d_max += 0.5\n",
    "        else:\n",
    "            y_margin = margin_scale * y_span\n",
    "            d_min -= y_margin\n",
    "            d_max += y_margin\n",
    "        ax_scatter.set_ylim(d_min, d_max)\n",
    "\n",
    "        ax_scatter.set_xlabel(\"Days Since Start\")\n",
    "        ax_scatter.set_ylabel(\"Delta (AU)\")\n",
    "        if len(label_list)>1:\n",
    "            ax_scatter.legend()\n",
    "\n",
    "        # Force 0.2-day intervals on X; no scientific notation on both axes\n",
    "        ax_scatter.xaxis.set_major_locator(mticker.MultipleLocator(0.1))\n",
    "        ax_scatter.ticklabel_format(style='plain', useOffset=False, axis='x')\n",
    "        ax_scatter.ticklabel_format(style='plain', useOffset=False, axis='y')\n",
    "        ax_scatter.yaxis.set_major_locator(mticker.MaxNLocator(nbins='auto'))\n",
    "\n",
    "        # Place images\n",
    "        for i_img, (fidx2, fname2, dobs2, tobs2, img_8u) in enumerate(matching_images):\n",
    "            row_i = 1 + (i_img // 8)\n",
    "            col_i = i_img % 8\n",
    "            ax_img = fig.add_subplot(gs[row_i, col_i])\n",
    "            ax_img.imshow(img_8u, cmap='gray', origin='lower')\n",
    "            ax_img.set_title(f\"File #{fidx2}\", fontsize=FILE_NUMBER_FONTSIZE)  # <-- FILE # FONT SIZE\n",
    "            ax_img.axis(\"off\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        if filename_col_present and have_cropped_images:\n",
    "            print(f\"   => Found {len(matching_images)} image(s) for clusters {label_list}.\")\n",
    "            for (xxid, xxfn, xxdt, xxt2, _) in matching_images:\n",
    "                print(f\"       * File #{xxid} => {xxfn} => {xxdt} {xxt2}\")\n",
    "\n",
    "        print(\"   ----------------------------------------------------\\n\")\n",
    "\n",
    "    print(\"===== End of DBSCAN + Image Plotting (Delta) (11B) =====\\n\")\n",
    "\n",
    "\n",
    "# Actually run 11B code on df_delta_merged\n",
    "run_dbscan_and_plot_with_images_delta(\n",
    "    df_delta_merged,\n",
    "    eps=0.5,\n",
    "    min_samples=1,\n",
    "    margin_scale=0.1,\n",
    "    use_merge_plots=True,\n",
    "    merges=[[11,12,13]],\n",
    "    cropped_images=cropped_images\n",
    ")\n",
    "print(\"=== END CELL 11B ===\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25a93d2-c3bf-492b-9735-dc3437c523c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "#                             CELL 11C\n",
    "###############################################################################\n",
    "print(\"=== BEGIN CELL 11C: DBSCAN + Image Plotting (Surface Brightness) ===\\n\")\n",
    "\n",
    "def run_dbscan_and_plot_with_images_sb(\n",
    "    df_sb,\n",
    "    eps=0.25,\n",
    "    min_samples=3,\n",
    "    margin_scale=0.1,\n",
    "    use_merge_plots=True,\n",
    "    merges=None,\n",
    "    cropped_images=None\n",
    "):\n",
    "    print(\"\\n==========================================\")\n",
    "    print(\"    DBSCAN + Image Plotting for S_BRT\")\n",
    "    print(\"==========================================\\n\")\n",
    "\n",
    "    required_cols = [\"datetime\", \"s_brt\", \"days_since_start\"]\n",
    "    for c in required_cols:\n",
    "        if c not in df_sb.columns:\n",
    "            print(f\"ERROR: Missing column '{c}' in df_sb! Cannot proceed.\")\n",
    "            return\n",
    "\n",
    "    print(\"STEP 1: Checking df_sb sample (first 5 rows):\")\n",
    "    print(df_sb.head(5))\n",
    "\n",
    "    nan_count_sbrt = df_sb[\"s_brt\"].isna().sum()\n",
    "    nan_count_days = df_sb[\"days_since_start\"].isna().sum()\n",
    "    if nan_count_sbrt > 0 or nan_count_days > 0:\n",
    "        print(f\"WARNING: Found {nan_count_sbrt} NaN in 's_brt' and {nan_count_days} in 'days_since_start'.\")\n",
    "    else:\n",
    "        print(\"No NaN values found.\")\n",
    "\n",
    "    days_min = df_sb[\"days_since_start\"].min()\n",
    "    days_max = df_sb[\"days_since_start\"].max()\n",
    "    print(f\"\\nTime range => min: {days_min:.3f}, max: {days_max:.3f}\")\n",
    "\n",
    "    df_sb = df_sb.copy()\n",
    "    df_sb[\"index_number\"] = np.arange(len(df_sb)) + 1\n",
    "\n",
    "    print(f\"\\nRunning DBSCAN (eps={eps}, min_samples={min_samples}) ...\")\n",
    "    X = df_sb[\"days_since_start\"].values.reshape(-1,1)\n",
    "    db = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    cluster_labels = db.fit_predict(X)\n",
    "\n",
    "    df_sb[\"cluster\"] = cluster_labels\n",
    "    df_sb.loc[df_sb[\"cluster\"] >= 0, \"cluster\"] = df_sb[\"cluster\"] + 1\n",
    "\n",
    "    clusters = df_sb[df_sb[\"cluster\"] != -1][\"cluster\"].unique()\n",
    "    clusters.sort()\n",
    "\n",
    "    print(\"DBSCAN done. Total points:\", len(df_sb))\n",
    "    n_noise = sum(cluster_labels == -1)\n",
    "    print(f\"Noise points: {n_noise}\")\n",
    "    print(\"Valid cluster labels:\", clusters)\n",
    "\n",
    "    if len(clusters) == 0:\n",
    "        print(\"No valid clusters.\")\n",
    "        return\n",
    "\n",
    "    print(\"\\nCluster summaries:\")\n",
    "    for label in clusters:\n",
    "        cdata = df_sb[df_sb[\"cluster\"] == label]\n",
    "        cmin = cdata[\"days_since_start\"].min()\n",
    "        cmax = cdata[\"days_since_start\"].max()\n",
    "        print(f\"  Cluster {label}: {len(cdata)} points, time-range=({cmin:.2f},{cmax:.2f})\")\n",
    "\n",
    "    if merges is None:\n",
    "        merges = []\n",
    "    merges = [set(m) for m in merges]\n",
    "    merged_clusters = set().union(*merges)\n",
    "\n",
    "    plot_items = []\n",
    "    used = set()\n",
    "    for lbl in clusters:\n",
    "        if lbl in used:\n",
    "            continue\n",
    "        belongs_to = None\n",
    "        for mset in merges:\n",
    "            if lbl in mset:\n",
    "                belongs_to = mset\n",
    "                break\n",
    "        if belongs_to is not None:\n",
    "            plot_items.append(belongs_to)\n",
    "            used.update(belongs_to)\n",
    "        else:\n",
    "            plot_items.append({lbl})\n",
    "            used.add(lbl)\n",
    "\n",
    "    filename_col_present = (\"filename\" in df_sb.columns)\n",
    "    have_cropped_images = (cropped_images is not None) and (len(cropped_images) > 0)\n",
    "\n",
    "    for item in plot_items:\n",
    "        label_list = sorted(item)\n",
    "        sub_df = df_sb[df_sb[\"cluster\"].isin(label_list)].copy()\n",
    "        if sub_df.empty:\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nPlotting cluster(s) {label_list}, total points: {len(sub_df)}\")\n",
    "        print(\"First 5 lines:\")\n",
    "        print(sub_df[[\"index_number\",\"days_since_start\",\"s_brt\"]].head(5))\n",
    "\n",
    "        matching_images = []\n",
    "        if filename_col_present and have_cropped_images:\n",
    "            valid_filenames = set(sub_df[\"filename\"].unique())\n",
    "            matching_images = [\n",
    "                (fidx,fname,dobs,tobs,img_8u)\n",
    "                for (fidx,fname,dobs,tobs,img_8u) in cropped_images\n",
    "                if fname in valid_filenames\n",
    "            ]\n",
    "\n",
    "        n_img = len(matching_images)\n",
    "        img_rows = math.ceil(n_img / 8)\n",
    "        fig = plt.figure(figsize=(25, 6 + 3 * img_rows))\n",
    "        gs = GridSpec(nrows=img_rows + 1, ncols=8,\n",
    "                      height_ratios=[6] + [3] * img_rows,\n",
    "                      figure=fig)\n",
    "\n",
    "        ax_scatter = fig.add_subplot(gs[0,:])\n",
    "        colors = plt.cm.tab10(np.linspace(0,1,len(label_list)))\n",
    "\n",
    "        for i, lbl3 in enumerate(label_list):\n",
    "            cdata = sub_df[sub_df[\"cluster\"] == lbl3]\n",
    "            ax_scatter.scatter(\n",
    "                cdata[\"days_since_start\"],\n",
    "                cdata[\"s_brt\"],\n",
    "                color=colors[i],\n",
    "                alpha=0.75,\n",
    "                s=POINT_SIZE,              # <-- SIZE OF DATA POINTS\n",
    "                label=f\"Cluster {lbl3}\"\n",
    "            )\n",
    "            for _, row in cdata.iterrows():\n",
    "                ax_scatter.text(\n",
    "                    row[\"days_since_start\"],\n",
    "                    row[\"s_brt\"],\n",
    "                    str(row[\"index_number\"]),\n",
    "                    fontsize=ANNOTATION_FONTSIZE,  # <-- FONT SIZE OF POINT ANNOTATION\n",
    "                    ha='left',\n",
    "                    va='bottom'\n",
    "                )\n",
    "\n",
    "        if len(label_list)>1:\n",
    "            ax_scatter.set_title(f\"Merged Clusters {label_list} (SB vs Time)\", fontsize=14)\n",
    "        else:\n",
    "            ax_scatter.set_title(f\"Cluster {label_list[0]} (SB vs Time)\", fontsize=14)\n",
    "\n",
    "        # X-limits\n",
    "        t_min = sub_df[\"days_since_start\"].min()\n",
    "        t_max = sub_df[\"days_since_start\"].max()\n",
    "        span = t_max - t_min\n",
    "        if span <= 0:\n",
    "            t_min -= 0.1\n",
    "            t_max += 0.1\n",
    "        else:\n",
    "            t_min -= margin_scale * span\n",
    "            t_max += margin_scale * span\n",
    "        ax_scatter.set_xlim(t_min, t_max)\n",
    "\n",
    "        # Y-limits\n",
    "        sb_min = sub_df[\"s_brt\"].min()\n",
    "        sb_max = sub_df[\"s_brt\"].max()\n",
    "        sb_span = sb_max - sb_min\n",
    "        if sb_span <= 0:\n",
    "            sb_min -= 0.1\n",
    "            sb_max += 0.1\n",
    "        else:\n",
    "            sb_min -= margin_scale * sb_span\n",
    "            sb_max += margin_scale * sb_span\n",
    "        ax_scatter.set_ylim(sb_min, sb_max)\n",
    "\n",
    "        ax_scatter.set_xlabel(\"Days Since Start\")\n",
    "        ax_scatter.set_ylabel(\"Surface Brightness (s_brt)\")\n",
    "        if len(label_list)>1:\n",
    "            ax_scatter.legend()\n",
    "\n",
    "        # Force 0.2-day intervals on X; no scientific notation on both axes\n",
    "        ax_scatter.xaxis.set_major_locator(mticker.MultipleLocator(0.1))\n",
    "        ax_scatter.ticklabel_format(style='plain', useOffset=False, axis='x')\n",
    "        ax_scatter.ticklabel_format(style='plain', useOffset=False, axis='y')\n",
    "        ax_scatter.yaxis.set_major_locator(mticker.MaxNLocator(nbins='auto'))\n",
    "\n",
    "        # Place images\n",
    "        for i_img, (fidx3, fname3, dobs3, tobs3, img_8u) in enumerate(matching_images):\n",
    "            row_i = 1 + (i_img // 8)\n",
    "            col_i = i_img % 8\n",
    "            ax_img = fig.add_subplot(gs[row_i,col_i])\n",
    "            ax_img.imshow(img_8u, cmap='gray', origin='lower')\n",
    "            ax_img.set_title(f\"File #{fidx3}\", fontsize=FILE_NUMBER_FONTSIZE)  # <-- FILE # FONT SIZE\n",
    "            ax_img.axis(\"off\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        print(f\" => Found {len(matching_images)} matching image(s).\")\n",
    "        for (xid, xfn, xdt, xdt2, _) in matching_images:\n",
    "            print(f\"     * File #{xid} => {xfn} => {xdt} {xdt2}\")\n",
    "\n",
    "    print(\"\\n===== End of DBSCAN + Image Plotting for S_BRT (Cell 11C) =====\\n\")\n",
    "\n",
    "\n",
    "# Actually run 11C code on df_sb_merged\n",
    "run_dbscan_and_plot_with_images_sb(\n",
    "    df_sb_merged,\n",
    "    eps=0.5,\n",
    "    min_samples=1,\n",
    "    merges=[[11,12,13]],\n",
    "    cropped_images=cropped_images\n",
    ")\n",
    "\n",
    "print(\"=== END CELL 11C ===\\n\")\n",
    "\n",
    "# ====================== END OF GRAND CELL ======================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc620ae-dabc-44f4-9283-af0e0cfade5f",
   "metadata": {},
   "source": [
    "CELL 12 - an overlay of the graphs that have been made for cell 12 all in one big cell to see how the points have been moving across each other and to spot any correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674e561c-9fc8-46f4-b513-ce6f7e933ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 12: Triple-Axis Overlay: f_avg, delta_au, s_brt vs. Time\n",
    "# Requirements:\n",
    "#   df_all MUST have columns: ['datetime','days_since_start','f_avg','delta_au','s_brt','filename'(opt)]\n",
    "#   'filename' aligns with cropped_images to see images.\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "df_all = (df_flux[[\"datetime\",\"days_since_start\",\"f_avg\",\"filename\"]]\n",
    "          .merge(df_delta[[\"datetime\",\"delta_au\"]], on=\"datetime\", how=\"inner\")\n",
    "          .merge(df_sb[[\"datetime\",\"s_brt\"]],       on=\"datetime\", how=\"inner\"))\n",
    "\n",
    "def run_dbscan_and_plot_with_images_triple(\n",
    "    df_all,\n",
    "    eps=0.25,\n",
    "    min_samples=3,\n",
    "    margin_scale=0.1,\n",
    "    use_merge_plots=True,\n",
    "    merges=None,\n",
    "    cropped_images=None\n",
    "):\n",
    "    print(\"\\n==========================================\")\n",
    "    print(\"    DBSCAN + Triple-Axis Plot + Images\")\n",
    "    print(\"==========================================\\n\")\n",
    "\n",
    "    needed = [\"datetime\", \"days_since_start\", \"f_avg\", \"delta_au\", \"s_brt\"]\n",
    "    for c in needed:\n",
    "        if c not in df_all.columns:\n",
    "            print(f\"ERROR: missing '{c}' column in df_all.\")\n",
    "            return\n",
    "\n",
    "    print(\"STEP 1: Checking df_all sample (first 5 rows):\")\n",
    "    print(df_all.head(5))\n",
    "\n",
    "    # Check for NaNs\n",
    "    nan_favg  = df_all[\"f_avg\"].isna().sum()\n",
    "    nan_delta = df_all[\"delta_au\"].isna().sum()\n",
    "    nan_sbrt  = df_all[\"s_brt\"].isna().sum()\n",
    "    nan_days  = df_all[\"days_since_start\"].isna().sum()\n",
    "    if (nan_favg + nan_delta + nan_sbrt + nan_days) > 0:\n",
    "        print(f\"WARNING: NaNs => f_avg({nan_favg}), delta({nan_delta}), s_brt({nan_sbrt}), days({nan_days}).\")\n",
    "\n",
    "    df_all = df_all.copy()\n",
    "    df_all[\"index_number\"] = np.arange(len(df_all)) + 1\n",
    "\n",
    "    print(f\"\\nSTEP 2: DBSCAN on 'days_since_start' (eps={eps}, min_samples={min_samples})...\")\n",
    "    X = df_all[\"days_since_start\"].values.reshape(-1,1)\n",
    "    db = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    cluster_labels = db.fit_predict(X)\n",
    "\n",
    "    df_all[\"cluster\"] = cluster_labels\n",
    "    df_all.loc[df_all[\"cluster\"] >= 0, \"cluster\"] = df_all[\"cluster\"] + 1\n",
    "    clusters = df_all[df_all[\"cluster\"] != -1][\"cluster\"].unique()\n",
    "    clusters.sort()\n",
    "\n",
    "    print(\"DBSCAN done. Total points:\", len(df_all))\n",
    "    n_noise = sum(cluster_labels == -1)\n",
    "    print(f\"Noise points: {n_noise}\")\n",
    "    print(\"Valid cluster labels (excluding -1):\", clusters)\n",
    "\n",
    "    if len(clusters) == 0:\n",
    "        print(\"No valid clusters found.\")\n",
    "        return\n",
    "\n",
    "    print(\"\\nSTEP 3: Summaries of clusters:\")\n",
    "    for label in clusters:\n",
    "        cdata = df_all[df_all[\"cluster\"] == label]\n",
    "        cmin = cdata[\"days_since_start\"].min()\n",
    "        cmax = cdata[\"days_since_start\"].max()\n",
    "        print(f\"  Cluster {label}: {len(cdata)} pts, time-range=({cmin:.2f}, {cmax:.2f})\")\n",
    "\n",
    "    if merges is None:\n",
    "        merges = []\n",
    "    merges = [set(m) for m in merges]\n",
    "\n",
    "    plot_items = []\n",
    "    used = set()\n",
    "    for lbl in clusters:\n",
    "        if lbl in used:\n",
    "            continue\n",
    "        belongs_to = None\n",
    "        for mset in merges:\n",
    "            if lbl in mset:\n",
    "                belongs_to = mset\n",
    "                break\n",
    "        if belongs_to is not None:\n",
    "            plot_items.append(belongs_to)\n",
    "            used.update(belongs_to)\n",
    "        else:\n",
    "            plot_items.append({lbl})\n",
    "            used.add(lbl)\n",
    "\n",
    "    filename_col_present = (\"filename\" in df_all.columns)\n",
    "    have_cropped_images = (cropped_images is not None) and (len(cropped_images) > 0)\n",
    "\n",
    "    for item in plot_items:\n",
    "        label_list = sorted(item)\n",
    "        sub_df = df_all[df_all[\"cluster\"].isin(label_list)].copy()\n",
    "        if sub_df.empty:\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n   => Plotting cluster(s) {label_list}, total points: {len(sub_df)}\")\n",
    "        snippet_data = sub_df.head(5)\n",
    "        print(\"   First 5 lines (with index_number):\")\n",
    "        print(snippet_data[[\"index_number\",\"days_since_start\",\"f_avg\",\"delta_au\",\"s_brt\"]])\n",
    "\n",
    "        # images\n",
    "        matching_images = []\n",
    "        if filename_col_present and have_cropped_images:\n",
    "            valid_filenames = set(sub_df[\"filename\"].unique())\n",
    "            matching_images = [\n",
    "                (fidx,fname,dobs,tobs,img_8u)\n",
    "                for (fidx,fname,dobs,tobs,img_8u) in cropped_images\n",
    "                if fname in valid_filenames\n",
    "            ]\n",
    "\n",
    "        n_img = len(matching_images)\n",
    "        img_rows = math.ceil(n_img/8)\n",
    "        fig = plt.figure(figsize=(25, 6 + 3*img_rows))\n",
    "        gs = GridSpec(nrows=img_rows+1, ncols=8,\n",
    "                      height_ratios=[6] + [3]*img_rows,\n",
    "                      figure=fig)\n",
    "\n",
    "        # triple-axis\n",
    "        ax1 = fig.add_subplot(gs[0,:])\n",
    "        ax2 = ax1.twinx()\n",
    "        ax3 = ax1.twinx()\n",
    "        ax3.spines.right.set_position((\"axes\",1.035))\n",
    "\n",
    "        col_favg  = 'blue'\n",
    "        col_delta = 'orange'\n",
    "        col_sbrt  = 'green'\n",
    "\n",
    "        # f_avg\n",
    "        for _, row in sub_df.iterrows():\n",
    "            ax1.scatter(row[\"days_since_start\"], row[\"f_avg\"], color=col_favg, s=20, alpha=0.8)\n",
    "            ax1.text(row[\"days_since_start\"], row[\"f_avg\"], str(row[\"index_number\"]),\n",
    "                     fontsize=8, ha='left', va='bottom', color=col_favg)\n",
    "\n",
    "        # delta\n",
    "        for _, row in sub_df.iterrows():\n",
    "            ax2.scatter(row[\"days_since_start\"], row[\"delta_au\"], color=col_delta, s=20, alpha=0.8)\n",
    "            ax2.text(row[\"days_since_start\"], row[\"delta_au\"], str(row[\"index_number\"]),\n",
    "                     fontsize=8, ha='left', va='bottom', color=col_delta)\n",
    "\n",
    "        # s_brt\n",
    "        for _, row in sub_df.iterrows():\n",
    "            ax3.scatter(row[\"days_since_start\"], row[\"s_brt\"], color=col_sbrt, s=20, alpha=0.8)\n",
    "            ax3.text(row[\"days_since_start\"], row[\"s_brt\"], str(row[\"index_number\"]),\n",
    "                     fontsize=8, ha='left', va='bottom', color=col_sbrt)\n",
    "\n",
    "        # Titles\n",
    "        if len(label_list)>1:\n",
    "            ax1.set_title(f\"Merged Clusters {label_list}: f_avg / delta / s_brt vs. Time\", fontsize=14)\n",
    "        else:\n",
    "            ax1.set_title(f\"Cluster {label_list[0]}: f_avg / delta / s_brt vs. Time\", fontsize=14)\n",
    "\n",
    "        ax1.set_xlabel(\"Days Since Start\")\n",
    "        ax1.set_ylabel(\"f_avg\", color=col_favg)\n",
    "        ax2.set_ylabel(\"delta_au\", color=col_delta)\n",
    "        ax3.set_ylabel(\"s_brt\", color=col_sbrt)\n",
    "\n",
    "        ax1.tick_params(axis='y', labelcolor=col_favg)\n",
    "        ax2.tick_params(axis='y', labelcolor=col_delta)\n",
    "        ax3.tick_params(axis='y', labelcolor=col_sbrt)\n",
    "\n",
    "        # X-limits\n",
    "        t_min = sub_df[\"days_since_start\"].min()\n",
    "        t_max = sub_df[\"days_since_start\"].max()\n",
    "        span = t_max - t_min\n",
    "        if span<=0:\n",
    "            t_min-=0.1\n",
    "            t_max+=0.1\n",
    "        else:\n",
    "            t_min-= margin_scale*span\n",
    "            t_max+= margin_scale*span\n",
    "        ax1.set_xlim(t_min, t_max)\n",
    "\n",
    "        # place images below\n",
    "        for i_img, (fidx,fname,dobs,tobs,img_8u) in enumerate(matching_images):\n",
    "            row_i = 1 + (i_img//8)\n",
    "            col_i = i_img % 8\n",
    "            ax_img = fig.add_subplot(gs[row_i,col_i])\n",
    "            ax_img.imshow(img_8u, cmap='gray', origin='lower')\n",
    "            ax_img.set_title(f\"File #{fidx}\", fontsize=8)\n",
    "            ax_img.axis(\"off\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        print(f\"   => Found {len(matching_images)} image(s) for clusters {label_list}.\")\n",
    "        for (fidx,fname,dobs,tobs,_) in matching_images:\n",
    "            print(f\"       * {fidx} => {fname} => {dobs} {tobs}\")\n",
    "\n",
    "    print(\"\\n===== End of DBSCAN + Triple-Axis Plot + Images =====\\n\")\n",
    "\n",
    "# Usage example (remember you must define df_all!):\n",
    "# df_all = df_flux.merge(...) # etc\n",
    "run_dbscan_and_plot_with_images_triple(\n",
    "     df_all,\n",
    "     eps=0.4,\n",
    "     min_samples=1,\n",
    "     merges=[[11,12,13]],\n",
    "     cropped_images=cropped_images\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8856128d-1d15-4b9a-931b-ce8cc203ff77",
   "metadata": {},
   "source": [
    "CELL 13 - an image saving system that is necessary so that i can then use them to do the next part of my code that will run independently of the analysis script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3f2500-7fec-4001-b888-638a0dc710c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Image Saving & Numbering System\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "# Make sure you have access to the list \"image_storage\" from Cell 5\n",
    "\n",
    "# 1) Create parent folder \"images\"\n",
    "parent_folder = \"images\"\n",
    "os.makedirs(parent_folder, exist_ok=True)  # doesn't fail if already exists\n",
    "\n",
    "# 2) Create the 4 subfolders inside \"images\"\n",
    "os.makedirs(os.path.join(parent_folder, \"1 Original_Images\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(parent_folder, \"2 Rotated_Images\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(parent_folder, \"3 Contour_Images\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(parent_folder, \"4 Cropped_Images\"), exist_ok=True)\n",
    "\n",
    "# 3) Create counters for each image type\n",
    "orig_counter = 1\n",
    "rot_counter = 1\n",
    "cont_counter = 1\n",
    "crop_counter = 1\n",
    "\n",
    "# 4) Iterate through image_storage to save each type\n",
    "for entry in image_storage:\n",
    "    i = entry[\"index\"]\n",
    "    file_name = entry[\"file_name\"]\n",
    "    date_obs = entry[\"date_obs\"]\n",
    "    time_obs = entry[\"time_obs\"]\n",
    "\n",
    "    # Convert date/time to a file-friendly format: e.g. \"20230101_123456\"\n",
    "    if date_obs != \"N/A\" and time_obs != \"N/A\":\n",
    "        date_str = date_obs.replace(\"-\", \"\")  # \"2023-01-01\" -> \"20230101\"\n",
    "        time_str = time_obs.replace(\":\", \"\")  # \"12:34:56\" -> \"123456\"\n",
    "        dt_stamp = f\"{date_str}_{time_str}\"\n",
    "    else:\n",
    "        # If missing date/time, fallback to something like \"NA\"\n",
    "        dt_stamp = \"NA\"\n",
    "\n",
    "    orig_img = entry.get(\"orig_image\")\n",
    "    rot_img  = entry.get(\"rotated_image\")\n",
    "    cont_img = entry.get(\"contour_image\")\n",
    "    crop_img = entry.get(\"cropped_image\")\n",
    "\n",
    "    if orig_img is not None:\n",
    "        orig_filename = f\"orig_{orig_counter:03d}_{dt_stamp}.png\"\n",
    "        orig_path = os.path.join(parent_folder, \"1 Original_Images\", orig_filename)\n",
    "        cv2.imwrite(orig_path, orig_img)\n",
    "        orig_counter += 1\n",
    "\n",
    "    if rot_img is not None:\n",
    "        rot_filename = f\"rot_{rot_counter:03d}_{dt_stamp}.png\"\n",
    "        rot_path = os.path.join(parent_folder, \"2 Rotated_Images\", rot_filename)\n",
    "        cv2.imwrite(rot_path, rot_img)\n",
    "        rot_counter += 1\n",
    "\n",
    "    if cont_img is not None:\n",
    "        cont_filename = f\"cont_{cont_counter:03d}_{dt_stamp}.png\"\n",
    "        cont_path = os.path.join(parent_folder, \"3 Contour_Images\", cont_filename)\n",
    "        cv2.imwrite(cont_path, cont_img)\n",
    "        cont_counter += 1\n",
    "\n",
    "    if crop_img is not None:\n",
    "        crop_filename = f\"crop_{crop_counter:03d}_{dt_stamp}.png\"\n",
    "        crop_path = os.path.join(parent_folder, \"4 Cropped_Images\", crop_filename)\n",
    "        cv2.imwrite(crop_path, crop_img)\n",
    "        crop_counter += 1\n",
    "\n",
    "print(\"All images have been saved into the 'images' folder with numbering and timestamps!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4ab544-af2c-4d3a-a3f1-5a6745debd7b",
   "metadata": {},
   "source": [
    "Here's a breakdown of what each of the 48 data points in the **Horizons API output** represents:\n",
    "\n",
    "---\n",
    "\n",
    "   **Position & Motion Data (1–6)**\n",
    "1. **Astrometric RA & DEC** – Right Ascension and Declination of the target in the ICRF frame, without light-time correction.\n",
    "2. **Apparent RA & DEC** – RA & DEC adjusted for light-time, stellar aberration, and gravitational deflection.\n",
    "3. **Rates: RA & DEC** – Change in RA & DEC over time (arcsec/hour).\n",
    "4. **Apparent AZ & EL** – Azimuth and elevation of the object from the observer’s location.\n",
    "5. **Rates: AZ & EL** – Change in AZ & EL over time (arcsec/minute).\n",
    "6. **Satellite X & Y, pos. angle** – Relative X and Y position of a satellite w.r.t. its primary body.\n",
    "\n",
    "---\n",
    "\n",
    "    **Time & Local Observational Data (7–11)**\n",
    "7.  **Local Apparent Sidereal Time** – Observer’s sidereal time at local meridian.\n",
    "8.  **Airmass & extinction** – Atmospheric extinction and relative optical airmass.\n",
    "9.  **Visual mag. & Surface Brightness** – Apparent visual magnitude and surface brightness.\n",
    "10. **Illuminated fraction** – Fraction of the object illuminated by the Sun (phase).\n",
    "11. **Defect of illumination** – Angular width of the unilluminated portion of the object.\n",
    "\n",
    "---\n",
    "\n",
    "    **Angular Data (12–16)**\n",
    "12. **Satellite angular separation/visibility** – Angular separation of a satellite from its primary.\n",
    "13. **Target angular diameter** – Angular size of the target object.\n",
    "14. **Observer sub-lon & sub-lat** – Sub-observer latitude and longitude on the target body.\n",
    "15. **Sun sub-longitude & sub-latitude** – Sub-solar latitude and longitude on the target body.\n",
    "16. **Sub-Sun position angle & distance** – Angular position of the sub-solar point and its separation from the observer's sub-point.\n",
    "\n",
    "---\n",
    "\n",
    "    **Orbital & Range Data (17–22)**\n",
    "17. **North Pole position angle & distance** – Orientation and distance of the celestial north pole from the observer.\n",
    "18. **Heliocentric ecliptic lon. & lat.** – Longitude and latitude of the target in the heliocentric ecliptic J2000 frame.\n",
    "19. **Heliocentric range & range-rate** – Distance of the target from the Sun and its radial velocity.\n",
    "20. **Observer range & range-rate** – Distance of the target from the observer and its radial velocity.\n",
    "21. **One-way (down-leg) light-time** – Time taken for light to travel from the object to the observer.\n",
    "22. **Speed wrt Sun & observer** – Target’s velocity relative to the Sun and the observer.\n",
    "\n",
    "---\n",
    "\n",
    "    **Elongation & Angles (23–28)**\n",
    "23. **Sun-Observer-Target ELONG angle** – Angle between the Sun, observer, and target.\n",
    "24. **Sun-Target-Observer ~PHASE angle** – Phase angle of the target as seen from the observer.\n",
    "25. **Target-Observer-Moon angle/Illum%** – Angular separation between the target and the Moon and Moon's illumination fraction.\n",
    "26. **Observer-Primary-Target angle** – Angle between the target, its primary body, and the observer.\n",
    "27. **Sun-Target radial & velocity pos. angle** – Radial and velocity position angle between the Sun and target.\n",
    "28. **Orbit plane angle** – Angle between the observer and the target’s orbital plane.\n",
    "\n",
    "---\n",
    " \n",
    "    **Constellation, Time & Reference Frames (29–34)**\n",
    "29. **Constellation ID** – 3-letter abbreviation of the constellation in which the target appears.\n",
    "30. **Delta-T (TDB - UT)** – Difference between Terrestrial Dynamical Time (TDB) and Universal Time (UT).\n",
    "31. **Observer ecliptic lon. & lat.** – Ecliptic longitude and latitude of the target as seen by the observer.\n",
    "32. **North pole RA & DEC** – Right Ascension and Declination of the target’s north pole.\n",
    "33. **Galactic longitude & latitude** – Target’s position in the galactic coordinate system.\n",
    "34. **Local apparent SOLAR time** – Local solar time at the observer’s location.\n",
    "\n",
    "---\n",
    "\n",
    "    **Uncertainty & Error Data (35–40)**\n",
    "35. **Earth->obs. site light-time** – Light-time from Earth’s center to the observer’s site.\n",
    "36. **RA & DEC uncertainty** – Uncertainty in the RA and DEC of the target.\n",
    "37. **Plane-of-sky error ellipse** – Elliptical uncertainty region in the sky for the target’s position.\n",
    "38. **POS uncertainty (RSS)** – Root-sum-square (RSS) uncertainty in the target’s position.\n",
    "39. **Range & range-rate 3-sigmas** – 3σ uncertainty in the range and range rate of the target.\n",
    "40. **Doppler & delay 3-sigmas** – 3σ uncertainty in the target’s Doppler and delay measurements.\n",
    "\n",
    "---\n",
    "\n",
    "    **Orbital & Physical Data (41–44)**\n",
    "41. **True anomaly angle** – Target’s position in its orbit relative to perihelion.\n",
    "42. **Local apparent hour angle** – Hour angle of the target as seen by the observer.\n",
    "43. **PHASE angle & bisector** – Phase angle and bisector between the Sun and observer.\n",
    "44. **Apparent longitude Sun (L_s)** – Solar longitude in the target’s coordinate system.\n",
    "\n",
    "---\n",
    "\n",
    "    **Apparent Motion Data (45–48)**\n",
    "45. **Inertial apparent RA & DEC** – RA & DEC in the inertial ICRF frame.\n",
    "46. **Rate: Inertial RA & DEC** – Change in RA & DEC in the ICRF frame.\n",
    "47. **Sky motion: rate & angles** – Rate of target’s motion in the sky.\n",
    "48. **Lunar sky-brightness & sky SNR** – Brightness of the sky due to moonlight and signal-to-noise ratio of the target.\n",
    "\n",
    "---\n",
    "\n",
    "This breakdown tells you what each data point represents in the Horizons API output. If you need further details or a focus on specific parameters, let me know!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
